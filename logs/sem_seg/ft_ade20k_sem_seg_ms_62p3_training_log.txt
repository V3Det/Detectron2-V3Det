2022-10-29 11:05:37,067 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.8 (default, Feb 24 2021, 21:46:12) [GCC 7.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-40GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.2, V11.2.142
GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
PyTorch: 1.9.0a0+df837d0
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2019.0.4 Product Build 20190411 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash N/A)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.2
  - NVCC architecture flags: -gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_86,code=compute_86
  - CuDNN 8.1.1
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.2, CUDNN_VERSION=8.1.1, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, FORCE_FALLBACK_CUDA_MPI=1, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=ON, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.0a0
OpenCV: 4.5.5
MMCV: 1.6.2
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.2
MMSegmentation: 0.20.2+dd8b542
------------------------------------------------------------

2022-10-29 11:05:37,067 - mmseg - INFO - Distributed training: True
2022-10-29 11:05:37,950 - mmseg - INFO - Config:
num_things_classes = 100
num_stuff_classes = 50
num_classes = 150
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoderMask2FormerAug',
    pretrained=
    '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/checkpoint-149/mp_rank_00_model_states_renamed-s14tos16.pt',
    backbone=dict(
        type='BEiTAdapter',
        patch_size=16,
        embed_dim=1408,
        depth=40,
        num_heads=16,
        mlp_ratio=4.363636363636363,
        qkv_bias=True,
        use_abs_pos_emb=True,
        use_rel_pos_bias=False,
        img_size=896,
        init_values=None,
        drop_path_rate=0.5,
        conv_inplane=64,
        n_points=4,
        deform_num_heads=16,
        cffn_ratio=0.25,
        deform_ratio=0.5,
        with_cp=True,
        interaction_indexes=[[0, 9], [10, 19], [20, 29], [30, 39]]),
    decode_head=dict(
        type='Mask2FormerHead',
        in_channels=[1408, 1408, 1408, 1408],
        feat_channels=1024,
        out_channels=1024,
        in_index=[0, 1, 2, 3],
        num_things_classes=100,
        num_stuff_classes=50,
        num_queries=200,
        num_transformer_feat_level=3,
        pixel_decoder=dict(
            type='MSDeformAttnPixelDecoder',
            num_outs=3,
            norm_cfg=dict(type='GN', num_groups=32),
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=1024,
                        num_heads=32,
                        num_levels=3,
                        num_points=4,
                        im2col_step=64,
                        dropout=0.0,
                        batch_first=False,
                        norm_cfg=None,
                        init_cfg=None),
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=1024,
                        feedforward_channels=4096,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True),
                        with_cp=True),
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')),
                init_cfg=None),
            positional_encoding=dict(
                type='SinePositionalEncoding', num_feats=512, normalize=True),
            init_cfg=None),
        enforce_decoder_input_project=False,
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=512, normalize=True),
        transformer_decoder=dict(
            type='DetrTransformerDecoder',
            return_intermediate=True,
            num_layers=8,
            transformerlayers=dict(
                type='DetrTransformerDecoderLayer',
                attn_cfgs=dict(
                    type='MultiheadAttention',
                    embed_dims=1024,
                    num_heads=32,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=False),
                ffn_cfgs=dict(
                    embed_dims=1024,
                    feedforward_channels=4096,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.0,
                    dropout_layer=None,
                    add_identity=True,
                    with_cp=True),
                feedforward_channels=4096,
                operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                 'ffn', 'norm')),
            init_cfg=None),
        loss_cls=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=2.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1
            ]),
        loss_mask=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            reduction='mean',
            loss_weight=5.0),
        loss_dice=dict(
            type='DiceLoss',
            use_sigmoid=True,
            activate=True,
            reduction='mean',
            naive_dice=True,
            eps=1.0,
            loss_weight=5.0)),
    train_cfg=dict(
        num_points=12544,
        oversample_ratio=3.0,
        importance_sample_ratio=0.75,
        assigner=dict(
            type='MaskHungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=2.0),
            mask_cost=dict(
                type='CrossEntropyLossCost', weight=5.0, use_sigmoid=True),
            dice_cost=dict(
                type='DiceCost', weight=5.0, pred_act=True, eps=1.0)),
        sampler=dict(type='MaskPseudoSampler')),
    test_cfg=dict(
        panoptic_on=True,
        semantic_on=False,
        instance_on=True,
        max_per_image=100,
        iou_thr=0.8,
        filter_low_score=True,
        mode='slide',
        crop_size=(896, 896),
        stride=(512, 512)),
    init_cfg=None)
dataset_type = 'ADE20KDataset'
data_root = '/sharefs/yxf/data/seg/ade/ADEChallengeData2016'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (896, 896)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', reduce_zero_label=True),
    dict(type='Resize', img_scale=(3584, 896), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
    dict(type='ToMask'),
    dict(type='DefaultFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(3584, 896),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='ResizeToMultiple', size_divisor=32),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=4,
    train=dict(
        type='ADE20KDataset',
        data_root='/sharefs/yxf/data/seg/ade/ADEChallengeData2016',
        img_dir='images/training',
        ann_dir='annotations/training',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', reduce_zero_label=True),
            dict(type='Resize', img_scale=(3584, 896), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
            dict(type='ToMask'),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
        ]),
    val=dict(
        type='ADE20KDataset',
        data_root='/sharefs/yxf/data/seg/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(3584, 896),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='ResizeToMultiple', size_divisor=32),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='ADE20KDataset',
        data_root='/sharefs/yxf/data/seg/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(3584, 896),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='ResizeToMultiple', size_divisor=32),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_80k_cocostuff164k_ss/lr1e-5_lrd0.95_enc6_dec8/iter_60000.pth'
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=2.5e-05,
    betas=(0.9, 0.999),
    weight_decay=0.05,
    constructor='LayerDecayOptimizerConstructor',
    paramwise_cfg=dict(num_layers=40, layer_decay_rate=0.95))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=20000)
checkpoint_config = dict(by_epoch=False, interval=2000, max_keep_ckpts=8)
evaluation = dict(
    interval=2000, metric='mIoU', pre_eval=True, save_best='mIoU')
pretrained = '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/checkpoint-149/mp_rank_00_model_states_renamed-s14tos16.pt'
work_dir = '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95'
gpu_ids = range(0, 64)
auto_resume = False

2022-10-29 11:06:00,837 - mmseg - INFO - Set random seed to 918205617, deterministic: False
2022-10-29 11:06:20,358 - mmseg - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: mask_token, norm.weight, norm.bias, lm_head.weight, lm_head.bias

Name of parameter - Initialization information

backbone.cls_token - torch.Size([1, 1, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.pos_embed - torch.Size([1, 3137, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.level_embed - torch.Size([3, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.patch_embed.proj.weight - torch.Size([1408, 3, 16, 16]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.patch_embed.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.0.weight - torch.Size([64, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.6.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.7.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.7.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv2.0.weight - torch.Size([128, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv2.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv2.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv3.0.weight - torch.Size([256, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv4.0.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc1.weight - torch.Size([1408, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc2.weight - torch.Size([1408, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc3.weight - torch.Size([1408, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc3.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc4.weight - torch.Size([1408, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc4.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.up.weight - torch.Size([1408, 1408, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.up.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm3.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm3.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm4.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm4.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.level_encoding.weight - torch.Size([3, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([1024, 1024, 3, 3]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.mask_feature.weight - torch.Size([1024, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.mask_feature.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.post_norm.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.post_norm.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.query_embed.weight - torch.Size([200, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.query_feat.weight - torch.Size([200, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.level_embed.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.cls_embed.weight - torch.Size([151, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.cls_embed.bias - torch.Size([151]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.0.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.2.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.4.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.4.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  
2022-10-29 11:06:24,761 - mmseg - INFO - EncoderDecoderMask2FormerAug(
  (backbone): BEiTAdapter(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1408, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.012820512987673283)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.025641025975346565)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.03846153989434242)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.05128205195069313)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.06410256773233414)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.07692307978868484)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.08974359184503555)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.10256410390138626)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.11538461595773697)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.12820513546466827)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.14102564752101898)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1538461595773697)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1666666716337204)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1794871836900711)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.19230769574642181)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (16): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.20512820780277252)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (17): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.21794871985912323)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (18): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.23076923191547394)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (19): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.24358974397182465)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (20): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.25641027092933655)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (21): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.26923078298568726)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (22): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.28205129504203796)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (23): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.29487180709838867)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (24): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3076923191547394)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (25): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3205128312110901)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (26): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3333333432674408)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (27): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3461538553237915)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (28): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3589743673801422)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (29): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3717948794364929)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (30): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.38461539149284363)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (31): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.39743590354919434)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (32): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.41025641560554504)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (33): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.42307692766189575)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (34): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.43589743971824646)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (35): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.44871795177459717)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (36): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.4615384638309479)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (37): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.4743589758872986)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (38): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.4871794879436493)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (39): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.5)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (spm): SpatialPriorModule(
      (stem): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
        (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      )
      (conv2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv4): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (fc1): Conv2d(64, 1408, kernel_size=(1, 1), stride=(1, 1))
      (fc2): Conv2d(128, 1408, kernel_size=(1, 1), stride=(1, 1))
      (fc3): Conv2d(256, 1408, kernel_size=(1, 1), stride=(1, 1))
      (fc4): Conv2d(256, 1408, kernel_size=(1, 1), stride=(1, 1))
    )
    (interactions): Sequential(
      (0): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (1): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (2): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (3): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (extra_extractors): Sequential(
          (0): Extractor(
            (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
              (value_proj): Linear(in_features=1408, out_features=704, bias=True)
              (output_proj): Linear(in_features=704, out_features=1408, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1408, out_features=352, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
              )
              (act): GELU()
              (fc2): Linear(in_features=352, out_features=1408, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
          (1): Extractor(
            (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
              (value_proj): Linear(in_features=1408, out_features=704, bias=True)
              (output_proj): Linear(in_features=704, out_features=1408, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1408, out_features=352, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
              )
              (act): GELU()
              (fc2): Linear(in_features=352, out_features=1408, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
        )
      )
    )
    (up): ConvTranspose2d(1408, 1408, kernel_size=(2, 2), stride=(2, 2))
    (norm1): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm2): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm3): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm4): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (decode_head): Mask2FormerHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (conv_seg): None
    (dropout): Dropout2d(p=0.1, inplace=False)
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (1): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (2): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (postional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
      (level_encoding): Embedding(3, 1024)
      (lateral_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (output_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
          (activate): ReLU(inplace=True)
        )
      )
      (mask_feature): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    )
    (transformer_decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (6): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (7): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (decoder_input_projs): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
    )
    (decoder_positional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (query_embed): Embedding(200, 1024)
    (query_feat): Embedding(200, 1024)
    (level_embed): Embedding(3, 1024)
    (cls_embed): Linear(in_features=1024, out_features=151, bias=True)
    (mask_embed): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (loss_cls): CrossEntropyLoss(avg_non_ignore=False)
    (loss_mask): CrossEntropyLoss(avg_non_ignore=False)
    (loss_dice): DiceLoss()
  )
)
2022-10-29 11:06:25,305 - mmseg - INFO - Loaded 20210 images
2022-10-29 11:06:33,277 - mmseg - INFO - load checkpoint from local path: /sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_80k_cocostuff164k_ss/lr1e-5_lrd0.95_enc6_dec8/iter_60000.pth
2022-10-29 11:06:55,054 - mmseg - WARNING - The model and loaded state dict do not match exactly

size mismatch for decode_head.cls_embed.weight: copying a param with shape torch.Size([172, 1024]) from checkpoint, the shape in current model is torch.Size([151, 1024]).
size mismatch for decode_head.cls_embed.bias: copying a param with shape torch.Size([172]) from checkpoint, the shape in current model is torch.Size([151]).
2022-10-29 11:06:55,507 - mmseg - INFO - Start running, host: fangyuxin@yxf-l2q9n-3208139-worker-0, work_dir: /sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95
2022-10-29 11:06:55,508 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2022-10-29 11:06:55,509 - mmseg - INFO - workflow: [('train', 1)], max: 20000 iters
2022-10-29 11:06:55,510 - mmseg - INFO - Checkpoints will be saved to /sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95 by HardDiskBackend.
2022-10-29 11:10:17,527 - mmseg - INFO - Iter [50/20000]	lr: 2.984e-07, eta: 19:34:25, time: 3.532, data_time: 0.018, memory: 35398, decode.loss_cls: 7.7001, decode.loss_mask: 0.7257, decode.loss_dice: 1.0220, decode.d0.loss_cls: 10.3820, decode.d0.loss_mask: 0.8012, decode.d0.loss_dice: 1.2624, decode.d1.loss_cls: 9.2209, decode.d1.loss_mask: 0.7497, decode.d1.loss_dice: 1.1142, decode.d2.loss_cls: 8.6395, decode.d2.loss_mask: 0.7333, decode.d2.loss_dice: 1.0531, decode.d3.loss_cls: 8.0371, decode.d3.loss_mask: 0.7251, decode.d3.loss_dice: 1.0269, decode.d4.loss_cls: 7.6647, decode.d4.loss_mask: 0.7269, decode.d4.loss_dice: 1.0304, decode.d5.loss_cls: 7.6126, decode.d5.loss_mask: 0.7255, decode.d5.loss_dice: 1.0190, decode.d6.loss_cls: 7.6271, decode.d6.loss_mask: 0.7220, decode.d6.loss_dice: 1.0182, decode.d7.loss_cls: 7.6057, decode.d7.loss_mask: 0.7243, decode.d7.loss_dice: 1.0196, loss: 90.6893
2022-10-29 11:12:13,196 - mmseg - INFO - Iter [100/20000]	lr: 6.013e-07, eta: 16:09:21, time: 2.313, data_time: 0.013, memory: 35398, decode.loss_cls: 3.1590, decode.loss_mask: 0.6584, decode.loss_dice: 0.9536, decode.d0.loss_cls: 10.3805, decode.d0.loss_mask: 0.7428, decode.d0.loss_dice: 1.2022, decode.d1.loss_cls: 3.8248, decode.d1.loss_mask: 0.6850, decode.d1.loss_dice: 1.0578, decode.d2.loss_cls: 3.3356, decode.d2.loss_mask: 0.6579, decode.d2.loss_dice: 0.9921, decode.d3.loss_cls: 3.2523, decode.d3.loss_mask: 0.6562, decode.d3.loss_dice: 0.9606, decode.d4.loss_cls: 3.2164, decode.d4.loss_mask: 0.6578, decode.d4.loss_dice: 0.9646, decode.d5.loss_cls: 3.1884, decode.d5.loss_mask: 0.6598, decode.d5.loss_dice: 0.9491, decode.d6.loss_cls: 3.1854, decode.d6.loss_mask: 0.6636, decode.d6.loss_dice: 0.9385, decode.d7.loss_cls: 3.1777, decode.d7.loss_mask: 0.6622, decode.d7.loss_dice: 0.9455, loss: 51.7276
2022-10-29 11:14:09,776 - mmseg - INFO - Iter [150/20000]	lr: 9.028e-07, eta: 15:01:44, time: 2.332, data_time: 0.013, memory: 35398, decode.loss_cls: 2.3434, decode.loss_mask: 0.6521, decode.loss_dice: 0.9201, decode.d0.loss_cls: 10.3727, decode.d0.loss_mask: 0.7201, decode.d0.loss_dice: 1.1755, decode.d1.loss_cls: 2.9437, decode.d1.loss_mask: 0.6613, decode.d1.loss_dice: 1.0227, decode.d2.loss_cls: 2.6658, decode.d2.loss_mask: 0.6401, decode.d2.loss_dice: 0.9655, decode.d3.loss_cls: 2.5116, decode.d3.loss_mask: 0.6448, decode.d3.loss_dice: 0.9327, decode.d4.loss_cls: 2.4126, decode.d4.loss_mask: 0.6480, decode.d4.loss_dice: 0.9298, decode.d5.loss_cls: 2.3845, decode.d5.loss_mask: 0.6507, decode.d5.loss_dice: 0.9200, decode.d6.loss_cls: 2.3766, decode.d6.loss_mask: 0.6517, decode.d6.loss_dice: 0.9151, decode.d7.loss_cls: 2.3423, decode.d7.loss_mask: 0.6514, decode.d7.loss_dice: 0.9155, loss: 44.9700
2022-10-29 11:16:07,606 - mmseg - INFO - Iter [200/20000]	lr: 1.203e-06, eta: 14:29:00, time: 2.357, data_time: 0.012, memory: 35398, decode.loss_cls: 1.8530, decode.loss_mask: 0.6487, decode.loss_dice: 0.9373, decode.d0.loss_cls: 10.3592, decode.d0.loss_mask: 0.6879, decode.d0.loss_dice: 1.1623, decode.d1.loss_cls: 2.4975, decode.d1.loss_mask: 0.6490, decode.d1.loss_dice: 1.0039, decode.d2.loss_cls: 2.1779, decode.d2.loss_mask: 0.6424, decode.d2.loss_dice: 0.9578, decode.d3.loss_cls: 2.0248, decode.d3.loss_mask: 0.6438, decode.d3.loss_dice: 0.9345, decode.d4.loss_cls: 1.9284, decode.d4.loss_mask: 0.6488, decode.d4.loss_dice: 0.9343, decode.d5.loss_cls: 1.8856, decode.d5.loss_mask: 0.6478, decode.d5.loss_dice: 0.9346, decode.d6.loss_cls: 1.8705, decode.d6.loss_mask: 0.6458, decode.d6.loss_dice: 0.9311, decode.d7.loss_cls: 1.8477, decode.d7.loss_mask: 0.6487, decode.d7.loss_dice: 0.9296, loss: 41.0331
2022-10-29 11:18:04,187 - mmseg - INFO - Iter [250/20000]	lr: 1.501e-06, eta: 14:06:57, time: 2.332, data_time: 0.013, memory: 35398, decode.loss_cls: 1.5149, decode.loss_mask: 0.6562, decode.loss_dice: 0.9452, decode.d0.loss_cls: 10.3408, decode.d0.loss_mask: 0.6755, decode.d0.loss_dice: 1.1351, decode.d1.loss_cls: 2.0798, decode.d1.loss_mask: 0.6599, decode.d1.loss_dice: 1.0054, decode.d2.loss_cls: 1.8004, decode.d2.loss_mask: 0.6551, decode.d2.loss_dice: 0.9568, decode.d3.loss_cls: 1.6648, decode.d3.loss_mask: 0.6520, decode.d3.loss_dice: 0.9423, decode.d4.loss_cls: 1.5841, decode.d4.loss_mask: 0.6536, decode.d4.loss_dice: 0.9449, decode.d5.loss_cls: 1.5431, decode.d5.loss_mask: 0.6576, decode.d5.loss_dice: 0.9453, decode.d6.loss_cls: 1.5296, decode.d6.loss_mask: 0.6517, decode.d6.loss_dice: 0.9426, decode.d7.loss_cls: 1.5091, decode.d7.loss_mask: 0.6570, decode.d7.loss_dice: 0.9404, loss: 38.2430
2022-10-29 11:20:01,162 - mmseg - INFO - Iter [300/20000]	lr: 1.798e-06, eta: 13:52:02, time: 2.339, data_time: 0.019, memory: 35398, decode.loss_cls: 1.2889, decode.loss_mask: 0.6642, decode.loss_dice: 0.9395, decode.d0.loss_cls: 10.3208, decode.d0.loss_mask: 0.6724, decode.d0.loss_dice: 1.1057, decode.d1.loss_cls: 1.7953, decode.d1.loss_mask: 0.6682, decode.d1.loss_dice: 0.9877, decode.d2.loss_cls: 1.5628, decode.d2.loss_mask: 0.6597, decode.d2.loss_dice: 0.9411, decode.d3.loss_cls: 1.4398, decode.d3.loss_mask: 0.6595, decode.d3.loss_dice: 0.9339, decode.d4.loss_cls: 1.3623, decode.d4.loss_mask: 0.6623, decode.d4.loss_dice: 0.9403, decode.d5.loss_cls: 1.3213, decode.d5.loss_mask: 0.6653, decode.d5.loss_dice: 0.9355, decode.d6.loss_cls: 1.3050, decode.d6.loss_mask: 0.6599, decode.d6.loss_dice: 0.9376, decode.d7.loss_cls: 1.2843, decode.d7.loss_mask: 0.6652, decode.d7.loss_dice: 0.9371, loss: 36.3159
2022-10-29 11:21:59,254 - mmseg - INFO - Iter [350/20000]	lr: 2.093e-06, eta: 13:41:51, time: 2.362, data_time: 0.066, memory: 35398, decode.loss_cls: 1.1152, decode.loss_mask: 0.6590, decode.loss_dice: 0.9426, decode.d0.loss_cls: 10.2949, decode.d0.loss_mask: 0.6507, decode.d0.loss_dice: 1.0799, decode.d1.loss_cls: 1.5940, decode.d1.loss_mask: 0.6603, decode.d1.loss_dice: 0.9825, decode.d2.loss_cls: 1.3775, decode.d2.loss_mask: 0.6534, decode.d2.loss_dice: 0.9419, decode.d3.loss_cls: 1.2582, decode.d3.loss_mask: 0.6496, decode.d3.loss_dice: 0.9364, decode.d4.loss_cls: 1.1850, decode.d4.loss_mask: 0.6541, decode.d4.loss_dice: 0.9445, decode.d5.loss_cls: 1.1464, decode.d5.loss_mask: 0.6546, decode.d5.loss_dice: 0.9409, decode.d6.loss_cls: 1.1320, decode.d6.loss_mask: 0.6498, decode.d6.loss_dice: 0.9377, decode.d7.loss_cls: 1.1123, decode.d7.loss_mask: 0.6560, decode.d7.loss_dice: 0.9406, loss: 34.7497
2022-10-29 11:23:54,664 - mmseg - INFO - Iter [400/20000]	lr: 2.387e-06, eta: 13:31:32, time: 2.308, data_time: 0.012, memory: 35398, decode.loss_cls: 1.0191, decode.loss_mask: 0.6626, decode.loss_dice: 0.9459, decode.d0.loss_cls: 10.2628, decode.d0.loss_mask: 0.6562, decode.d0.loss_dice: 1.0882, decode.d1.loss_cls: 1.4665, decode.d1.loss_mask: 0.6730, decode.d1.loss_dice: 0.9894, decode.d2.loss_cls: 1.2679, decode.d2.loss_mask: 0.6622, decode.d2.loss_dice: 0.9484, decode.d3.loss_cls: 1.1569, decode.d3.loss_mask: 0.6546, decode.d3.loss_dice: 0.9412, decode.d4.loss_cls: 1.0891, decode.d4.loss_mask: 0.6622, decode.d4.loss_dice: 0.9475, decode.d5.loss_cls: 1.0507, decode.d5.loss_mask: 0.6629, decode.d5.loss_dice: 0.9461, decode.d6.loss_cls: 1.0353, decode.d6.loss_mask: 0.6609, decode.d6.loss_dice: 0.9436, decode.d7.loss_cls: 1.0149, decode.d7.loss_mask: 0.6636, decode.d7.loss_dice: 0.9425, loss: 34.0143
2022-10-29 11:25:48,511 - mmseg - INFO - Iter [450/20000]	lr: 2.679e-06, eta: 13:21:58, time: 2.277, data_time: 0.018, memory: 35398, decode.loss_cls: 0.9204, decode.loss_mask: 0.6703, decode.loss_dice: 0.9526, decode.d0.loss_cls: 10.2283, decode.d0.loss_mask: 0.6546, decode.d0.loss_dice: 1.0600, decode.d1.loss_cls: 1.3337, decode.d1.loss_mask: 0.6775, decode.d1.loss_dice: 0.9862, decode.d2.loss_cls: 1.1448, decode.d2.loss_mask: 0.6670, decode.d2.loss_dice: 0.9516, decode.d3.loss_cls: 1.0405, decode.d3.loss_mask: 0.6641, decode.d3.loss_dice: 0.9463, decode.d4.loss_cls: 0.9825, decode.d4.loss_mask: 0.6679, decode.d4.loss_dice: 0.9559, decode.d5.loss_cls: 0.9548, decode.d5.loss_mask: 0.6662, decode.d5.loss_dice: 0.9501, decode.d6.loss_cls: 0.9405, decode.d6.loss_mask: 0.6665, decode.d6.loss_dice: 0.9497, decode.d7.loss_cls: 0.9237, decode.d7.loss_mask: 0.6689, decode.d7.loss_dice: 0.9531, loss: 33.1777
2022-10-29 11:27:41,272 - mmseg - INFO - Iter [500/20000]	lr: 2.970e-06, eta: 13:13:13, time: 2.255, data_time: 0.012, memory: 35398, decode.loss_cls: 0.9080, decode.loss_mask: 0.6772, decode.loss_dice: 0.9800, decode.d0.loss_cls: 10.1815, decode.d0.loss_mask: 0.6585, decode.d0.loss_dice: 1.0892, decode.d1.loss_cls: 1.2967, decode.d1.loss_mask: 0.6845, decode.d1.loss_dice: 1.0196, decode.d2.loss_cls: 1.1181, decode.d2.loss_mask: 0.6714, decode.d2.loss_dice: 0.9790, decode.d3.loss_cls: 1.0147, decode.d3.loss_mask: 0.6688, decode.d3.loss_dice: 0.9683, decode.d4.loss_cls: 0.9539, decode.d4.loss_mask: 0.6742, decode.d4.loss_dice: 0.9743, decode.d5.loss_cls: 0.9341, decode.d5.loss_mask: 0.6741, decode.d5.loss_dice: 0.9748, decode.d6.loss_cls: 0.9225, decode.d6.loss_mask: 0.6744, decode.d6.loss_dice: 0.9705, decode.d7.loss_cls: 0.9033, decode.d7.loss_mask: 0.6761, decode.d7.loss_dice: 0.9762, loss: 33.2237
2022-10-29 11:29:34,830 - mmseg - INFO - Iter [550/20000]	lr: 2.968e-06, eta: 13:06:11, time: 2.271, data_time: 0.013, memory: 35398, decode.loss_cls: 0.8530, decode.loss_mask: 0.6690, decode.loss_dice: 0.9636, decode.d0.loss_cls: 10.1318, decode.d0.loss_mask: 0.6500, decode.d0.loss_dice: 1.0778, decode.d1.loss_cls: 1.2024, decode.d1.loss_mask: 0.6794, decode.d1.loss_dice: 1.0070, decode.d2.loss_cls: 1.0379, decode.d2.loss_mask: 0.6661, decode.d2.loss_dice: 0.9656, decode.d3.loss_cls: 0.9430, decode.d3.loss_mask: 0.6639, decode.d3.loss_dice: 0.9606, decode.d4.loss_cls: 0.8952, decode.d4.loss_mask: 0.6703, decode.d4.loss_dice: 0.9679, decode.d5.loss_cls: 0.8750, decode.d5.loss_mask: 0.6681, decode.d5.loss_dice: 0.9631, decode.d6.loss_cls: 0.8651, decode.d6.loss_mask: 0.6665, decode.d6.loss_dice: 0.9579, decode.d7.loss_cls: 0.8496, decode.d7.loss_mask: 0.6672, decode.d7.loss_dice: 0.9606, loss: 32.4776
2022-10-29 11:31:37,853 - mmseg - INFO - Iter [600/20000]	lr: 2.961e-06, eta: 13:05:07, time: 2.460, data_time: 0.012, memory: 35398, decode.loss_cls: 0.8108, decode.loss_mask: 0.6615, decode.loss_dice: 0.9678, decode.d0.loss_cls: 10.0831, decode.d0.loss_mask: 0.6407, decode.d0.loss_dice: 1.0746, decode.d1.loss_cls: 1.1442, decode.d1.loss_mask: 0.6711, decode.d1.loss_dice: 1.0090, decode.d2.loss_cls: 0.9821, decode.d2.loss_mask: 0.6622, decode.d2.loss_dice: 0.9741, decode.d3.loss_cls: 0.8996, decode.d3.loss_mask: 0.6582, decode.d3.loss_dice: 0.9638, decode.d4.loss_cls: 0.8489, decode.d4.loss_mask: 0.6607, decode.d4.loss_dice: 0.9699, decode.d5.loss_cls: 0.8321, decode.d5.loss_mask: 0.6594, decode.d5.loss_dice: 0.9685, decode.d6.loss_cls: 0.8260, decode.d6.loss_mask: 0.6589, decode.d6.loss_dice: 0.9628, decode.d7.loss_cls: 0.8156, decode.d7.loss_mask: 0.6604, decode.d7.loss_dice: 0.9651, loss: 32.0315
2022-10-29 11:33:40,323 - mmseg - INFO - Iter [650/20000]	lr: 2.953e-06, eta: 13:03:37, time: 2.449, data_time: 0.059, memory: 35398, decode.loss_cls: 0.7930, decode.loss_mask: 0.6535, decode.loss_dice: 0.9540, decode.d0.loss_cls: 10.0293, decode.d0.loss_mask: 0.6289, decode.d0.loss_dice: 1.0588, decode.d1.loss_cls: 1.1036, decode.d1.loss_mask: 0.6633, decode.d1.loss_dice: 0.9952, decode.d2.loss_cls: 0.9531, decode.d2.loss_mask: 0.6523, decode.d2.loss_dice: 0.9596, decode.d3.loss_cls: 0.8688, decode.d3.loss_mask: 0.6501, decode.d3.loss_dice: 0.9547, decode.d4.loss_cls: 0.8298, decode.d4.loss_mask: 0.6533, decode.d4.loss_dice: 0.9583, decode.d5.loss_cls: 0.8157, decode.d5.loss_mask: 0.6529, decode.d5.loss_dice: 0.9551, decode.d6.loss_cls: 0.8031, decode.d6.loss_mask: 0.6524, decode.d6.loss_dice: 0.9481, decode.d7.loss_cls: 0.7950, decode.d7.loss_mask: 0.6514, decode.d7.loss_dice: 0.9535, loss: 31.5869
2022-10-29 11:35:37,842 - mmseg - INFO - Iter [700/20000]	lr: 2.945e-06, eta: 12:59:46, time: 2.350, data_time: 0.012, memory: 35398, decode.loss_cls: 0.7634, decode.loss_mask: 0.6474, decode.loss_dice: 0.9349, decode.d0.loss_cls: 9.9672, decode.d0.loss_mask: 0.6229, decode.d0.loss_dice: 1.0376, decode.d1.loss_cls: 1.0483, decode.d1.loss_mask: 0.6580, decode.d1.loss_dice: 0.9887, decode.d2.loss_cls: 0.9062, decode.d2.loss_mask: 0.6498, decode.d2.loss_dice: 0.9471, decode.d3.loss_cls: 0.8254, decode.d3.loss_mask: 0.6458, decode.d3.loss_dice: 0.9399, decode.d4.loss_cls: 0.7888, decode.d4.loss_mask: 0.6490, decode.d4.loss_dice: 0.9404, decode.d5.loss_cls: 0.7783, decode.d5.loss_mask: 0.6486, decode.d5.loss_dice: 0.9352, decode.d6.loss_cls: 0.7711, decode.d6.loss_mask: 0.6443, decode.d6.loss_dice: 0.9336, decode.d7.loss_cls: 0.7632, decode.d7.loss_mask: 0.6458, decode.d7.loss_dice: 0.9341, loss: 31.0149
2022-10-29 11:37:35,521 - mmseg - INFO - Iter [750/20000]	lr: 2.938e-06, eta: 12:56:14, time: 2.354, data_time: 0.012, memory: 35398, decode.loss_cls: 0.7542, decode.loss_mask: 0.6711, decode.loss_dice: 0.9648, decode.d0.loss_cls: 9.9128, decode.d0.loss_mask: 0.6446, decode.d0.loss_dice: 1.0619, decode.d1.loss_cls: 1.0162, decode.d1.loss_mask: 0.6847, decode.d1.loss_dice: 1.0146, decode.d2.loss_cls: 0.8809, decode.d2.loss_mask: 0.6730, decode.d2.loss_dice: 0.9725, decode.d3.loss_cls: 0.8092, decode.d3.loss_mask: 0.6690, decode.d3.loss_dice: 0.9616, decode.d4.loss_cls: 0.7790, decode.d4.loss_mask: 0.6701, decode.d4.loss_dice: 0.9654, decode.d5.loss_cls: 0.7658, decode.d5.loss_mask: 0.6675, decode.d5.loss_dice: 0.9610, decode.d6.loss_cls: 0.7610, decode.d6.loss_mask: 0.6683, decode.d6.loss_dice: 0.9586, decode.d7.loss_cls: 0.7518, decode.d7.loss_mask: 0.6706, decode.d7.loss_dice: 0.9640, loss: 31.2742
2022-10-29 11:39:32,653 - mmseg - INFO - Iter [800/20000]	lr: 2.930e-06, eta: 12:52:39, time: 2.341, data_time: 0.013, memory: 35398, decode.loss_cls: 0.7220, decode.loss_mask: 0.6473, decode.loss_dice: 0.9426, decode.d0.loss_cls: 9.8537, decode.d0.loss_mask: 0.6190, decode.d0.loss_dice: 1.0319, decode.d1.loss_cls: 0.9719, decode.d1.loss_mask: 0.6596, decode.d1.loss_dice: 0.9877, decode.d2.loss_cls: 0.8426, decode.d2.loss_mask: 0.6499, decode.d2.loss_dice: 0.9515, decode.d3.loss_cls: 0.7756, decode.d3.loss_mask: 0.6455, decode.d3.loss_dice: 0.9392, decode.d4.loss_cls: 0.7456, decode.d4.loss_mask: 0.6440, decode.d4.loss_dice: 0.9437, decode.d5.loss_cls: 0.7389, decode.d5.loss_mask: 0.6459, decode.d5.loss_dice: 0.9390, decode.d6.loss_cls: 0.7309, decode.d6.loss_mask: 0.6448, decode.d6.loss_dice: 0.9375, decode.d7.loss_cls: 0.7245, decode.d7.loss_mask: 0.6451, decode.d7.loss_dice: 0.9389, loss: 30.5189
2022-10-29 11:41:38,800 - mmseg - INFO - Iter [850/20000]	lr: 2.923e-06, eta: 12:52:42, time: 2.524, data_time: 0.014, memory: 35398, decode.loss_cls: 0.7240, decode.loss_mask: 0.6552, decode.loss_dice: 0.9487, decode.d0.loss_cls: 9.7959, decode.d0.loss_mask: 0.6292, decode.d0.loss_dice: 1.0368, decode.d1.loss_cls: 0.9521, decode.d1.loss_mask: 0.6707, decode.d1.loss_dice: 0.9921, decode.d2.loss_cls: 0.8367, decode.d2.loss_mask: 0.6552, decode.d2.loss_dice: 0.9611, decode.d3.loss_cls: 0.7745, decode.d3.loss_mask: 0.6531, decode.d3.loss_dice: 0.9453, decode.d4.loss_cls: 0.7497, decode.d4.loss_mask: 0.6552, decode.d4.loss_dice: 0.9466, decode.d5.loss_cls: 0.7383, decode.d5.loss_mask: 0.6555, decode.d5.loss_dice: 0.9458, decode.d6.loss_cls: 0.7312, decode.d6.loss_mask: 0.6532, decode.d6.loss_dice: 0.9423, decode.d7.loss_cls: 0.7257, decode.d7.loss_mask: 0.6538, decode.d7.loss_dice: 0.9461, loss: 30.5738
2022-10-29 11:43:37,186 - mmseg - INFO - Iter [900/20000]	lr: 2.915e-06, eta: 12:49:44, time: 2.368, data_time: 0.024, memory: 35398, decode.loss_cls: 0.7025, decode.loss_mask: 0.6702, decode.loss_dice: 0.9583, decode.d0.loss_cls: 9.7349, decode.d0.loss_mask: 0.6411, decode.d0.loss_dice: 1.0421, decode.d1.loss_cls: 0.9373, decode.d1.loss_mask: 0.6849, decode.d1.loss_dice: 1.0050, decode.d2.loss_cls: 0.8119, decode.d2.loss_mask: 0.6728, decode.d2.loss_dice: 0.9657, decode.d3.loss_cls: 0.7523, decode.d3.loss_mask: 0.6700, decode.d3.loss_dice: 0.9568, decode.d4.loss_cls: 0.7230, decode.d4.loss_mask: 0.6722, decode.d4.loss_dice: 0.9594, decode.d5.loss_cls: 0.7147, decode.d5.loss_mask: 0.6687, decode.d5.loss_dice: 0.9566, decode.d6.loss_cls: 0.7065, decode.d6.loss_mask: 0.6688, decode.d6.loss_dice: 0.9538, decode.d7.loss_cls: 0.7051, decode.d7.loss_mask: 0.6684, decode.d7.loss_dice: 0.9529, loss: 30.5560
2022-10-29 11:45:38,484 - mmseg - INFO - Iter [950/20000]	lr: 2.907e-06, eta: 12:47:51, time: 2.426, data_time: 0.058, memory: 35404, decode.loss_cls: 0.6980, decode.loss_mask: 0.6593, decode.loss_dice: 0.9563, decode.d0.loss_cls: 9.6737, decode.d0.loss_mask: 0.6238, decode.d0.loss_dice: 1.0376, decode.d1.loss_cls: 0.9242, decode.d1.loss_mask: 0.6670, decode.d1.loss_dice: 0.9960, decode.d2.loss_cls: 0.8019, decode.d2.loss_mask: 0.6595, decode.d2.loss_dice: 0.9617, decode.d3.loss_cls: 0.7429, decode.d3.loss_mask: 0.6585, decode.d3.loss_dice: 0.9513, decode.d4.loss_cls: 0.7188, decode.d4.loss_mask: 0.6572, decode.d4.loss_dice: 0.9524, decode.d5.loss_cls: 0.7093, decode.d5.loss_mask: 0.6545, decode.d5.loss_dice: 0.9527, decode.d6.loss_cls: 0.7027, decode.d6.loss_mask: 0.6556, decode.d6.loss_dice: 0.9515, decode.d7.loss_cls: 0.6977, decode.d7.loss_mask: 0.6577, decode.d7.loss_dice: 0.9541, loss: 30.2761
2022-10-29 11:47:35,163 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-29 11:47:35,164 - mmseg - INFO - Iter [1000/20000]	lr: 2.900e-06, eta: 12:44:30, time: 2.334, data_time: 0.012, memory: 35404, decode.loss_cls: 0.6614, decode.loss_mask: 0.6486, decode.loss_dice: 0.9515, decode.d0.loss_cls: 9.6063, decode.d0.loss_mask: 0.6167, decode.d0.loss_dice: 1.0228, decode.d1.loss_cls: 0.8852, decode.d1.loss_mask: 0.6632, decode.d1.loss_dice: 0.9976, decode.d2.loss_cls: 0.7637, decode.d2.loss_mask: 0.6552, decode.d2.loss_dice: 0.9584, decode.d3.loss_cls: 0.7073, decode.d3.loss_mask: 0.6498, decode.d3.loss_dice: 0.9547, decode.d4.loss_cls: 0.6839, decode.d4.loss_mask: 0.6508, decode.d4.loss_dice: 0.9540, decode.d5.loss_cls: 0.6733, decode.d5.loss_mask: 0.6483, decode.d5.loss_dice: 0.9506, decode.d6.loss_cls: 0.6652, decode.d6.loss_mask: 0.6454, decode.d6.loss_dice: 0.9486, decode.d7.loss_cls: 0.6602, decode.d7.loss_mask: 0.6461, decode.d7.loss_dice: 0.9501, loss: 29.8190
2022-10-29 11:49:31,677 - mmseg - INFO - Iter [1050/20000]	lr: 2.892e-06, eta: 12:41:13, time: 2.330, data_time: 0.012, memory: 35404, decode.loss_cls: 0.6506, decode.loss_mask: 0.6458, decode.loss_dice: 0.9262, decode.d0.loss_cls: 9.5413, decode.d0.loss_mask: 0.6118, decode.d0.loss_dice: 1.0093, decode.d1.loss_cls: 0.8578, decode.d1.loss_mask: 0.6605, decode.d1.loss_dice: 0.9753, decode.d2.loss_cls: 0.7439, decode.d2.loss_mask: 0.6484, decode.d2.loss_dice: 0.9405, decode.d3.loss_cls: 0.6924, decode.d3.loss_mask: 0.6473, decode.d3.loss_dice: 0.9327, decode.d4.loss_cls: 0.6721, decode.d4.loss_mask: 0.6447, decode.d4.loss_dice: 0.9305, decode.d5.loss_cls: 0.6637, decode.d5.loss_mask: 0.6443, decode.d5.loss_dice: 0.9251, decode.d6.loss_cls: 0.6532, decode.d6.loss_mask: 0.6426, decode.d6.loss_dice: 0.9216, decode.d7.loss_cls: 0.6480, decode.d7.loss_mask: 0.6442, decode.d7.loss_dice: 0.9259, loss: 29.3995
2022-10-29 11:51:31,005 - mmseg - INFO - Iter [1100/20000]	lr: 2.884e-06, eta: 12:38:53, time: 2.387, data_time: 0.013, memory: 35404, decode.loss_cls: 0.6465, decode.loss_mask: 0.6282, decode.loss_dice: 0.9311, decode.d0.loss_cls: 9.4785, decode.d0.loss_mask: 0.5958, decode.d0.loss_dice: 1.0120, decode.d1.loss_cls: 0.8537, decode.d1.loss_mask: 0.6432, decode.d1.loss_dice: 0.9793, decode.d2.loss_cls: 0.7459, decode.d2.loss_mask: 0.6297, decode.d2.loss_dice: 0.9412, decode.d3.loss_cls: 0.6931, decode.d3.loss_mask: 0.6251, decode.d3.loss_dice: 0.9323, decode.d4.loss_cls: 0.6708, decode.d4.loss_mask: 0.6245, decode.d4.loss_dice: 0.9326, decode.d5.loss_cls: 0.6597, decode.d5.loss_mask: 0.6273, decode.d5.loss_dice: 0.9322, decode.d6.loss_cls: 0.6532, decode.d6.loss_mask: 0.6264, decode.d6.loss_dice: 0.9285, decode.d7.loss_cls: 0.6479, decode.d7.loss_mask: 0.6284, decode.d7.loss_dice: 0.9340, loss: 29.2013
2022-10-29 11:53:28,845 - mmseg - INFO - Iter [1150/20000]	lr: 2.877e-06, eta: 12:36:09, time: 2.357, data_time: 0.013, memory: 35404, decode.loss_cls: 0.6473, decode.loss_mask: 0.6428, decode.loss_dice: 0.9306, decode.d0.loss_cls: 9.4127, decode.d0.loss_mask: 0.6154, decode.d0.loss_dice: 1.0103, decode.d1.loss_cls: 0.8435, decode.d1.loss_mask: 0.6596, decode.d1.loss_dice: 0.9840, decode.d2.loss_cls: 0.7384, decode.d2.loss_mask: 0.6460, decode.d2.loss_dice: 0.9441, decode.d3.loss_cls: 0.6855, decode.d3.loss_mask: 0.6430, decode.d3.loss_dice: 0.9334, decode.d4.loss_cls: 0.6648, decode.d4.loss_mask: 0.6430, decode.d4.loss_dice: 0.9393, decode.d5.loss_cls: 0.6600, decode.d5.loss_mask: 0.6401, decode.d5.loss_dice: 0.9310, decode.d6.loss_cls: 0.6532, decode.d6.loss_mask: 0.6409, decode.d6.loss_dice: 0.9249, decode.d7.loss_cls: 0.6450, decode.d7.loss_mask: 0.6421, decode.d7.loss_dice: 0.9329, loss: 29.2538
2022-10-29 11:55:26,970 - mmseg - INFO - Iter [1200/20000]	lr: 2.869e-06, eta: 12:33:34, time: 2.362, data_time: 0.012, memory: 35404, decode.loss_cls: 0.6500, decode.loss_mask: 0.6451, decode.loss_dice: 0.9333, decode.d0.loss_cls: 9.3461, decode.d0.loss_mask: 0.6125, decode.d0.loss_dice: 1.0122, decode.d1.loss_cls: 0.8405, decode.d1.loss_mask: 0.6612, decode.d1.loss_dice: 0.9817, decode.d2.loss_cls: 0.7426, decode.d2.loss_mask: 0.6492, decode.d2.loss_dice: 0.9451, decode.d3.loss_cls: 0.6889, decode.d3.loss_mask: 0.6459, decode.d3.loss_dice: 0.9330, decode.d4.loss_cls: 0.6711, decode.d4.loss_mask: 0.6459, decode.d4.loss_dice: 0.9361, decode.d5.loss_cls: 0.6650, decode.d5.loss_mask: 0.6456, decode.d5.loss_dice: 0.9307, decode.d6.loss_cls: 0.6552, decode.d6.loss_mask: 0.6427, decode.d6.loss_dice: 0.9259, decode.d7.loss_cls: 0.6489, decode.d7.loss_mask: 0.6465, decode.d7.loss_dice: 0.9341, loss: 29.2350
2022-10-29 11:57:27,800 - mmseg - INFO - Iter [1250/20000]	lr: 2.862e-06, eta: 12:31:43, time: 2.417, data_time: 0.012, memory: 35404, decode.loss_cls: 0.6454, decode.loss_mask: 0.6500, decode.loss_dice: 0.9270, decode.d0.loss_cls: 9.2760, decode.d0.loss_mask: 0.6107, decode.d0.loss_dice: 1.0106, decode.d1.loss_cls: 0.8324, decode.d1.loss_mask: 0.6645, decode.d1.loss_dice: 0.9752, decode.d2.loss_cls: 0.7327, decode.d2.loss_mask: 0.6514, decode.d2.loss_dice: 0.9435, decode.d3.loss_cls: 0.6868, decode.d3.loss_mask: 0.6490, decode.d3.loss_dice: 0.9349, decode.d4.loss_cls: 0.6705, decode.d4.loss_mask: 0.6496, decode.d4.loss_dice: 0.9337, decode.d5.loss_cls: 0.6578, decode.d5.loss_mask: 0.6472, decode.d5.loss_dice: 0.9306, decode.d6.loss_cls: 0.6491, decode.d6.loss_mask: 0.6481, decode.d6.loss_dice: 0.9288, decode.d7.loss_cls: 0.6469, decode.d7.loss_mask: 0.6488, decode.d7.loss_dice: 0.9278, loss: 29.1290
2022-10-29 11:59:32,845 - mmseg - INFO - Iter [1300/20000]	lr: 2.854e-06, eta: 12:30:51, time: 2.501, data_time: 0.061, memory: 35404, decode.loss_cls: 0.6227, decode.loss_mask: 0.6435, decode.loss_dice: 0.9340, decode.d0.loss_cls: 9.2042, decode.d0.loss_mask: 0.6109, decode.d0.loss_dice: 1.0057, decode.d1.loss_cls: 0.8138, decode.d1.loss_mask: 0.6540, decode.d1.loss_dice: 0.9740, decode.d2.loss_cls: 0.7134, decode.d2.loss_mask: 0.6428, decode.d2.loss_dice: 0.9440, decode.d3.loss_cls: 0.6594, decode.d3.loss_mask: 0.6392, decode.d3.loss_dice: 0.9306, decode.d4.loss_cls: 0.6427, decode.d4.loss_mask: 0.6392, decode.d4.loss_dice: 0.9314, decode.d5.loss_cls: 0.6364, decode.d5.loss_mask: 0.6403, decode.d5.loss_dice: 0.9292, decode.d6.loss_cls: 0.6272, decode.d6.loss_mask: 0.6385, decode.d6.loss_dice: 0.9252, decode.d7.loss_cls: 0.6254, decode.d7.loss_mask: 0.6413, decode.d7.loss_dice: 0.9288, loss: 28.7976
2022-10-29 12:01:33,579 - mmseg - INFO - Iter [1350/20000]	lr: 2.846e-06, eta: 12:28:54, time: 2.415, data_time: 0.013, memory: 35404, decode.loss_cls: 0.6271, decode.loss_mask: 0.6339, decode.loss_dice: 0.9183, decode.d0.loss_cls: 9.1295, decode.d0.loss_mask: 0.6060, decode.d0.loss_dice: 0.9996, decode.d1.loss_cls: 0.8073, decode.d1.loss_mask: 0.6502, decode.d1.loss_dice: 0.9685, decode.d2.loss_cls: 0.7113, decode.d2.loss_mask: 0.6399, decode.d2.loss_dice: 0.9359, decode.d3.loss_cls: 0.6670, decode.d3.loss_mask: 0.6368, decode.d3.loss_dice: 0.9265, decode.d4.loss_cls: 0.6501, decode.d4.loss_mask: 0.6346, decode.d4.loss_dice: 0.9248, decode.d5.loss_cls: 0.6403, decode.d5.loss_mask: 0.6327, decode.d5.loss_dice: 0.9198, decode.d6.loss_cls: 0.6327, decode.d6.loss_mask: 0.6313, decode.d6.loss_dice: 0.9157, decode.d7.loss_cls: 0.6287, decode.d7.loss_mask: 0.6318, decode.d7.loss_dice: 0.9209, loss: 28.6212
2022-10-29 12:03:29,755 - mmseg - INFO - Iter [1400/20000]	lr: 2.839e-06, eta: 12:25:57, time: 2.323, data_time: 0.020, memory: 35404, decode.loss_cls: 0.6153, decode.loss_mask: 0.6263, decode.loss_dice: 0.9193, decode.d0.loss_cls: 9.0610, decode.d0.loss_mask: 0.5962, decode.d0.loss_dice: 0.9982, decode.d1.loss_cls: 0.7927, decode.d1.loss_mask: 0.6411, decode.d1.loss_dice: 0.9700, decode.d2.loss_cls: 0.6965, decode.d2.loss_mask: 0.6304, decode.d2.loss_dice: 0.9350, decode.d3.loss_cls: 0.6508, decode.d3.loss_mask: 0.6259, decode.d3.loss_dice: 0.9242, decode.d4.loss_cls: 0.6346, decode.d4.loss_mask: 0.6257, decode.d4.loss_dice: 0.9240, decode.d5.loss_cls: 0.6281, decode.d5.loss_mask: 0.6234, decode.d5.loss_dice: 0.9181, decode.d6.loss_cls: 0.6238, decode.d6.loss_mask: 0.6237, decode.d6.loss_dice: 0.9157, decode.d7.loss_cls: 0.6166, decode.d7.loss_mask: 0.6248, decode.d7.loss_dice: 0.9194, loss: 28.3608
2022-10-29 12:05:28,103 - mmseg - INFO - Iter [1450/20000]	lr: 2.831e-06, eta: 12:23:31, time: 2.367, data_time: 0.013, memory: 35404, decode.loss_cls: 0.6007, decode.loss_mask: 0.6376, decode.loss_dice: 0.9309, decode.d0.loss_cls: 8.9886, decode.d0.loss_mask: 0.6007, decode.d0.loss_dice: 0.9971, decode.d1.loss_cls: 0.7700, decode.d1.loss_mask: 0.6514, decode.d1.loss_dice: 0.9788, decode.d2.loss_cls: 0.6774, decode.d2.loss_mask: 0.6401, decode.d2.loss_dice: 0.9445, decode.d3.loss_cls: 0.6334, decode.d3.loss_mask: 0.6350, decode.d3.loss_dice: 0.9349, decode.d4.loss_cls: 0.6169, decode.d4.loss_mask: 0.6369, decode.d4.loss_dice: 0.9302, decode.d5.loss_cls: 0.6123, decode.d5.loss_mask: 0.6354, decode.d5.loss_dice: 0.9300, decode.d6.loss_cls: 0.6041, decode.d6.loss_mask: 0.6355, decode.d6.loss_dice: 0.9296, decode.d7.loss_cls: 0.5984, decode.d7.loss_mask: 0.6370, decode.d7.loss_dice: 0.9313, loss: 28.3190
2022-10-29 12:07:29,249 - mmseg - INFO - Iter [1500/20000]	lr: 2.823e-06, eta: 12:21:42, time: 2.423, data_time: 0.013, memory: 35404, decode.loss_cls: 0.5943, decode.loss_mask: 0.6437, decode.loss_dice: 0.9268, decode.d0.loss_cls: 8.9134, decode.d0.loss_mask: 0.6122, decode.d0.loss_dice: 1.0014, decode.d1.loss_cls: 0.7619, decode.d1.loss_mask: 0.6586, decode.d1.loss_dice: 0.9805, decode.d2.loss_cls: 0.6718, decode.d2.loss_mask: 0.6493, decode.d2.loss_dice: 0.9424, decode.d3.loss_cls: 0.6274, decode.d3.loss_mask: 0.6442, decode.d3.loss_dice: 0.9305, decode.d4.loss_cls: 0.6161, decode.d4.loss_mask: 0.6450, decode.d4.loss_dice: 0.9274, decode.d5.loss_cls: 0.6026, decode.d5.loss_mask: 0.6435, decode.d5.loss_dice: 0.9261, decode.d6.loss_cls: 0.6009, decode.d6.loss_mask: 0.6408, decode.d6.loss_dice: 0.9196, decode.d7.loss_cls: 0.5963, decode.d7.loss_mask: 0.6421, decode.d7.loss_dice: 0.9225, loss: 28.2414
2022-10-29 12:09:24,786 - mmseg - INFO - Iter [1550/20000]	lr: 2.816e-06, eta: 12:18:45, time: 2.311, data_time: 0.013, memory: 35404, decode.loss_cls: 0.5942, decode.loss_mask: 0.6400, decode.loss_dice: 0.9286, decode.d0.loss_cls: 8.8378, decode.d0.loss_mask: 0.6026, decode.d0.loss_dice: 0.9991, decode.d1.loss_cls: 0.7538, decode.d1.loss_mask: 0.6531, decode.d1.loss_dice: 0.9754, decode.d2.loss_cls: 0.6661, decode.d2.loss_mask: 0.6394, decode.d2.loss_dice: 0.9396, decode.d3.loss_cls: 0.6220, decode.d3.loss_mask: 0.6362, decode.d3.loss_dice: 0.9314, decode.d4.loss_cls: 0.6084, decode.d4.loss_mask: 0.6384, decode.d4.loss_dice: 0.9295, decode.d5.loss_cls: 0.5989, decode.d5.loss_mask: 0.6384, decode.d5.loss_dice: 0.9285, decode.d6.loss_cls: 0.5927, decode.d6.loss_mask: 0.6391, decode.d6.loss_dice: 0.9271, decode.d7.loss_cls: 0.5903, decode.d7.loss_mask: 0.6405, decode.d7.loss_dice: 0.9290, loss: 28.0799
2022-10-29 12:11:24,622 - mmseg - INFO - Iter [1600/20000]	lr: 2.808e-06, eta: 12:16:42, time: 2.397, data_time: 0.059, memory: 35404, decode.loss_cls: 0.5818, decode.loss_mask: 0.6226, decode.loss_dice: 0.9051, decode.d0.loss_cls: 8.7636, decode.d0.loss_mask: 0.5896, decode.d0.loss_dice: 0.9754, decode.d1.loss_cls: 0.7427, decode.d1.loss_mask: 0.6398, decode.d1.loss_dice: 0.9577, decode.d2.loss_cls: 0.6585, decode.d2.loss_mask: 0.6255, decode.d2.loss_dice: 0.9178, decode.d3.loss_cls: 0.6155, decode.d3.loss_mask: 0.6227, decode.d3.loss_dice: 0.9082, decode.d4.loss_cls: 0.5987, decode.d4.loss_mask: 0.6215, decode.d4.loss_dice: 0.9099, decode.d5.loss_cls: 0.5894, decode.d5.loss_mask: 0.6196, decode.d5.loss_dice: 0.9064, decode.d6.loss_cls: 0.5848, decode.d6.loss_mask: 0.6204, decode.d6.loss_dice: 0.9007, decode.d7.loss_cls: 0.5812, decode.d7.loss_mask: 0.6210, decode.d7.loss_dice: 0.9057, loss: 27.5859
2022-10-29 12:13:26,429 - mmseg - INFO - Iter [1650/20000]	lr: 2.801e-06, eta: 12:15:01, time: 2.436, data_time: 0.012, memory: 35404, decode.loss_cls: 0.5699, decode.loss_mask: 0.6205, decode.loss_dice: 0.9057, decode.d0.loss_cls: 8.6766, decode.d0.loss_mask: 0.5880, decode.d0.loss_dice: 0.9763, decode.d1.loss_cls: 0.7306, decode.d1.loss_mask: 0.6386, decode.d1.loss_dice: 0.9575, decode.d2.loss_cls: 0.6428, decode.d2.loss_mask: 0.6267, decode.d2.loss_dice: 0.9202, decode.d3.loss_cls: 0.6000, decode.d3.loss_mask: 0.6206, decode.d3.loss_dice: 0.9074, decode.d4.loss_cls: 0.5857, decode.d4.loss_mask: 0.6206, decode.d4.loss_dice: 0.9080, decode.d5.loss_cls: 0.5787, decode.d5.loss_mask: 0.6194, decode.d5.loss_dice: 0.9025, decode.d6.loss_cls: 0.5709, decode.d6.loss_mask: 0.6213, decode.d6.loss_dice: 0.9062, decode.d7.loss_cls: 0.5663, decode.d7.loss_mask: 0.6211, decode.d7.loss_dice: 0.9074, loss: 27.3895
2022-10-29 12:15:22,032 - mmseg - INFO - Iter [1700/20000]	lr: 2.793e-06, eta: 12:12:11, time: 2.312, data_time: 0.012, memory: 35404, decode.loss_cls: 0.5649, decode.loss_mask: 0.6289, decode.loss_dice: 0.9066, decode.d0.loss_cls: 8.6056, decode.d0.loss_mask: 0.5993, decode.d0.loss_dice: 0.9812, decode.d1.loss_cls: 0.7164, decode.d1.loss_mask: 0.6451, decode.d1.loss_dice: 0.9593, decode.d2.loss_cls: 0.6358, decode.d2.loss_mask: 0.6335, decode.d2.loss_dice: 0.9206, decode.d3.loss_cls: 0.5908, decode.d3.loss_mask: 0.6283, decode.d3.loss_dice: 0.9104, decode.d4.loss_cls: 0.5772, decode.d4.loss_mask: 0.6270, decode.d4.loss_dice: 0.9101, decode.d5.loss_cls: 0.5731, decode.d5.loss_mask: 0.6284, decode.d5.loss_dice: 0.9077, decode.d6.loss_cls: 0.5672, decode.d6.loss_mask: 0.6249, decode.d6.loss_dice: 0.9046, decode.d7.loss_cls: 0.5628, decode.d7.loss_mask: 0.6276, decode.d7.loss_dice: 0.9070, loss: 27.3442
2022-10-29 12:17:16,301 - mmseg - INFO - Iter [1750/20000]	lr: 2.785e-06, eta: 12:09:11, time: 2.285, data_time: 0.013, memory: 35404, decode.loss_cls: 0.5806, decode.loss_mask: 0.6298, decode.loss_dice: 0.9113, decode.d0.loss_cls: 8.5247, decode.d0.loss_mask: 0.6001, decode.d0.loss_dice: 0.9875, decode.d1.loss_cls: 0.7299, decode.d1.loss_mask: 0.6455, decode.d1.loss_dice: 0.9640, decode.d2.loss_cls: 0.6479, decode.d2.loss_mask: 0.6309, decode.d2.loss_dice: 0.9260, decode.d3.loss_cls: 0.6097, decode.d3.loss_mask: 0.6279, decode.d3.loss_dice: 0.9109, decode.d4.loss_cls: 0.5954, decode.d4.loss_mask: 0.6275, decode.d4.loss_dice: 0.9145, decode.d5.loss_cls: 0.5921, decode.d5.loss_mask: 0.6248, decode.d5.loss_dice: 0.9098, decode.d6.loss_cls: 0.5852, decode.d6.loss_mask: 0.6259, decode.d6.loss_dice: 0.9043, decode.d7.loss_cls: 0.5833, decode.d7.loss_mask: 0.6265, decode.d7.loss_dice: 0.9074, loss: 27.4235
2022-10-29 12:19:18,715 - mmseg - INFO - Iter [1800/20000]	lr: 2.778e-06, eta: 12:07:37, time: 2.448, data_time: 0.013, memory: 35404, decode.loss_cls: 0.5685, decode.loss_mask: 0.6223, decode.loss_dice: 0.9101, decode.d0.loss_cls: 8.4440, decode.d0.loss_mask: 0.5925, decode.d0.loss_dice: 0.9750, decode.d1.loss_cls: 0.7201, decode.d1.loss_mask: 0.6397, decode.d1.loss_dice: 0.9638, decode.d2.loss_cls: 0.6345, decode.d2.loss_mask: 0.6273, decode.d2.loss_dice: 0.9293, decode.d3.loss_cls: 0.5947, decode.d3.loss_mask: 0.6240, decode.d3.loss_dice: 0.9132, decode.d4.loss_cls: 0.5859, decode.d4.loss_mask: 0.6237, decode.d4.loss_dice: 0.9132, decode.d5.loss_cls: 0.5746, decode.d5.loss_mask: 0.6222, decode.d5.loss_dice: 0.9106, decode.d6.loss_cls: 0.5679, decode.d6.loss_mask: 0.6203, decode.d6.loss_dice: 0.9075, decode.d7.loss_cls: 0.5675, decode.d7.loss_mask: 0.6218, decode.d7.loss_dice: 0.9081, loss: 27.1823
2022-10-29 12:21:15,195 - mmseg - INFO - Iter [1850/20000]	lr: 2.770e-06, eta: 12:05:03, time: 2.330, data_time: 0.017, memory: 35410, decode.loss_cls: 0.5747, decode.loss_mask: 0.6323, decode.loss_dice: 0.9037, decode.d0.loss_cls: 8.3636, decode.d0.loss_mask: 0.6039, decode.d0.loss_dice: 0.9792, decode.d1.loss_cls: 0.7230, decode.d1.loss_mask: 0.6457, decode.d1.loss_dice: 0.9556, decode.d2.loss_cls: 0.6401, decode.d2.loss_mask: 0.6358, decode.d2.loss_dice: 0.9203, decode.d3.loss_cls: 0.6002, decode.d3.loss_mask: 0.6330, decode.d3.loss_dice: 0.9069, decode.d4.loss_cls: 0.5871, decode.d4.loss_mask: 0.6317, decode.d4.loss_dice: 0.9083, decode.d5.loss_cls: 0.5800, decode.d5.loss_mask: 0.6338, decode.d5.loss_dice: 0.9075, decode.d6.loss_cls: 0.5760, decode.d6.loss_mask: 0.6316, decode.d6.loss_dice: 0.9032, decode.d7.loss_cls: 0.5707, decode.d7.loss_mask: 0.6327, decode.d7.loss_dice: 0.9027, loss: 27.1833
2022-10-29 12:23:12,021 - mmseg - INFO - Iter [1900/20000]	lr: 2.762e-06, eta: 12:02:35, time: 2.336, data_time: 0.058, memory: 35410, decode.loss_cls: 0.5624, decode.loss_mask: 0.6338, decode.loss_dice: 0.9041, decode.d0.loss_cls: 8.2843, decode.d0.loss_mask: 0.6040, decode.d0.loss_dice: 0.9704, decode.d1.loss_cls: 0.7035, decode.d1.loss_mask: 0.6527, decode.d1.loss_dice: 0.9564, decode.d2.loss_cls: 0.6293, decode.d2.loss_mask: 0.6420, decode.d2.loss_dice: 0.9201, decode.d3.loss_cls: 0.5885, decode.d3.loss_mask: 0.6375, decode.d3.loss_dice: 0.9072, decode.d4.loss_cls: 0.5769, decode.d4.loss_mask: 0.6343, decode.d4.loss_dice: 0.9111, decode.d5.loss_cls: 0.5697, decode.d5.loss_mask: 0.6335, decode.d5.loss_dice: 0.9048, decode.d6.loss_cls: 0.5637, decode.d6.loss_mask: 0.6342, decode.d6.loss_dice: 0.9002, decode.d7.loss_cls: 0.5623, decode.d7.loss_mask: 0.6344, decode.d7.loss_dice: 0.9052, loss: 27.0265
2022-10-29 12:25:14,594 - mmseg - INFO - Iter [1950/20000]	lr: 2.755e-06, eta: 12:01:01, time: 2.452, data_time: 0.013, memory: 35410, decode.loss_cls: 0.5401, decode.loss_mask: 0.6190, decode.loss_dice: 0.8947, decode.d0.loss_cls: 8.1921, decode.d0.loss_mask: 0.5911, decode.d0.loss_dice: 0.9691, decode.d1.loss_cls: 0.6840, decode.d1.loss_mask: 0.6348, decode.d1.loss_dice: 0.9538, decode.d2.loss_cls: 0.6078, decode.d2.loss_mask: 0.6232, decode.d2.loss_dice: 0.9127, decode.d3.loss_cls: 0.5627, decode.d3.loss_mask: 0.6173, decode.d3.loss_dice: 0.9010, decode.d4.loss_cls: 0.5531, decode.d4.loss_mask: 0.6187, decode.d4.loss_dice: 0.9006, decode.d5.loss_cls: 0.5462, decode.d5.loss_mask: 0.6167, decode.d5.loss_dice: 0.8978, decode.d6.loss_cls: 0.5399, decode.d6.loss_mask: 0.6183, decode.d6.loss_dice: 0.8966, decode.d7.loss_cls: 0.5402, decode.d7.loss_mask: 0.6189, decode.d7.loss_dice: 0.8956, loss: 26.5458
2022-10-29 12:27:11,093 - mmseg - INFO - Saving checkpoint at 2000 iterations
2022-10-29 12:27:43,424 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-29 12:27:43,425 - mmseg - INFO - Iter [2000/20000]	lr: 2.747e-06, eta: 12:03:22, time: 2.977, data_time: 0.012, memory: 35410, decode.loss_cls: 0.5268, decode.loss_mask: 0.6297, decode.loss_dice: 0.8964, decode.d0.loss_cls: 8.1064, decode.d0.loss_mask: 0.5957, decode.d0.loss_dice: 0.9571, decode.d1.loss_cls: 0.6559, decode.d1.loss_mask: 0.6462, decode.d1.loss_dice: 0.9517, decode.d2.loss_cls: 0.5860, decode.d2.loss_mask: 0.6357, decode.d2.loss_dice: 0.9092, decode.d3.loss_cls: 0.5518, decode.d3.loss_mask: 0.6268, decode.d3.loss_dice: 0.8955, decode.d4.loss_cls: 0.5388, decode.d4.loss_mask: 0.6257, decode.d4.loss_dice: 0.8933, decode.d5.loss_cls: 0.5324, decode.d5.loss_mask: 0.6273, decode.d5.loss_dice: 0.8922, decode.d6.loss_cls: 0.5274, decode.d6.loss_mask: 0.6269, decode.d6.loss_dice: 0.8923, decode.d7.loss_cls: 0.5227, decode.d7.loss_mask: 0.6291, decode.d7.loss_dice: 0.8952, loss: 26.3743
2022-10-29 12:29:45,818 - mmseg - INFO - Iter [2050/20000]	lr: 2.739e-06, eta: 12:01:37, time: 2.448, data_time: 0.013, memory: 35410, decode.loss_cls: 0.5364, decode.loss_mask: 0.6164, decode.loss_dice: 0.8878, decode.d0.loss_cls: 8.0321, decode.d0.loss_mask: 0.5844, decode.d0.loss_dice: 0.9568, decode.d1.loss_cls: 0.6781, decode.d1.loss_mask: 0.6358, decode.d1.loss_dice: 0.9404, decode.d2.loss_cls: 0.6023, decode.d2.loss_mask: 0.6247, decode.d2.loss_dice: 0.9098, decode.d3.loss_cls: 0.5644, decode.d3.loss_mask: 0.6197, decode.d3.loss_dice: 0.8931, decode.d4.loss_cls: 0.5547, decode.d4.loss_mask: 0.6179, decode.d4.loss_dice: 0.8920, decode.d5.loss_cls: 0.5454, decode.d5.loss_mask: 0.6174, decode.d5.loss_dice: 0.8915, decode.d6.loss_cls: 0.5416, decode.d6.loss_mask: 0.6159, decode.d6.loss_dice: 0.8890, decode.d7.loss_cls: 0.5349, decode.d7.loss_mask: 0.6178, decode.d7.loss_dice: 0.8908, loss: 26.2913
2022-10-29 12:31:41,845 - mmseg - INFO - Iter [2100/20000]	lr: 2.732e-06, eta: 11:58:58, time: 2.321, data_time: 0.013, memory: 35410, decode.loss_cls: 0.5667, decode.loss_mask: 0.6173, decode.loss_dice: 0.9038, decode.d0.loss_cls: 7.9481, decode.d0.loss_mask: 0.5913, decode.d0.loss_dice: 0.9669, decode.d1.loss_cls: 0.6990, decode.d1.loss_mask: 0.6368, decode.d1.loss_dice: 0.9537, decode.d2.loss_cls: 0.6314, decode.d2.loss_mask: 0.6229, decode.d2.loss_dice: 0.9177, decode.d3.loss_cls: 0.5931, decode.d3.loss_mask: 0.6193, decode.d3.loss_dice: 0.9037, decode.d4.loss_cls: 0.5812, decode.d4.loss_mask: 0.6201, decode.d4.loss_dice: 0.9054, decode.d5.loss_cls: 0.5741, decode.d5.loss_mask: 0.6165, decode.d5.loss_dice: 0.9073, decode.d6.loss_cls: 0.5689, decode.d6.loss_mask: 0.6182, decode.d6.loss_dice: 0.8984, decode.d7.loss_cls: 0.5638, decode.d7.loss_mask: 0.6197, decode.d7.loss_dice: 0.8994, loss: 26.5448
2022-10-29 12:33:40,511 - mmseg - INFO - Iter [2150/20000]	lr: 2.724e-06, eta: 11:56:42, time: 2.373, data_time: 0.018, memory: 35410, decode.loss_cls: 0.5459, decode.loss_mask: 0.6213, decode.loss_dice: 0.9054, decode.d0.loss_cls: 7.8553, decode.d0.loss_mask: 0.5923, decode.d0.loss_dice: 0.9759, decode.d1.loss_cls: 0.6725, decode.d1.loss_mask: 0.6405, decode.d1.loss_dice: 0.9636, decode.d2.loss_cls: 0.6047, decode.d2.loss_mask: 0.6281, decode.d2.loss_dice: 0.9209, decode.d3.loss_cls: 0.5668, decode.d3.loss_mask: 0.6236, decode.d3.loss_dice: 0.9096, decode.d4.loss_cls: 0.5591, decode.d4.loss_mask: 0.6231, decode.d4.loss_dice: 0.9086, decode.d5.loss_cls: 0.5515, decode.d5.loss_mask: 0.6215, decode.d5.loss_dice: 0.9082, decode.d6.loss_cls: 0.5472, decode.d6.loss_mask: 0.6203, decode.d6.loss_dice: 0.9074, decode.d7.loss_cls: 0.5459, decode.d7.loss_mask: 0.6212, decode.d7.loss_dice: 0.9090, loss: 26.3494
2022-10-29 12:35:40,863 - mmseg - INFO - Iter [2200/20000]	lr: 2.717e-06, eta: 11:54:41, time: 2.408, data_time: 0.014, memory: 35410, decode.loss_cls: 0.5483, decode.loss_mask: 0.6194, decode.loss_dice: 0.8921, decode.d0.loss_cls: 7.7732, decode.d0.loss_mask: 0.5877, decode.d0.loss_dice: 0.9619, decode.d1.loss_cls: 0.6754, decode.d1.loss_mask: 0.6317, decode.d1.loss_dice: 0.9450, decode.d2.loss_cls: 0.6076, decode.d2.loss_mask: 0.6219, decode.d2.loss_dice: 0.9050, decode.d3.loss_cls: 0.5714, decode.d3.loss_mask: 0.6177, decode.d3.loss_dice: 0.8942, decode.d4.loss_cls: 0.5624, decode.d4.loss_mask: 0.6167, decode.d4.loss_dice: 0.8952, decode.d5.loss_cls: 0.5546, decode.d5.loss_mask: 0.6162, decode.d5.loss_dice: 0.8946, decode.d6.loss_cls: 0.5522, decode.d6.loss_mask: 0.6162, decode.d6.loss_dice: 0.8898, decode.d7.loss_cls: 0.5456, decode.d7.loss_mask: 0.6169, decode.d7.loss_dice: 0.8914, loss: 26.1044
2022-10-29 12:37:41,888 - mmseg - INFO - Iter [2250/20000]	lr: 2.709e-06, eta: 11:52:45, time: 2.420, data_time: 0.056, memory: 35410, decode.loss_cls: 0.5259, decode.loss_mask: 0.6088, decode.loss_dice: 0.8894, decode.d0.loss_cls: 7.6800, decode.d0.loss_mask: 0.5765, decode.d0.loss_dice: 0.9481, decode.d1.loss_cls: 0.6554, decode.d1.loss_mask: 0.6265, decode.d1.loss_dice: 0.9364, decode.d2.loss_cls: 0.5808, decode.d2.loss_mask: 0.6137, decode.d2.loss_dice: 0.9021, decode.d3.loss_cls: 0.5505, decode.d3.loss_mask: 0.6100, decode.d3.loss_dice: 0.8878, decode.d4.loss_cls: 0.5409, decode.d4.loss_mask: 0.6079, decode.d4.loss_dice: 0.8872, decode.d5.loss_cls: 0.5335, decode.d5.loss_mask: 0.6076, decode.d5.loss_dice: 0.8870, decode.d6.loss_cls: 0.5283, decode.d6.loss_mask: 0.6083, decode.d6.loss_dice: 0.8826, decode.d7.loss_cls: 0.5261, decode.d7.loss_mask: 0.6084, decode.d7.loss_dice: 0.8888, loss: 25.6983
2022-10-29 12:39:45,026 - mmseg - INFO - Iter [2300/20000]	lr: 2.701e-06, eta: 11:51:05, time: 2.463, data_time: 0.013, memory: 35410, decode.loss_cls: 0.5294, decode.loss_mask: 0.6151, decode.loss_dice: 0.8941, decode.d0.loss_cls: 7.5950, decode.d0.loss_mask: 0.5825, decode.d0.loss_dice: 0.9616, decode.d1.loss_cls: 0.6621, decode.d1.loss_mask: 0.6275, decode.d1.loss_dice: 0.9427, decode.d2.loss_cls: 0.5878, decode.d2.loss_mask: 0.6203, decode.d2.loss_dice: 0.9098, decode.d3.loss_cls: 0.5602, decode.d3.loss_mask: 0.6149, decode.d3.loss_dice: 0.8986, decode.d4.loss_cls: 0.5471, decode.d4.loss_mask: 0.6142, decode.d4.loss_dice: 0.8957, decode.d5.loss_cls: 0.5413, decode.d5.loss_mask: 0.6115, decode.d5.loss_dice: 0.8956, decode.d6.loss_cls: 0.5341, decode.d6.loss_mask: 0.6147, decode.d6.loss_dice: 0.8925, decode.d7.loss_cls: 0.5294, decode.d7.loss_mask: 0.6149, decode.d7.loss_dice: 0.8948, loss: 25.7875
2022-10-29 12:41:47,954 - mmseg - INFO - Iter [2350/20000]	lr: 2.694e-06, eta: 11:49:22, time: 2.459, data_time: 0.012, memory: 35410, decode.loss_cls: 0.5161, decode.loss_mask: 0.6131, decode.loss_dice: 0.8865, decode.d0.loss_cls: 7.5103, decode.d0.loss_mask: 0.5842, decode.d0.loss_dice: 0.9454, decode.d1.loss_cls: 0.6434, decode.d1.loss_mask: 0.6278, decode.d1.loss_dice: 0.9281, decode.d2.loss_cls: 0.5725, decode.d2.loss_mask: 0.6159, decode.d2.loss_dice: 0.8980, decode.d3.loss_cls: 0.5369, decode.d3.loss_mask: 0.6115, decode.d3.loss_dice: 0.8873, decode.d4.loss_cls: 0.5271, decode.d4.loss_mask: 0.6122, decode.d4.loss_dice: 0.8869, decode.d5.loss_cls: 0.5251, decode.d5.loss_mask: 0.6108, decode.d5.loss_dice: 0.8865, decode.d6.loss_cls: 0.5205, decode.d6.loss_mask: 0.6102, decode.d6.loss_dice: 0.8794, decode.d7.loss_cls: 0.5164, decode.d7.loss_mask: 0.6122, decode.d7.loss_dice: 0.8869, loss: 25.4514
2022-10-29 12:43:51,630 - mmseg - INFO - Iter [2400/20000]	lr: 2.686e-06, eta: 11:47:45, time: 2.474, data_time: 0.013, memory: 35410, decode.loss_cls: 0.5276, decode.loss_mask: 0.5990, decode.loss_dice: 0.8791, decode.d0.loss_cls: 7.4178, decode.d0.loss_mask: 0.5715, decode.d0.loss_dice: 0.9497, decode.d1.loss_cls: 0.6573, decode.d1.loss_mask: 0.6142, decode.d1.loss_dice: 0.9288, decode.d2.loss_cls: 0.5847, decode.d2.loss_mask: 0.6049, decode.d2.loss_dice: 0.8972, decode.d3.loss_cls: 0.5488, decode.d3.loss_mask: 0.5988, decode.d3.loss_dice: 0.8836, decode.d4.loss_cls: 0.5444, decode.d4.loss_mask: 0.5982, decode.d4.loss_dice: 0.8803, decode.d5.loss_cls: 0.5360, decode.d5.loss_mask: 0.5959, decode.d5.loss_dice: 0.8774, decode.d6.loss_cls: 0.5334, decode.d6.loss_mask: 0.5975, decode.d6.loss_dice: 0.8737, decode.d7.loss_cls: 0.5286, decode.d7.loss_mask: 0.5981, decode.d7.loss_dice: 0.8789, loss: 25.3054
2022-10-29 12:45:51,209 - mmseg - INFO - Iter [2450/20000]	lr: 2.678e-06, eta: 11:45:36, time: 2.392, data_time: 0.012, memory: 35410, decode.loss_cls: 0.5273, decode.loss_mask: 0.6035, decode.loss_dice: 0.8813, decode.d0.loss_cls: 7.3375, decode.d0.loss_mask: 0.5789, decode.d0.loss_dice: 0.9521, decode.d1.loss_cls: 0.6501, decode.d1.loss_mask: 0.6204, decode.d1.loss_dice: 0.9316, decode.d2.loss_cls: 0.5789, decode.d2.loss_mask: 0.6088, decode.d2.loss_dice: 0.9032, decode.d3.loss_cls: 0.5477, decode.d3.loss_mask: 0.6021, decode.d3.loss_dice: 0.8906, decode.d4.loss_cls: 0.5418, decode.d4.loss_mask: 0.6033, decode.d4.loss_dice: 0.8878, decode.d5.loss_cls: 0.5362, decode.d5.loss_mask: 0.6018, decode.d5.loss_dice: 0.8857, decode.d6.loss_cls: 0.5270, decode.d6.loss_mask: 0.6016, decode.d6.loss_dice: 0.8795, decode.d7.loss_cls: 0.5283, decode.d7.loss_mask: 0.6016, decode.d7.loss_dice: 0.8807, loss: 25.2890
2022-10-29 12:47:49,167 - mmseg - INFO - Iter [2500/20000]	lr: 2.671e-06, eta: 11:43:17, time: 2.359, data_time: 0.022, memory: 35410, decode.loss_cls: 0.5142, decode.loss_mask: 0.6130, decode.loss_dice: 0.8897, decode.d0.loss_cls: 7.2442, decode.d0.loss_mask: 0.5838, decode.d0.loss_dice: 0.9497, decode.d1.loss_cls: 0.6375, decode.d1.loss_mask: 0.6268, decode.d1.loss_dice: 0.9353, decode.d2.loss_cls: 0.5729, decode.d2.loss_mask: 0.6158, decode.d2.loss_dice: 0.9005, decode.d3.loss_cls: 0.5353, decode.d3.loss_mask: 0.6109, decode.d3.loss_dice: 0.8874, decode.d4.loss_cls: 0.5293, decode.d4.loss_mask: 0.6132, decode.d4.loss_dice: 0.8872, decode.d5.loss_cls: 0.5208, decode.d5.loss_mask: 0.6122, decode.d5.loss_dice: 0.8861, decode.d6.loss_cls: 0.5155, decode.d6.loss_mask: 0.6105, decode.d6.loss_dice: 0.8828, decode.d7.loss_cls: 0.5133, decode.d7.loss_mask: 0.6129, decode.d7.loss_dice: 0.8873, loss: 25.1882
2022-10-29 12:49:48,606 - mmseg - INFO - Iter [2550/20000]	lr: 2.663e-06, eta: 11:41:08, time: 2.388, data_time: 0.057, memory: 35410, decode.loss_cls: 0.5188, decode.loss_mask: 0.6135, decode.loss_dice: 0.8955, decode.d0.loss_cls: 7.1552, decode.d0.loss_mask: 0.5858, decode.d0.loss_dice: 0.9623, decode.d1.loss_cls: 0.6406, decode.d1.loss_mask: 0.6301, decode.d1.loss_dice: 0.9494, decode.d2.loss_cls: 0.5725, decode.d2.loss_mask: 0.6223, decode.d2.loss_dice: 0.9163, decode.d3.loss_cls: 0.5401, decode.d3.loss_mask: 0.6166, decode.d3.loss_dice: 0.9030, decode.d4.loss_cls: 0.5322, decode.d4.loss_mask: 0.6136, decode.d4.loss_dice: 0.9018, decode.d5.loss_cls: 0.5241, decode.d5.loss_mask: 0.6124, decode.d5.loss_dice: 0.9006, decode.d6.loss_cls: 0.5203, decode.d6.loss_mask: 0.6116, decode.d6.loss_dice: 0.8918, decode.d7.loss_cls: 0.5204, decode.d7.loss_mask: 0.6139, decode.d7.loss_dice: 0.8976, loss: 25.2622
2022-10-29 12:51:51,922 - mmseg - INFO - Iter [2600/20000]	lr: 2.656e-06, eta: 11:39:27, time: 2.467, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4994, decode.loss_mask: 0.6019, decode.loss_dice: 0.8738, decode.d0.loss_cls: 7.0616, decode.d0.loss_mask: 0.5798, decode.d0.loss_dice: 0.9433, decode.d1.loss_cls: 0.6120, decode.d1.loss_mask: 0.6190, decode.d1.loss_dice: 0.9324, decode.d2.loss_cls: 0.5482, decode.d2.loss_mask: 0.6086, decode.d2.loss_dice: 0.8973, decode.d3.loss_cls: 0.5200, decode.d3.loss_mask: 0.6037, decode.d3.loss_dice: 0.8807, decode.d4.loss_cls: 0.5118, decode.d4.loss_mask: 0.6019, decode.d4.loss_dice: 0.8777, decode.d5.loss_cls: 0.5071, decode.d5.loss_mask: 0.6008, decode.d5.loss_dice: 0.8770, decode.d6.loss_cls: 0.5017, decode.d6.loss_mask: 0.5991, decode.d6.loss_dice: 0.8705, decode.d7.loss_cls: 0.5007, decode.d7.loss_mask: 0.6022, decode.d7.loss_dice: 0.8740, loss: 24.7060
2022-10-29 12:53:52,452 - mmseg - INFO - Iter [2650/20000]	lr: 2.648e-06, eta: 11:37:26, time: 2.411, data_time: 0.012, memory: 35422, decode.loss_cls: 0.5086, decode.loss_mask: 0.5982, decode.loss_dice: 0.8735, decode.d0.loss_cls: 6.9747, decode.d0.loss_mask: 0.5745, decode.d0.loss_dice: 0.9390, decode.d1.loss_cls: 0.6228, decode.d1.loss_mask: 0.6157, decode.d1.loss_dice: 0.9246, decode.d2.loss_cls: 0.5591, decode.d2.loss_mask: 0.6048, decode.d2.loss_dice: 0.8945, decode.d3.loss_cls: 0.5274, decode.d3.loss_mask: 0.5975, decode.d3.loss_dice: 0.8789, decode.d4.loss_cls: 0.5205, decode.d4.loss_mask: 0.5971, decode.d4.loss_dice: 0.8768, decode.d5.loss_cls: 0.5117, decode.d5.loss_mask: 0.5978, decode.d5.loss_dice: 0.8793, decode.d6.loss_cls: 0.5101, decode.d6.loss_mask: 0.5950, decode.d6.loss_dice: 0.8720, decode.d7.loss_cls: 0.5079, decode.d7.loss_mask: 0.5972, decode.d7.loss_dice: 0.8732, loss: 24.6325
2022-10-29 12:55:47,981 - mmseg - INFO - Iter [2700/20000]	lr: 2.640e-06, eta: 11:34:53, time: 2.311, data_time: 0.012, memory: 35422, decode.loss_cls: 0.5130, decode.loss_mask: 0.6101, decode.loss_dice: 0.8938, decode.d0.loss_cls: 6.8967, decode.d0.loss_mask: 0.5861, decode.d0.loss_dice: 0.9598, decode.d1.loss_cls: 0.6256, decode.d1.loss_mask: 0.6267, decode.d1.loss_dice: 0.9537, decode.d2.loss_cls: 0.5599, decode.d2.loss_mask: 0.6181, decode.d2.loss_dice: 0.9141, decode.d3.loss_cls: 0.5324, decode.d3.loss_mask: 0.6105, decode.d3.loss_dice: 0.9021, decode.d4.loss_cls: 0.5243, decode.d4.loss_mask: 0.6104, decode.d4.loss_dice: 0.8991, decode.d5.loss_cls: 0.5203, decode.d5.loss_mask: 0.6104, decode.d5.loss_dice: 0.8971, decode.d6.loss_cls: 0.5151, decode.d6.loss_mask: 0.6093, decode.d6.loss_dice: 0.8905, decode.d7.loss_cls: 0.5115, decode.d7.loss_mask: 0.6092, decode.d7.loss_dice: 0.8925, loss: 24.8923
2022-10-29 12:57:43,004 - mmseg - INFO - Iter [2750/20000]	lr: 2.633e-06, eta: 11:32:18, time: 2.300, data_time: 0.013, memory: 35422, decode.loss_cls: 0.5020, decode.loss_mask: 0.6078, decode.loss_dice: 0.8787, decode.d0.loss_cls: 6.7999, decode.d0.loss_mask: 0.5876, decode.d0.loss_dice: 0.9475, decode.d1.loss_cls: 0.6202, decode.d1.loss_mask: 0.6252, decode.d1.loss_dice: 0.9378, decode.d2.loss_cls: 0.5527, decode.d2.loss_mask: 0.6135, decode.d2.loss_dice: 0.8978, decode.d3.loss_cls: 0.5228, decode.d3.loss_mask: 0.6086, decode.d3.loss_dice: 0.8835, decode.d4.loss_cls: 0.5150, decode.d4.loss_mask: 0.6078, decode.d4.loss_dice: 0.8856, decode.d5.loss_cls: 0.5073, decode.d5.loss_mask: 0.6093, decode.d5.loss_dice: 0.8815, decode.d6.loss_cls: 0.5050, decode.d6.loss_mask: 0.6061, decode.d6.loss_dice: 0.8808, decode.d7.loss_cls: 0.5041, decode.d7.loss_mask: 0.6071, decode.d7.loss_dice: 0.8814, loss: 24.5765
2022-10-29 12:59:41,037 - mmseg - INFO - Iter [2800/20000]	lr: 2.625e-06, eta: 11:30:03, time: 2.361, data_time: 0.013, memory: 35422, decode.loss_cls: 0.5159, decode.loss_mask: 0.6083, decode.loss_dice: 0.8818, decode.d0.loss_cls: 6.7142, decode.d0.loss_mask: 0.5812, decode.d0.loss_dice: 0.9449, decode.d1.loss_cls: 0.6235, decode.d1.loss_mask: 0.6260, decode.d1.loss_dice: 0.9287, decode.d2.loss_cls: 0.5596, decode.d2.loss_mask: 0.6153, decode.d2.loss_dice: 0.9015, decode.d3.loss_cls: 0.5310, decode.d3.loss_mask: 0.6078, decode.d3.loss_dice: 0.8892, decode.d4.loss_cls: 0.5222, decode.d4.loss_mask: 0.6063, decode.d4.loss_dice: 0.8888, decode.d5.loss_cls: 0.5182, decode.d5.loss_mask: 0.6056, decode.d5.loss_dice: 0.8818, decode.d6.loss_cls: 0.5152, decode.d6.loss_mask: 0.6047, decode.d6.loss_dice: 0.8789, decode.d7.loss_cls: 0.5116, decode.d7.loss_mask: 0.6068, decode.d7.loss_dice: 0.8815, loss: 24.5505
2022-10-29 13:01:40,379 - mmseg - INFO - Iter [2850/20000]	lr: 2.617e-06, eta: 11:27:56, time: 2.387, data_time: 0.058, memory: 35422, decode.loss_cls: 0.5061, decode.loss_mask: 0.5948, decode.loss_dice: 0.8701, decode.d0.loss_cls: 6.6189, decode.d0.loss_mask: 0.5732, decode.d0.loss_dice: 0.9320, decode.d1.loss_cls: 0.6160, decode.d1.loss_mask: 0.6132, decode.d1.loss_dice: 0.9208, decode.d2.loss_cls: 0.5521, decode.d2.loss_mask: 0.6038, decode.d2.loss_dice: 0.8863, decode.d3.loss_cls: 0.5268, decode.d3.loss_mask: 0.5984, decode.d3.loss_dice: 0.8727, decode.d4.loss_cls: 0.5177, decode.d4.loss_mask: 0.5972, decode.d4.loss_dice: 0.8718, decode.d5.loss_cls: 0.5088, decode.d5.loss_mask: 0.5976, decode.d5.loss_dice: 0.8693, decode.d6.loss_cls: 0.5081, decode.d6.loss_mask: 0.5967, decode.d6.loss_dice: 0.8656, decode.d7.loss_cls: 0.5031, decode.d7.loss_mask: 0.5966, decode.d7.loss_dice: 0.8687, loss: 24.1862
2022-10-29 13:03:40,434 - mmseg - INFO - Iter [2900/20000]	lr: 2.610e-06, eta: 11:25:54, time: 2.401, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4810, decode.loss_mask: 0.5939, decode.loss_dice: 0.8591, decode.d0.loss_cls: 6.5311, decode.d0.loss_mask: 0.5710, decode.d0.loss_dice: 0.9268, decode.d1.loss_cls: 0.5918, decode.d1.loss_mask: 0.6124, decode.d1.loss_dice: 0.9237, decode.d2.loss_cls: 0.5297, decode.d2.loss_mask: 0.5996, decode.d2.loss_dice: 0.8846, decode.d3.loss_cls: 0.4993, decode.d3.loss_mask: 0.5937, decode.d3.loss_dice: 0.8681, decode.d4.loss_cls: 0.4930, decode.d4.loss_mask: 0.5943, decode.d4.loss_dice: 0.8638, decode.d5.loss_cls: 0.4866, decode.d5.loss_mask: 0.5910, decode.d5.loss_dice: 0.8634, decode.d6.loss_cls: 0.4802, decode.d6.loss_mask: 0.5919, decode.d6.loss_dice: 0.8606, decode.d7.loss_cls: 0.4812, decode.d7.loss_mask: 0.5919, decode.d7.loss_dice: 0.8603, loss: 23.8241
2022-10-29 13:05:39,788 - mmseg - INFO - Iter [2950/20000]	lr: 2.602e-06, eta: 11:23:48, time: 2.387, data_time: 0.014, memory: 35422, decode.loss_cls: 0.4886, decode.loss_mask: 0.6123, decode.loss_dice: 0.8740, decode.d0.loss_cls: 6.4326, decode.d0.loss_mask: 0.5933, decode.d0.loss_dice: 0.9356, decode.d1.loss_cls: 0.5962, decode.d1.loss_mask: 0.6317, decode.d1.loss_dice: 0.9247, decode.d2.loss_cls: 0.5334, decode.d2.loss_mask: 0.6229, decode.d2.loss_dice: 0.8926, decode.d3.loss_cls: 0.5070, decode.d3.loss_mask: 0.6158, decode.d3.loss_dice: 0.8775, decode.d4.loss_cls: 0.4987, decode.d4.loss_mask: 0.6140, decode.d4.loss_dice: 0.8795, decode.d5.loss_cls: 0.4928, decode.d5.loss_mask: 0.6143, decode.d5.loss_dice: 0.8780, decode.d6.loss_cls: 0.4906, decode.d6.loss_mask: 0.6126, decode.d6.loss_dice: 0.8704, decode.d7.loss_cls: 0.4866, decode.d7.loss_mask: 0.6128, decode.d7.loss_dice: 0.8732, loss: 24.0617
2022-10-29 13:07:38,603 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-29 13:07:38,604 - mmseg - INFO - Iter [3000/20000]	lr: 2.594e-06, eta: 11:21:40, time: 2.376, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4925, decode.loss_mask: 0.6050, decode.loss_dice: 0.8760, decode.d0.loss_cls: 6.3508, decode.d0.loss_mask: 0.5820, decode.d0.loss_dice: 0.9358, decode.d1.loss_cls: 0.6037, decode.d1.loss_mask: 0.6209, decode.d1.loss_dice: 0.9250, decode.d2.loss_cls: 0.5412, decode.d2.loss_mask: 0.6110, decode.d2.loss_dice: 0.8887, decode.d3.loss_cls: 0.5157, decode.d3.loss_mask: 0.6063, decode.d3.loss_dice: 0.8775, decode.d4.loss_cls: 0.5086, decode.d4.loss_mask: 0.6045, decode.d4.loss_dice: 0.8783, decode.d5.loss_cls: 0.5022, decode.d5.loss_mask: 0.6044, decode.d5.loss_dice: 0.8780, decode.d6.loss_cls: 0.5015, decode.d6.loss_mask: 0.6019, decode.d6.loss_dice: 0.8703, decode.d7.loss_cls: 0.4940, decode.d7.loss_mask: 0.6036, decode.d7.loss_dice: 0.8770, loss: 23.9568
2022-10-29 13:09:34,237 - mmseg - INFO - Iter [3050/20000]	lr: 2.587e-06, eta: 11:19:13, time: 2.312, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4865, decode.loss_mask: 0.5905, decode.loss_dice: 0.8649, decode.d0.loss_cls: 6.2608, decode.d0.loss_mask: 0.5732, decode.d0.loss_dice: 0.9358, decode.d1.loss_cls: 0.5956, decode.d1.loss_mask: 0.6094, decode.d1.loss_dice: 0.9224, decode.d2.loss_cls: 0.5369, decode.d2.loss_mask: 0.5976, decode.d2.loss_dice: 0.8870, decode.d3.loss_cls: 0.5084, decode.d3.loss_mask: 0.5916, decode.d3.loss_dice: 0.8732, decode.d4.loss_cls: 0.5004, decode.d4.loss_mask: 0.5919, decode.d4.loss_dice: 0.8734, decode.d5.loss_cls: 0.4970, decode.d5.loss_mask: 0.5894, decode.d5.loss_dice: 0.8712, decode.d6.loss_cls: 0.4887, decode.d6.loss_mask: 0.5895, decode.d6.loss_dice: 0.8687, decode.d7.loss_cls: 0.4894, decode.d7.loss_mask: 0.5885, decode.d7.loss_dice: 0.8682, loss: 23.6501
2022-10-29 13:11:30,318 - mmseg - INFO - Iter [3100/20000]	lr: 2.579e-06, eta: 11:16:51, time: 2.322, data_time: 0.014, memory: 35422, decode.loss_cls: 0.4893, decode.loss_mask: 0.6051, decode.loss_dice: 0.8839, decode.d0.loss_cls: 6.1813, decode.d0.loss_mask: 0.5851, decode.d0.loss_dice: 0.9452, decode.d1.loss_cls: 0.5968, decode.d1.loss_mask: 0.6193, decode.d1.loss_dice: 0.9275, decode.d2.loss_cls: 0.5368, decode.d2.loss_mask: 0.6088, decode.d2.loss_dice: 0.9008, decode.d3.loss_cls: 0.5084, decode.d3.loss_mask: 0.6026, decode.d3.loss_dice: 0.8851, decode.d4.loss_cls: 0.5019, decode.d4.loss_mask: 0.6025, decode.d4.loss_dice: 0.8821, decode.d5.loss_cls: 0.4937, decode.d5.loss_mask: 0.6045, decode.d5.loss_dice: 0.8800, decode.d6.loss_cls: 0.4913, decode.d6.loss_mask: 0.6038, decode.d6.loss_dice: 0.8759, decode.d7.loss_cls: 0.4894, decode.d7.loss_mask: 0.6034, decode.d7.loss_dice: 0.8771, loss: 23.7822
2022-10-29 13:13:33,017 - mmseg - INFO - Iter [3150/20000]	lr: 2.572e-06, eta: 11:15:04, time: 2.454, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4933, decode.loss_mask: 0.6059, decode.loss_dice: 0.8918, decode.d0.loss_cls: 6.0902, decode.d0.loss_mask: 0.5886, decode.d0.loss_dice: 0.9612, decode.d1.loss_cls: 0.6011, decode.d1.loss_mask: 0.6211, decode.d1.loss_dice: 0.9432, decode.d2.loss_cls: 0.5409, decode.d2.loss_mask: 0.6115, decode.d2.loss_dice: 0.9072, decode.d3.loss_cls: 0.5151, decode.d3.loss_mask: 0.6072, decode.d3.loss_dice: 0.8975, decode.d4.loss_cls: 0.5082, decode.d4.loss_mask: 0.6058, decode.d4.loss_dice: 0.8959, decode.d5.loss_cls: 0.5026, decode.d5.loss_mask: 0.6056, decode.d5.loss_dice: 0.8933, decode.d6.loss_cls: 0.4993, decode.d6.loss_mask: 0.6030, decode.d6.loss_dice: 0.8882, decode.d7.loss_cls: 0.4963, decode.d7.loss_mask: 0.6050, decode.d7.loss_dice: 0.8928, loss: 23.8721
2022-10-29 13:15:31,257 - mmseg - INFO - Iter [3200/20000]	lr: 2.564e-06, eta: 11:12:54, time: 2.365, data_time: 0.055, memory: 35422, decode.loss_cls: 0.4737, decode.loss_mask: 0.5909, decode.loss_dice: 0.8766, decode.d0.loss_cls: 6.0061, decode.d0.loss_mask: 0.5760, decode.d0.loss_dice: 0.9411, decode.d1.loss_cls: 0.5875, decode.d1.loss_mask: 0.6086, decode.d1.loss_dice: 0.9229, decode.d2.loss_cls: 0.5224, decode.d2.loss_mask: 0.5998, decode.d2.loss_dice: 0.8925, decode.d3.loss_cls: 0.4935, decode.d3.loss_mask: 0.5912, decode.d3.loss_dice: 0.8848, decode.d4.loss_cls: 0.4857, decode.d4.loss_mask: 0.5907, decode.d4.loss_dice: 0.8773, decode.d5.loss_cls: 0.4775, decode.d5.loss_mask: 0.5909, decode.d5.loss_dice: 0.8771, decode.d6.loss_cls: 0.4763, decode.d6.loss_mask: 0.5889, decode.d6.loss_dice: 0.8727, decode.d7.loss_cls: 0.4743, decode.d7.loss_mask: 0.5891, decode.d7.loss_dice: 0.8745, loss: 23.3426
2022-10-29 13:17:29,114 - mmseg - INFO - Iter [3250/20000]	lr: 2.556e-06, eta: 11:10:42, time: 2.357, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4721, decode.loss_mask: 0.5925, decode.loss_dice: 0.8561, decode.d0.loss_cls: 5.9148, decode.d0.loss_mask: 0.5736, decode.d0.loss_dice: 0.9211, decode.d1.loss_cls: 0.5810, decode.d1.loss_mask: 0.6087, decode.d1.loss_dice: 0.9063, decode.d2.loss_cls: 0.5175, decode.d2.loss_mask: 0.5982, decode.d2.loss_dice: 0.8734, decode.d3.loss_cls: 0.4905, decode.d3.loss_mask: 0.5919, decode.d3.loss_dice: 0.8603, decode.d4.loss_cls: 0.4819, decode.d4.loss_mask: 0.5937, decode.d4.loss_dice: 0.8566, decode.d5.loss_cls: 0.4749, decode.d5.loss_mask: 0.5922, decode.d5.loss_dice: 0.8577, decode.d6.loss_cls: 0.4763, decode.d6.loss_mask: 0.5906, decode.d6.loss_dice: 0.8491, decode.d7.loss_cls: 0.4713, decode.d7.loss_mask: 0.5930, decode.d7.loss_dice: 0.8541, loss: 23.0495
2022-10-29 13:19:26,002 - mmseg - INFO - Iter [3300/20000]	lr: 2.549e-06, eta: 11:08:25, time: 2.338, data_time: 0.011, memory: 35422, decode.loss_cls: 0.4671, decode.loss_mask: 0.6005, decode.loss_dice: 0.8775, decode.d0.loss_cls: 5.8370, decode.d0.loss_mask: 0.5836, decode.d0.loss_dice: 0.9404, decode.d1.loss_cls: 0.5783, decode.d1.loss_mask: 0.6186, decode.d1.loss_dice: 0.9275, decode.d2.loss_cls: 0.5149, decode.d2.loss_mask: 0.6082, decode.d2.loss_dice: 0.8885, decode.d3.loss_cls: 0.4916, decode.d3.loss_mask: 0.6007, decode.d3.loss_dice: 0.8783, decode.d4.loss_cls: 0.4812, decode.d4.loss_mask: 0.6007, decode.d4.loss_dice: 0.8762, decode.d5.loss_cls: 0.4771, decode.d5.loss_mask: 0.5993, decode.d5.loss_dice: 0.8773, decode.d6.loss_cls: 0.4718, decode.d6.loss_mask: 0.5994, decode.d6.loss_dice: 0.8745, decode.d7.loss_cls: 0.4680, decode.d7.loss_mask: 0.5999, decode.d7.loss_dice: 0.8757, loss: 23.2138
2022-10-29 13:21:22,167 - mmseg - INFO - Iter [3350/20000]	lr: 2.541e-06, eta: 11:06:06, time: 2.323, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4790, decode.loss_mask: 0.6021, decode.loss_dice: 0.8816, decode.d0.loss_cls: 5.7350, decode.d0.loss_mask: 0.5854, decode.d0.loss_dice: 0.9428, decode.d1.loss_cls: 0.5786, decode.d1.loss_mask: 0.6182, decode.d1.loss_dice: 0.9275, decode.d2.loss_cls: 0.5211, decode.d2.loss_mask: 0.6065, decode.d2.loss_dice: 0.8928, decode.d3.loss_cls: 0.4979, decode.d3.loss_mask: 0.6022, decode.d3.loss_dice: 0.8826, decode.d4.loss_cls: 0.4888, decode.d4.loss_mask: 0.6004, decode.d4.loss_dice: 0.8808, decode.d5.loss_cls: 0.4827, decode.d5.loss_mask: 0.6034, decode.d5.loss_dice: 0.8820, decode.d6.loss_cls: 0.4820, decode.d6.loss_mask: 0.6019, decode.d6.loss_dice: 0.8768, decode.d7.loss_cls: 0.4789, decode.d7.loss_mask: 0.6010, decode.d7.loss_dice: 0.8837, loss: 23.2157
2022-10-29 13:23:22,576 - mmseg - INFO - Iter [3400/20000]	lr: 2.533e-06, eta: 11:04:07, time: 2.408, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4695, decode.loss_mask: 0.5857, decode.loss_dice: 0.8581, decode.d0.loss_cls: 5.6487, decode.d0.loss_mask: 0.5735, decode.d0.loss_dice: 0.9298, decode.d1.loss_cls: 0.5712, decode.d1.loss_mask: 0.6004, decode.d1.loss_dice: 0.9118, decode.d2.loss_cls: 0.5137, decode.d2.loss_mask: 0.5894, decode.d2.loss_dice: 0.8799, decode.d3.loss_cls: 0.4868, decode.d3.loss_mask: 0.5866, decode.d3.loss_dice: 0.8642, decode.d4.loss_cls: 0.4838, decode.d4.loss_mask: 0.5836, decode.d4.loss_dice: 0.8633, decode.d5.loss_cls: 0.4739, decode.d5.loss_mask: 0.5844, decode.d5.loss_dice: 0.8608, decode.d6.loss_cls: 0.4734, decode.d6.loss_mask: 0.5836, decode.d6.loss_dice: 0.8600, decode.d7.loss_cls: 0.4719, decode.d7.loss_mask: 0.5848, decode.d7.loss_dice: 0.8593, loss: 22.7522
2022-10-29 13:25:16,443 - mmseg - INFO - Iter [3450/20000]	lr: 2.526e-06, eta: 11:01:37, time: 2.275, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4662, decode.loss_mask: 0.5868, decode.loss_dice: 0.8524, decode.d0.loss_cls: 5.5666, decode.d0.loss_mask: 0.5805, decode.d0.loss_dice: 0.9254, decode.d1.loss_cls: 0.5702, decode.d1.loss_mask: 0.6039, decode.d1.loss_dice: 0.9068, decode.d2.loss_cls: 0.5105, decode.d2.loss_mask: 0.5940, decode.d2.loss_dice: 0.8725, decode.d3.loss_cls: 0.4802, decode.d3.loss_mask: 0.5882, decode.d3.loss_dice: 0.8585, decode.d4.loss_cls: 0.4754, decode.d4.loss_mask: 0.5882, decode.d4.loss_dice: 0.8574, decode.d5.loss_cls: 0.4687, decode.d5.loss_mask: 0.5848, decode.d5.loss_dice: 0.8566, decode.d6.loss_cls: 0.4665, decode.d6.loss_mask: 0.5845, decode.d6.loss_dice: 0.8505, decode.d7.loss_cls: 0.4639, decode.d7.loss_mask: 0.5859, decode.d7.loss_dice: 0.8529, loss: 22.5979
2022-10-29 13:27:11,341 - mmseg - INFO - Iter [3500/20000]	lr: 2.518e-06, eta: 10:59:14, time: 2.300, data_time: 0.061, memory: 35422, decode.loss_cls: 0.4611, decode.loss_mask: 0.5882, decode.loss_dice: 0.8482, decode.d0.loss_cls: 5.4914, decode.d0.loss_mask: 0.5781, decode.d0.loss_dice: 0.9230, decode.d1.loss_cls: 0.5592, decode.d1.loss_mask: 0.6044, decode.d1.loss_dice: 0.8972, decode.d2.loss_cls: 0.5013, decode.d2.loss_mask: 0.5937, decode.d2.loss_dice: 0.8650, decode.d3.loss_cls: 0.4767, decode.d3.loss_mask: 0.5873, decode.d3.loss_dice: 0.8521, decode.d4.loss_cls: 0.4713, decode.d4.loss_mask: 0.5880, decode.d4.loss_dice: 0.8508, decode.d5.loss_cls: 0.4650, decode.d5.loss_mask: 0.5856, decode.d5.loss_dice: 0.8520, decode.d6.loss_cls: 0.4649, decode.d6.loss_mask: 0.5848, decode.d6.loss_dice: 0.8437, decode.d7.loss_cls: 0.4644, decode.d7.loss_mask: 0.5879, decode.d7.loss_dice: 0.8494, loss: 22.4350
2022-10-29 13:29:06,399 - mmseg - INFO - Iter [3550/20000]	lr: 2.511e-06, eta: 10:56:52, time: 2.301, data_time: 0.032, memory: 35422, decode.loss_cls: 0.4593, decode.loss_mask: 0.5813, decode.loss_dice: 0.8573, decode.d0.loss_cls: 5.4084, decode.d0.loss_mask: 0.5756, decode.d0.loss_dice: 0.9276, decode.d1.loss_cls: 0.5622, decode.d1.loss_mask: 0.6034, decode.d1.loss_dice: 0.9107, decode.d2.loss_cls: 0.5038, decode.d2.loss_mask: 0.5908, decode.d2.loss_dice: 0.8726, decode.d3.loss_cls: 0.4759, decode.d3.loss_mask: 0.5851, decode.d3.loss_dice: 0.8626, decode.d4.loss_cls: 0.4703, decode.d4.loss_mask: 0.5821, decode.d4.loss_dice: 0.8610, decode.d5.loss_cls: 0.4655, decode.d5.loss_mask: 0.5827, decode.d5.loss_dice: 0.8595, decode.d6.loss_cls: 0.4592, decode.d6.loss_mask: 0.5822, decode.d6.loss_dice: 0.8539, decode.d7.loss_cls: 0.4577, decode.d7.loss_mask: 0.5820, decode.d7.loss_dice: 0.8552, loss: 22.3880
2022-10-29 13:30:59,834 - mmseg - INFO - Iter [3600/20000]	lr: 2.503e-06, eta: 10:54:23, time: 2.269, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4479, decode.loss_mask: 0.5862, decode.loss_dice: 0.8466, decode.d0.loss_cls: 5.3192, decode.d0.loss_mask: 0.5761, decode.d0.loss_dice: 0.9210, decode.d1.loss_cls: 0.5510, decode.d1.loss_mask: 0.6064, decode.d1.loss_dice: 0.8983, decode.d2.loss_cls: 0.4909, decode.d2.loss_mask: 0.5939, decode.d2.loss_dice: 0.8661, decode.d3.loss_cls: 0.4650, decode.d3.loss_mask: 0.5865, decode.d3.loss_dice: 0.8558, decode.d4.loss_cls: 0.4582, decode.d4.loss_mask: 0.5853, decode.d4.loss_dice: 0.8517, decode.d5.loss_cls: 0.4520, decode.d5.loss_mask: 0.5856, decode.d5.loss_dice: 0.8512, decode.d6.loss_cls: 0.4492, decode.d6.loss_mask: 0.5862, decode.d6.loss_dice: 0.8459, decode.d7.loss_cls: 0.4468, decode.d7.loss_mask: 0.5854, decode.d7.loss_dice: 0.8466, loss: 22.1550
2022-10-29 13:33:01,915 - mmseg - INFO - Iter [3650/20000]	lr: 2.495e-06, eta: 10:52:34, time: 2.442, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4492, decode.loss_mask: 0.5920, decode.loss_dice: 0.8560, decode.d0.loss_cls: 5.2240, decode.d0.loss_mask: 0.5839, decode.d0.loss_dice: 0.9226, decode.d1.loss_cls: 0.5438, decode.d1.loss_mask: 0.6112, decode.d1.loss_dice: 0.9066, decode.d2.loss_cls: 0.4902, decode.d2.loss_mask: 0.5998, decode.d2.loss_dice: 0.8749, decode.d3.loss_cls: 0.4652, decode.d3.loss_mask: 0.5927, decode.d3.loss_dice: 0.8581, decode.d4.loss_cls: 0.4581, decode.d4.loss_mask: 0.5920, decode.d4.loss_dice: 0.8577, decode.d5.loss_cls: 0.4528, decode.d5.loss_mask: 0.5884, decode.d5.loss_dice: 0.8572, decode.d6.loss_cls: 0.4501, decode.d6.loss_mask: 0.5902, decode.d6.loss_dice: 0.8550, decode.d7.loss_cls: 0.4469, decode.d7.loss_mask: 0.5911, decode.d7.loss_dice: 0.8590, loss: 22.1684
2022-10-29 13:35:03,351 - mmseg - INFO - Iter [3700/20000]	lr: 2.488e-06, eta: 10:50:42, time: 2.429, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4478, decode.loss_mask: 0.5810, decode.loss_dice: 0.8419, decode.d0.loss_cls: 5.1470, decode.d0.loss_mask: 0.5743, decode.d0.loss_dice: 0.9231, decode.d1.loss_cls: 0.5454, decode.d1.loss_mask: 0.6020, decode.d1.loss_dice: 0.8960, decode.d2.loss_cls: 0.4860, decode.d2.loss_mask: 0.5901, decode.d2.loss_dice: 0.8588, decode.d3.loss_cls: 0.4613, decode.d3.loss_mask: 0.5834, decode.d3.loss_dice: 0.8458, decode.d4.loss_cls: 0.4557, decode.d4.loss_mask: 0.5833, decode.d4.loss_dice: 0.8433, decode.d5.loss_cls: 0.4531, decode.d5.loss_mask: 0.5808, decode.d5.loss_dice: 0.8395, decode.d6.loss_cls: 0.4495, decode.d6.loss_mask: 0.5799, decode.d6.loss_dice: 0.8427, decode.d7.loss_cls: 0.4455, decode.d7.loss_mask: 0.5812, decode.d7.loss_dice: 0.8430, loss: 21.8815
2022-10-29 13:37:01,400 - mmseg - INFO - Iter [3750/20000]	lr: 2.480e-06, eta: 10:48:35, time: 2.361, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4598, decode.loss_mask: 0.5919, decode.loss_dice: 0.8558, decode.d0.loss_cls: 5.0725, decode.d0.loss_mask: 0.5880, decode.d0.loss_dice: 0.9233, decode.d1.loss_cls: 0.5584, decode.d1.loss_mask: 0.6133, decode.d1.loss_dice: 0.9010, decode.d2.loss_cls: 0.5046, decode.d2.loss_mask: 0.6005, decode.d2.loss_dice: 0.8700, decode.d3.loss_cls: 0.4786, decode.d3.loss_mask: 0.5931, decode.d3.loss_dice: 0.8586, decode.d4.loss_cls: 0.4700, decode.d4.loss_mask: 0.5914, decode.d4.loss_dice: 0.8570, decode.d5.loss_cls: 0.4622, decode.d5.loss_mask: 0.5907, decode.d5.loss_dice: 0.8568, decode.d6.loss_cls: 0.4594, decode.d6.loss_mask: 0.5910, decode.d6.loss_dice: 0.8521, decode.d7.loss_cls: 0.4613, decode.d7.loss_mask: 0.5924, decode.d7.loss_dice: 0.8548, loss: 22.1085
2022-10-29 13:38:59,146 - mmseg - INFO - Iter [3800/20000]	lr: 2.472e-06, eta: 10:46:27, time: 2.355, data_time: 0.059, memory: 35422, decode.loss_cls: 0.4574, decode.loss_mask: 0.5917, decode.loss_dice: 0.8549, decode.d0.loss_cls: 4.9869, decode.d0.loss_mask: 0.5820, decode.d0.loss_dice: 0.9236, decode.d1.loss_cls: 0.5563, decode.d1.loss_mask: 0.6075, decode.d1.loss_dice: 0.9047, decode.d2.loss_cls: 0.5013, decode.d2.loss_mask: 0.5960, decode.d2.loss_dice: 0.8735, decode.d3.loss_cls: 0.4775, decode.d3.loss_mask: 0.5921, decode.d3.loss_dice: 0.8574, decode.d4.loss_cls: 0.4679, decode.d4.loss_mask: 0.5918, decode.d4.loss_dice: 0.8572, decode.d5.loss_cls: 0.4619, decode.d5.loss_mask: 0.5896, decode.d5.loss_dice: 0.8600, decode.d6.loss_cls: 0.4595, decode.d6.loss_mask: 0.5906, decode.d6.loss_dice: 0.8534, decode.d7.loss_cls: 0.4582, decode.d7.loss_mask: 0.5903, decode.d7.loss_dice: 0.8533, loss: 21.9962
2022-10-29 13:40:52,993 - mmseg - INFO - Iter [3850/20000]	lr: 2.465e-06, eta: 10:44:02, time: 2.277, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4318, decode.loss_mask: 0.5793, decode.loss_dice: 0.8421, decode.d0.loss_cls: 4.9146, decode.d0.loss_mask: 0.5751, decode.d0.loss_dice: 0.9101, decode.d1.loss_cls: 0.5338, decode.d1.loss_mask: 0.5942, decode.d1.loss_dice: 0.8884, decode.d2.loss_cls: 0.4729, decode.d2.loss_mask: 0.5850, decode.d2.loss_dice: 0.8579, decode.d3.loss_cls: 0.4478, decode.d3.loss_mask: 0.5789, decode.d3.loss_dice: 0.8457, decode.d4.loss_cls: 0.4391, decode.d4.loss_mask: 0.5795, decode.d4.loss_dice: 0.8463, decode.d5.loss_cls: 0.4361, decode.d5.loss_mask: 0.5782, decode.d5.loss_dice: 0.8449, decode.d6.loss_cls: 0.4316, decode.d6.loss_mask: 0.5777, decode.d6.loss_dice: 0.8415, decode.d7.loss_cls: 0.4294, decode.d7.loss_mask: 0.5798, decode.d7.loss_dice: 0.8449, loss: 21.4865
2022-10-29 13:42:51,486 - mmseg - INFO - Iter [3900/20000]	lr: 2.457e-06, eta: 10:41:58, time: 2.370, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4457, decode.loss_mask: 0.5794, decode.loss_dice: 0.8400, decode.d0.loss_cls: 4.8455, decode.d0.loss_mask: 0.5809, decode.d0.loss_dice: 0.9146, decode.d1.loss_cls: 0.5437, decode.d1.loss_mask: 0.5994, decode.d1.loss_dice: 0.8823, decode.d2.loss_cls: 0.4887, decode.d2.loss_mask: 0.5880, decode.d2.loss_dice: 0.8535, decode.d3.loss_cls: 0.4652, decode.d3.loss_mask: 0.5827, decode.d3.loss_dice: 0.8391, decode.d4.loss_cls: 0.4577, decode.d4.loss_mask: 0.5792, decode.d4.loss_dice: 0.8425, decode.d5.loss_cls: 0.4507, decode.d5.loss_mask: 0.5792, decode.d5.loss_dice: 0.8397, decode.d6.loss_cls: 0.4474, decode.d6.loss_mask: 0.5795, decode.d6.loss_dice: 0.8383, decode.d7.loss_cls: 0.4458, decode.d7.loss_mask: 0.5792, decode.d7.loss_dice: 0.8396, loss: 21.5276
2022-10-29 13:44:50,161 - mmseg - INFO - Iter [3950/20000]	lr: 2.450e-06, eta: 10:39:54, time: 2.373, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4425, decode.loss_mask: 0.5878, decode.loss_dice: 0.8622, decode.d0.loss_cls: 4.7574, decode.d0.loss_mask: 0.5848, decode.d0.loss_dice: 0.9304, decode.d1.loss_cls: 0.5368, decode.d1.loss_mask: 0.6061, decode.d1.loss_dice: 0.9112, decode.d2.loss_cls: 0.4825, decode.d2.loss_mask: 0.5947, decode.d2.loss_dice: 0.8749, decode.d3.loss_cls: 0.4597, decode.d3.loss_mask: 0.5905, decode.d3.loss_dice: 0.8662, decode.d4.loss_cls: 0.4565, decode.d4.loss_mask: 0.5879, decode.d4.loss_dice: 0.8648, decode.d5.loss_cls: 0.4470, decode.d5.loss_mask: 0.5858, decode.d5.loss_dice: 0.8641, decode.d6.loss_cls: 0.4441, decode.d6.loss_mask: 0.5855, decode.d6.loss_dice: 0.8601, decode.d7.loss_cls: 0.4418, decode.d7.loss_mask: 0.5872, decode.d7.loss_dice: 0.8617, loss: 21.6743
2022-10-29 13:46:47,486 - mmseg - INFO - Saving checkpoint at 4000 iterations
2022-10-29 13:47:20,344 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-29 13:47:20,344 - mmseg - INFO - Iter [4000/20000]	lr: 2.442e-06, eta: 10:39:57, time: 3.004, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4498, decode.loss_mask: 0.5727, decode.loss_dice: 0.8482, decode.d0.loss_cls: 4.6825, decode.d0.loss_mask: 0.5712, decode.d0.loss_dice: 0.9231, decode.d1.loss_cls: 0.5393, decode.d1.loss_mask: 0.5920, decode.d1.loss_dice: 0.8993, decode.d2.loss_cls: 0.4918, decode.d2.loss_mask: 0.5787, decode.d2.loss_dice: 0.8634, decode.d3.loss_cls: 0.4673, decode.d3.loss_mask: 0.5731, decode.d3.loss_dice: 0.8511, decode.d4.loss_cls: 0.4623, decode.d4.loss_mask: 0.5727, decode.d4.loss_dice: 0.8521, decode.d5.loss_cls: 0.4522, decode.d5.loss_mask: 0.5717, decode.d5.loss_dice: 0.8496, decode.d6.loss_cls: 0.4514, decode.d6.loss_mask: 0.5706, decode.d6.loss_dice: 0.8425, decode.d7.loss_cls: 0.4499, decode.d7.loss_mask: 0.5706, decode.d7.loss_dice: 0.8466, loss: 21.3956
2022-10-29 13:49:18,300 - mmseg - INFO - Iter [4050/20000]	lr: 2.434e-06, eta: 10:37:49, time: 2.359, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4364, decode.loss_mask: 0.5781, decode.loss_dice: 0.8494, decode.d0.loss_cls: 4.5994, decode.d0.loss_mask: 0.5791, decode.d0.loss_dice: 0.9282, decode.d1.loss_cls: 0.5316, decode.d1.loss_mask: 0.5967, decode.d1.loss_dice: 0.8997, decode.d2.loss_cls: 0.4796, decode.d2.loss_mask: 0.5853, decode.d2.loss_dice: 0.8713, decode.d3.loss_cls: 0.4556, decode.d3.loss_mask: 0.5794, decode.d3.loss_dice: 0.8548, decode.d4.loss_cls: 0.4483, decode.d4.loss_mask: 0.5790, decode.d4.loss_dice: 0.8538, decode.d5.loss_cls: 0.4419, decode.d5.loss_mask: 0.5787, decode.d5.loss_dice: 0.8518, decode.d6.loss_cls: 0.4381, decode.d6.loss_mask: 0.5776, decode.d6.loss_dice: 0.8494, decode.d7.loss_cls: 0.4361, decode.d7.loss_mask: 0.5779, decode.d7.loss_dice: 0.8507, loss: 21.3079
2022-10-29 13:51:14,577 - mmseg - INFO - Iter [4100/20000]	lr: 2.427e-06, eta: 10:35:35, time: 2.326, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4524, decode.loss_mask: 0.5809, decode.loss_dice: 0.8516, decode.d0.loss_cls: 4.5410, decode.d0.loss_mask: 0.5782, decode.d0.loss_dice: 0.9294, decode.d1.loss_cls: 0.5428, decode.d1.loss_mask: 0.5973, decode.d1.loss_dice: 0.8995, decode.d2.loss_cls: 0.4911, decode.d2.loss_mask: 0.5882, decode.d2.loss_dice: 0.8660, decode.d3.loss_cls: 0.4685, decode.d3.loss_mask: 0.5810, decode.d3.loss_dice: 0.8513, decode.d4.loss_cls: 0.4616, decode.d4.loss_mask: 0.5821, decode.d4.loss_dice: 0.8524, decode.d5.loss_cls: 0.4557, decode.d5.loss_mask: 0.5809, decode.d5.loss_dice: 0.8478, decode.d6.loss_cls: 0.4519, decode.d6.loss_mask: 0.5794, decode.d6.loss_dice: 0.8496, decode.d7.loss_cls: 0.4504, decode.d7.loss_mask: 0.5810, decode.d7.loss_dice: 0.8498, loss: 21.3620
2022-10-29 13:53:13,489 - mmseg - INFO - Iter [4150/20000]	lr: 2.419e-06, eta: 10:33:31, time: 2.378, data_time: 0.056, memory: 35422, decode.loss_cls: 0.4330, decode.loss_mask: 0.5627, decode.loss_dice: 0.8353, decode.d0.loss_cls: 4.4682, decode.d0.loss_mask: 0.5636, decode.d0.loss_dice: 0.9136, decode.d1.loss_cls: 0.5329, decode.d1.loss_mask: 0.5813, decode.d1.loss_dice: 0.8828, decode.d2.loss_cls: 0.4771, decode.d2.loss_mask: 0.5688, decode.d2.loss_dice: 0.8522, decode.d3.loss_cls: 0.4530, decode.d3.loss_mask: 0.5633, decode.d3.loss_dice: 0.8392, decode.d4.loss_cls: 0.4458, decode.d4.loss_mask: 0.5635, decode.d4.loss_dice: 0.8356, decode.d5.loss_cls: 0.4394, decode.d5.loss_mask: 0.5604, decode.d5.loss_dice: 0.8358, decode.d6.loss_cls: 0.4337, decode.d6.loss_mask: 0.5620, decode.d6.loss_dice: 0.8343, decode.d7.loss_cls: 0.4338, decode.d7.loss_mask: 0.5611, decode.d7.loss_dice: 0.8364, loss: 20.8689
2022-10-29 13:55:15,072 - mmseg - INFO - Iter [4200/20000]	lr: 2.411e-06, eta: 10:31:37, time: 2.431, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4277, decode.loss_mask: 0.5820, decode.loss_dice: 0.8546, decode.d0.loss_cls: 4.4020, decode.d0.loss_mask: 0.5806, decode.d0.loss_dice: 0.9249, decode.d1.loss_cls: 0.5218, decode.d1.loss_mask: 0.5984, decode.d1.loss_dice: 0.9029, decode.d2.loss_cls: 0.4704, decode.d2.loss_mask: 0.5868, decode.d2.loss_dice: 0.8734, decode.d3.loss_cls: 0.4466, decode.d3.loss_mask: 0.5829, decode.d3.loss_dice: 0.8584, decode.d4.loss_cls: 0.4394, decode.d4.loss_mask: 0.5820, decode.d4.loss_dice: 0.8552, decode.d5.loss_cls: 0.4347, decode.d5.loss_mask: 0.5813, decode.d5.loss_dice: 0.8543, decode.d6.loss_cls: 0.4321, decode.d6.loss_mask: 0.5805, decode.d6.loss_dice: 0.8522, decode.d7.loss_cls: 0.4276, decode.d7.loss_mask: 0.5819, decode.d7.loss_dice: 0.8531, loss: 21.0875
2022-10-29 13:57:11,654 - mmseg - INFO - Iter [4250/20000]	lr: 2.404e-06, eta: 10:29:25, time: 2.332, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4282, decode.loss_mask: 0.5765, decode.loss_dice: 0.8406, decode.d0.loss_cls: 4.3253, decode.d0.loss_mask: 0.5729, decode.d0.loss_dice: 0.9176, decode.d1.loss_cls: 0.5258, decode.d1.loss_mask: 0.5908, decode.d1.loss_dice: 0.8864, decode.d2.loss_cls: 0.4713, decode.d2.loss_mask: 0.5793, decode.d2.loss_dice: 0.8547, decode.d3.loss_cls: 0.4453, decode.d3.loss_mask: 0.5755, decode.d3.loss_dice: 0.8453, decode.d4.loss_cls: 0.4398, decode.d4.loss_mask: 0.5738, decode.d4.loss_dice: 0.8406, decode.d5.loss_cls: 0.4328, decode.d5.loss_mask: 0.5745, decode.d5.loss_dice: 0.8376, decode.d6.loss_cls: 0.4275, decode.d6.loss_mask: 0.5745, decode.d6.loss_dice: 0.8395, decode.d7.loss_cls: 0.4275, decode.d7.loss_mask: 0.5763, decode.d7.loss_dice: 0.8390, loss: 20.8188
2022-10-29 13:59:12,414 - mmseg - INFO - Iter [4300/20000]	lr: 2.396e-06, eta: 10:27:28, time: 2.415, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4217, decode.loss_mask: 0.5867, decode.loss_dice: 0.8402, decode.d0.loss_cls: 4.2440, decode.d0.loss_mask: 0.5901, decode.d0.loss_dice: 0.9152, decode.d1.loss_cls: 0.5142, decode.d1.loss_mask: 0.6027, decode.d1.loss_dice: 0.8942, decode.d2.loss_cls: 0.4653, decode.d2.loss_mask: 0.5926, decode.d2.loss_dice: 0.8562, decode.d3.loss_cls: 0.4377, decode.d3.loss_mask: 0.5872, decode.d3.loss_dice: 0.8457, decode.d4.loss_cls: 0.4331, decode.d4.loss_mask: 0.5860, decode.d4.loss_dice: 0.8443, decode.d5.loss_cls: 0.4259, decode.d5.loss_mask: 0.5866, decode.d5.loss_dice: 0.8414, decode.d6.loss_cls: 0.4237, decode.d6.loss_mask: 0.5858, decode.d6.loss_dice: 0.8396, decode.d7.loss_cls: 0.4230, decode.d7.loss_mask: 0.5873, decode.d7.loss_dice: 0.8403, loss: 20.8107
2022-10-29 14:01:18,153 - mmseg - INFO - Iter [4350/20000]	lr: 2.388e-06, eta: 10:25:49, time: 2.515, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4367, decode.loss_mask: 0.5920, decode.loss_dice: 0.8610, decode.d0.loss_cls: 4.1779, decode.d0.loss_mask: 0.5918, decode.d0.loss_dice: 0.9343, decode.d1.loss_cls: 0.5252, decode.d1.loss_mask: 0.6068, decode.d1.loss_dice: 0.9042, decode.d2.loss_cls: 0.4753, decode.d2.loss_mask: 0.5949, decode.d2.loss_dice: 0.8750, decode.d3.loss_cls: 0.4550, decode.d3.loss_mask: 0.5918, decode.d3.loss_dice: 0.8602, decode.d4.loss_cls: 0.4472, decode.d4.loss_mask: 0.5905, decode.d4.loss_dice: 0.8625, decode.d5.loss_cls: 0.4442, decode.d5.loss_mask: 0.5897, decode.d5.loss_dice: 0.8595, decode.d6.loss_cls: 0.4414, decode.d6.loss_mask: 0.5904, decode.d6.loss_dice: 0.8567, decode.d7.loss_cls: 0.4384, decode.d7.loss_mask: 0.5908, decode.d7.loss_dice: 0.8575, loss: 21.0511
2022-10-29 14:03:19,434 - mmseg - INFO - Iter [4400/20000]	lr: 2.381e-06, eta: 10:23:54, time: 2.426, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4258, decode.loss_mask: 0.5741, decode.loss_dice: 0.8471, decode.d0.loss_cls: 4.1156, decode.d0.loss_mask: 0.5747, decode.d0.loss_dice: 0.9198, decode.d1.loss_cls: 0.5170, decode.d1.loss_mask: 0.5904, decode.d1.loss_dice: 0.8976, decode.d2.loss_cls: 0.4667, decode.d2.loss_mask: 0.5794, decode.d2.loss_dice: 0.8643, decode.d3.loss_cls: 0.4440, decode.d3.loss_mask: 0.5718, decode.d3.loss_dice: 0.8517, decode.d4.loss_cls: 0.4364, decode.d4.loss_mask: 0.5739, decode.d4.loss_dice: 0.8558, decode.d5.loss_cls: 0.4309, decode.d5.loss_mask: 0.5732, decode.d5.loss_dice: 0.8503, decode.d6.loss_cls: 0.4268, decode.d6.loss_mask: 0.5720, decode.d6.loss_dice: 0.8496, decode.d7.loss_cls: 0.4245, decode.d7.loss_mask: 0.5737, decode.d7.loss_dice: 0.8516, loss: 20.6588
2022-10-29 14:05:20,365 - mmseg - INFO - Iter [4450/20000]	lr: 2.373e-06, eta: 10:21:57, time: 2.419, data_time: 0.058, memory: 35422, decode.loss_cls: 0.4224, decode.loss_mask: 0.5724, decode.loss_dice: 0.8371, decode.d0.loss_cls: 4.0557, decode.d0.loss_mask: 0.5758, decode.d0.loss_dice: 0.9104, decode.d1.loss_cls: 0.5197, decode.d1.loss_mask: 0.5907, decode.d1.loss_dice: 0.8827, decode.d2.loss_cls: 0.4633, decode.d2.loss_mask: 0.5772, decode.d2.loss_dice: 0.8528, decode.d3.loss_cls: 0.4404, decode.d3.loss_mask: 0.5722, decode.d3.loss_dice: 0.8409, decode.d4.loss_cls: 0.4339, decode.d4.loss_mask: 0.5711, decode.d4.loss_dice: 0.8397, decode.d5.loss_cls: 0.4268, decode.d5.loss_mask: 0.5699, decode.d5.loss_dice: 0.8371, decode.d6.loss_cls: 0.4239, decode.d6.loss_mask: 0.5710, decode.d6.loss_dice: 0.8336, decode.d7.loss_cls: 0.4211, decode.d7.loss_mask: 0.5716, decode.d7.loss_dice: 0.8365, loss: 20.4496
2022-10-29 14:07:20,650 - mmseg - INFO - Iter [4500/20000]	lr: 2.366e-06, eta: 10:19:58, time: 2.406, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4235, decode.loss_mask: 0.5731, decode.loss_dice: 0.8249, decode.d0.loss_cls: 3.9934, decode.d0.loss_mask: 0.5787, decode.d0.loss_dice: 0.9078, decode.d1.loss_cls: 0.5149, decode.d1.loss_mask: 0.5902, decode.d1.loss_dice: 0.8753, decode.d2.loss_cls: 0.4623, decode.d2.loss_mask: 0.5777, decode.d2.loss_dice: 0.8412, decode.d3.loss_cls: 0.4402, decode.d3.loss_mask: 0.5725, decode.d3.loss_dice: 0.8297, decode.d4.loss_cls: 0.4307, decode.d4.loss_mask: 0.5722, decode.d4.loss_dice: 0.8293, decode.d5.loss_cls: 0.4292, decode.d5.loss_mask: 0.5710, decode.d5.loss_dice: 0.8264, decode.d6.loss_cls: 0.4227, decode.d6.loss_mask: 0.5708, decode.d6.loss_dice: 0.8240, decode.d7.loss_cls: 0.4210, decode.d7.loss_mask: 0.5726, decode.d7.loss_dice: 0.8277, loss: 20.3032
2022-10-29 14:09:22,167 - mmseg - INFO - Iter [4550/20000]	lr: 2.358e-06, eta: 10:18:04, time: 2.430, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4185, decode.loss_mask: 0.5681, decode.loss_dice: 0.8365, decode.d0.loss_cls: 3.9209, decode.d0.loss_mask: 0.5701, decode.d0.loss_dice: 0.9146, decode.d1.loss_cls: 0.5081, decode.d1.loss_mask: 0.5881, decode.d1.loss_dice: 0.8901, decode.d2.loss_cls: 0.4569, decode.d2.loss_mask: 0.5755, decode.d2.loss_dice: 0.8560, decode.d3.loss_cls: 0.4320, decode.d3.loss_mask: 0.5705, decode.d3.loss_dice: 0.8394, decode.d4.loss_cls: 0.4260, decode.d4.loss_mask: 0.5695, decode.d4.loss_dice: 0.8355, decode.d5.loss_cls: 0.4207, decode.d5.loss_mask: 0.5687, decode.d5.loss_dice: 0.8385, decode.d6.loss_cls: 0.4179, decode.d6.loss_mask: 0.5677, decode.d6.loss_dice: 0.8346, decode.d7.loss_cls: 0.4179, decode.d7.loss_mask: 0.5686, decode.d7.loss_dice: 0.8370, loss: 20.2479
2022-10-29 14:11:24,693 - mmseg - INFO - Iter [4600/20000]	lr: 2.350e-06, eta: 10:16:12, time: 2.451, data_time: 0.014, memory: 35422, decode.loss_cls: 0.4103, decode.loss_mask: 0.5731, decode.loss_dice: 0.8352, decode.d0.loss_cls: 3.8439, decode.d0.loss_mask: 0.5739, decode.d0.loss_dice: 0.9050, decode.d1.loss_cls: 0.4936, decode.d1.loss_mask: 0.5919, decode.d1.loss_dice: 0.8772, decode.d2.loss_cls: 0.4475, decode.d2.loss_mask: 0.5794, decode.d2.loss_dice: 0.8447, decode.d3.loss_cls: 0.4255, decode.d3.loss_mask: 0.5736, decode.d3.loss_dice: 0.8363, decode.d4.loss_cls: 0.4192, decode.d4.loss_mask: 0.5745, decode.d4.loss_dice: 0.8381, decode.d5.loss_cls: 0.4151, decode.d5.loss_mask: 0.5729, decode.d5.loss_dice: 0.8367, decode.d6.loss_cls: 0.4103, decode.d6.loss_mask: 0.5723, decode.d6.loss_dice: 0.8308, decode.d7.loss_cls: 0.4076, decode.d7.loss_mask: 0.5734, decode.d7.loss_dice: 0.8349, loss: 20.0971
2022-10-29 14:13:25,549 - mmseg - INFO - Iter [4650/20000]	lr: 2.343e-06, eta: 10:14:15, time: 2.417, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4321, decode.loss_mask: 0.5642, decode.loss_dice: 0.8274, decode.d0.loss_cls: 3.8023, decode.d0.loss_mask: 0.5734, decode.d0.loss_dice: 0.9126, decode.d1.loss_cls: 0.5185, decode.d1.loss_mask: 0.5841, decode.d1.loss_dice: 0.8777, decode.d2.loss_cls: 0.4662, decode.d2.loss_mask: 0.5719, decode.d2.loss_dice: 0.8479, decode.d3.loss_cls: 0.4429, decode.d3.loss_mask: 0.5652, decode.d3.loss_dice: 0.8387, decode.d4.loss_cls: 0.4390, decode.d4.loss_mask: 0.5643, decode.d4.loss_dice: 0.8316, decode.d5.loss_cls: 0.4327, decode.d5.loss_mask: 0.5642, decode.d5.loss_dice: 0.8296, decode.d6.loss_cls: 0.4285, decode.d6.loss_mask: 0.5623, decode.d6.loss_dice: 0.8285, decode.d7.loss_cls: 0.4284, decode.d7.loss_mask: 0.5642, decode.d7.loss_dice: 0.8297, loss: 20.1284
2022-10-29 14:15:24,597 - mmseg - INFO - Iter [4700/20000]	lr: 2.335e-06, eta: 10:12:11, time: 2.381, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4220, decode.loss_mask: 0.5681, decode.loss_dice: 0.8442, decode.d0.loss_cls: 3.7441, decode.d0.loss_mask: 0.5769, decode.d0.loss_dice: 0.9323, decode.d1.loss_cls: 0.5105, decode.d1.loss_mask: 0.5882, decode.d1.loss_dice: 0.8972, decode.d2.loss_cls: 0.4576, decode.d2.loss_mask: 0.5758, decode.d2.loss_dice: 0.8672, decode.d3.loss_cls: 0.4407, decode.d3.loss_mask: 0.5692, decode.d3.loss_dice: 0.8466, decode.d4.loss_cls: 0.4336, decode.d4.loss_mask: 0.5684, decode.d4.loss_dice: 0.8463, decode.d5.loss_cls: 0.4279, decode.d5.loss_mask: 0.5669, decode.d5.loss_dice: 0.8463, decode.d6.loss_cls: 0.4216, decode.d6.loss_mask: 0.5675, decode.d6.loss_dice: 0.8427, decode.d7.loss_cls: 0.4215, decode.d7.loss_mask: 0.5689, decode.d7.loss_dice: 0.8415, loss: 20.1936
2022-10-29 14:17:23,678 - mmseg - INFO - Iter [4750/20000]	lr: 2.327e-06, eta: 10:10:08, time: 2.382, data_time: 0.059, memory: 35422, decode.loss_cls: 0.4161, decode.loss_mask: 0.5789, decode.loss_dice: 0.8371, decode.d0.loss_cls: 3.6857, decode.d0.loss_mask: 0.5844, decode.d0.loss_dice: 0.9173, decode.d1.loss_cls: 0.5076, decode.d1.loss_mask: 0.5957, decode.d1.loss_dice: 0.8877, decode.d2.loss_cls: 0.4560, decode.d2.loss_mask: 0.5875, decode.d2.loss_dice: 0.8550, decode.d3.loss_cls: 0.4350, decode.d3.loss_mask: 0.5818, decode.d3.loss_dice: 0.8422, decode.d4.loss_cls: 0.4297, decode.d4.loss_mask: 0.5788, decode.d4.loss_dice: 0.8401, decode.d5.loss_cls: 0.4220, decode.d5.loss_mask: 0.5800, decode.d5.loss_dice: 0.8391, decode.d6.loss_cls: 0.4174, decode.d6.loss_mask: 0.5782, decode.d6.loss_dice: 0.8363, decode.d7.loss_cls: 0.4143, decode.d7.loss_mask: 0.5790, decode.d7.loss_dice: 0.8379, loss: 20.1208
2022-10-29 14:19:21,964 - mmseg - INFO - Iter [4800/20000]	lr: 2.320e-06, eta: 10:08:03, time: 2.365, data_time: 0.017, memory: 35422, decode.loss_cls: 0.4139, decode.loss_mask: 0.5599, decode.loss_dice: 0.8201, decode.d0.loss_cls: 3.6378, decode.d0.loss_mask: 0.5669, decode.d0.loss_dice: 0.9036, decode.d1.loss_cls: 0.5063, decode.d1.loss_mask: 0.5784, decode.d1.loss_dice: 0.8670, decode.d2.loss_cls: 0.4511, decode.d2.loss_mask: 0.5650, decode.d2.loss_dice: 0.8374, decode.d3.loss_cls: 0.4263, decode.d3.loss_mask: 0.5623, decode.d3.loss_dice: 0.8235, decode.d4.loss_cls: 0.4221, decode.d4.loss_mask: 0.5604, decode.d4.loss_dice: 0.8241, decode.d5.loss_cls: 0.4130, decode.d5.loss_mask: 0.5588, decode.d5.loss_dice: 0.8229, decode.d6.loss_cls: 0.4108, decode.d6.loss_mask: 0.5599, decode.d6.loss_dice: 0.8175, decode.d7.loss_cls: 0.4123, decode.d7.loss_mask: 0.5602, decode.d7.loss_dice: 0.8192, loss: 19.7008
2022-10-29 14:21:18,923 - mmseg - INFO - Iter [4850/20000]	lr: 2.312e-06, eta: 10:05:53, time: 2.340, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4041, decode.loss_mask: 0.5697, decode.loss_dice: 0.8336, decode.d0.loss_cls: 3.5687, decode.d0.loss_mask: 0.5767, decode.d0.loss_dice: 0.9163, decode.d1.loss_cls: 0.4975, decode.d1.loss_mask: 0.5845, decode.d1.loss_dice: 0.8834, decode.d2.loss_cls: 0.4471, decode.d2.loss_mask: 0.5751, decode.d2.loss_dice: 0.8517, decode.d3.loss_cls: 0.4253, decode.d3.loss_mask: 0.5699, decode.d3.loss_dice: 0.8376, decode.d4.loss_cls: 0.4171, decode.d4.loss_mask: 0.5689, decode.d4.loss_dice: 0.8372, decode.d5.loss_cls: 0.4134, decode.d5.loss_mask: 0.5681, decode.d5.loss_dice: 0.8349, decode.d6.loss_cls: 0.4067, decode.d6.loss_mask: 0.5677, decode.d6.loss_dice: 0.8346, decode.d7.loss_cls: 0.4051, decode.d7.loss_mask: 0.5678, decode.d7.loss_dice: 0.8370, loss: 19.7998
2022-10-29 14:23:18,336 - mmseg - INFO - Iter [4900/20000]	lr: 2.305e-06, eta: 10:03:51, time: 2.388, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4189, decode.loss_mask: 0.5701, decode.loss_dice: 0.8238, decode.d0.loss_cls: 3.5144, decode.d0.loss_mask: 0.5797, decode.d0.loss_dice: 0.9094, decode.d1.loss_cls: 0.5080, decode.d1.loss_mask: 0.5881, decode.d1.loss_dice: 0.8754, decode.d2.loss_cls: 0.4590, decode.d2.loss_mask: 0.5754, decode.d2.loss_dice: 0.8416, decode.d3.loss_cls: 0.4378, decode.d3.loss_mask: 0.5686, decode.d3.loss_dice: 0.8305, decode.d4.loss_cls: 0.4317, decode.d4.loss_mask: 0.5683, decode.d4.loss_dice: 0.8295, decode.d5.loss_cls: 0.4261, decode.d5.loss_mask: 0.5694, decode.d5.loss_dice: 0.8238, decode.d6.loss_cls: 0.4213, decode.d6.loss_mask: 0.5677, decode.d6.loss_dice: 0.8223, decode.d7.loss_cls: 0.4202, decode.d7.loss_mask: 0.5693, decode.d7.loss_dice: 0.8261, loss: 19.7765
2022-10-29 14:25:15,188 - mmseg - INFO - Iter [4950/20000]	lr: 2.297e-06, eta: 10:01:42, time: 2.337, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4093, decode.loss_mask: 0.5657, decode.loss_dice: 0.8370, decode.d0.loss_cls: 3.4614, decode.d0.loss_mask: 0.5709, decode.d0.loss_dice: 0.9115, decode.d1.loss_cls: 0.5025, decode.d1.loss_mask: 0.5789, decode.d1.loss_dice: 0.8758, decode.d2.loss_cls: 0.4441, decode.d2.loss_mask: 0.5712, decode.d2.loss_dice: 0.8531, decode.d3.loss_cls: 0.4226, decode.d3.loss_mask: 0.5662, decode.d3.loss_dice: 0.8388, decode.d4.loss_cls: 0.4194, decode.d4.loss_mask: 0.5638, decode.d4.loss_dice: 0.8366, decode.d5.loss_cls: 0.4158, decode.d5.loss_mask: 0.5642, decode.d5.loss_dice: 0.8331, decode.d6.loss_cls: 0.4102, decode.d6.loss_mask: 0.5634, decode.d6.loss_dice: 0.8348, decode.d7.loss_cls: 0.4074, decode.d7.loss_mask: 0.5640, decode.d7.loss_dice: 0.8385, loss: 19.6600
2022-10-29 14:27:09,155 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-29 14:27:09,156 - mmseg - INFO - Iter [5000/20000]	lr: 2.289e-06, eta: 9:59:24, time: 2.279, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4097, decode.loss_mask: 0.5682, decode.loss_dice: 0.8317, decode.d0.loss_cls: 3.4196, decode.d0.loss_mask: 0.5738, decode.d0.loss_dice: 0.9014, decode.d1.loss_cls: 0.4953, decode.d1.loss_mask: 0.5849, decode.d1.loss_dice: 0.8743, decode.d2.loss_cls: 0.4490, decode.d2.loss_mask: 0.5724, decode.d2.loss_dice: 0.8483, decode.d3.loss_cls: 0.4262, decode.d3.loss_mask: 0.5688, decode.d3.loss_dice: 0.8364, decode.d4.loss_cls: 0.4209, decode.d4.loss_mask: 0.5675, decode.d4.loss_dice: 0.8321, decode.d5.loss_cls: 0.4127, decode.d5.loss_mask: 0.5681, decode.d5.loss_dice: 0.8317, decode.d6.loss_cls: 0.4101, decode.d6.loss_mask: 0.5665, decode.d6.loss_dice: 0.8284, decode.d7.loss_cls: 0.4093, decode.d7.loss_mask: 0.5680, decode.d7.loss_dice: 0.8297, loss: 19.6049
2022-10-29 14:29:09,459 - mmseg - INFO - Iter [5050/20000]	lr: 2.282e-06, eta: 9:57:25, time: 2.406, data_time: 0.012, memory: 35422, decode.loss_cls: 0.4188, decode.loss_mask: 0.5693, decode.loss_dice: 0.8346, decode.d0.loss_cls: 3.3595, decode.d0.loss_mask: 0.5769, decode.d0.loss_dice: 0.9090, decode.d1.loss_cls: 0.4987, decode.d1.loss_mask: 0.5834, decode.d1.loss_dice: 0.8851, decode.d2.loss_cls: 0.4579, decode.d2.loss_mask: 0.5734, decode.d2.loss_dice: 0.8517, decode.d3.loss_cls: 0.4319, decode.d3.loss_mask: 0.5702, decode.d3.loss_dice: 0.8416, decode.d4.loss_cls: 0.4276, decode.d4.loss_mask: 0.5711, decode.d4.loss_dice: 0.8437, decode.d5.loss_cls: 0.4218, decode.d5.loss_mask: 0.5676, decode.d5.loss_dice: 0.8388, decode.d6.loss_cls: 0.4226, decode.d6.loss_mask: 0.5657, decode.d6.loss_dice: 0.8318, decode.d7.loss_cls: 0.4203, decode.d7.loss_mask: 0.5690, decode.d7.loss_dice: 0.8363, loss: 19.6783
2022-10-29 14:31:12,182 - mmseg - INFO - Iter [5100/20000]	lr: 2.274e-06, eta: 9:55:34, time: 2.454, data_time: 0.057, memory: 35422, decode.loss_cls: 0.3979, decode.loss_mask: 0.5665, decode.loss_dice: 0.8257, decode.d0.loss_cls: 3.3047, decode.d0.loss_mask: 0.5812, decode.d0.loss_dice: 0.9021, decode.d1.loss_cls: 0.4848, decode.d1.loss_mask: 0.5843, decode.d1.loss_dice: 0.8728, decode.d2.loss_cls: 0.4339, decode.d2.loss_mask: 0.5749, decode.d2.loss_dice: 0.8431, decode.d3.loss_cls: 0.4141, decode.d3.loss_mask: 0.5686, decode.d3.loss_dice: 0.8287, decode.d4.loss_cls: 0.4054, decode.d4.loss_mask: 0.5677, decode.d4.loss_dice: 0.8275, decode.d5.loss_cls: 0.4008, decode.d5.loss_mask: 0.5650, decode.d5.loss_dice: 0.8269, decode.d6.loss_cls: 0.3981, decode.d6.loss_mask: 0.5657, decode.d6.loss_dice: 0.8243, decode.d7.loss_cls: 0.3959, decode.d7.loss_mask: 0.5663, decode.d7.loss_dice: 0.8252, loss: 19.3522
2022-10-29 14:33:10,630 - mmseg - INFO - Iter [5150/20000]	lr: 2.266e-06, eta: 9:53:30, time: 2.369, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3952, decode.loss_mask: 0.5595, decode.loss_dice: 0.8235, decode.d0.loss_cls: 3.2445, decode.d0.loss_mask: 0.5701, decode.d0.loss_dice: 0.8955, decode.d1.loss_cls: 0.4826, decode.d1.loss_mask: 0.5767, decode.d1.loss_dice: 0.8638, decode.d2.loss_cls: 0.4333, decode.d2.loss_mask: 0.5646, decode.d2.loss_dice: 0.8342, decode.d3.loss_cls: 0.4078, decode.d3.loss_mask: 0.5615, decode.d3.loss_dice: 0.8262, decode.d4.loss_cls: 0.4027, decode.d4.loss_mask: 0.5607, decode.d4.loss_dice: 0.8258, decode.d5.loss_cls: 0.3958, decode.d5.loss_mask: 0.5599, decode.d5.loss_dice: 0.8255, decode.d6.loss_cls: 0.3955, decode.d6.loss_mask: 0.5582, decode.d6.loss_dice: 0.8193, decode.d7.loss_cls: 0.3944, decode.d7.loss_mask: 0.5583, decode.d7.loss_dice: 0.8213, loss: 19.1564
2022-10-29 14:35:13,485 - mmseg - INFO - Iter [5200/20000]	lr: 2.259e-06, eta: 9:51:38, time: 2.457, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3988, decode.loss_mask: 0.5527, decode.loss_dice: 0.8225, decode.d0.loss_cls: 3.2102, decode.d0.loss_mask: 0.5615, decode.d0.loss_dice: 0.8976, decode.d1.loss_cls: 0.4831, decode.d1.loss_mask: 0.5697, decode.d1.loss_dice: 0.8690, decode.d2.loss_cls: 0.4353, decode.d2.loss_mask: 0.5562, decode.d2.loss_dice: 0.8382, decode.d3.loss_cls: 0.4101, decode.d3.loss_mask: 0.5537, decode.d3.loss_dice: 0.8262, decode.d4.loss_cls: 0.4042, decode.d4.loss_mask: 0.5537, decode.d4.loss_dice: 0.8248, decode.d5.loss_cls: 0.4016, decode.d5.loss_mask: 0.5516, decode.d5.loss_dice: 0.8254, decode.d6.loss_cls: 0.3982, decode.d6.loss_mask: 0.5511, decode.d6.loss_dice: 0.8185, decode.d7.loss_cls: 0.3982, decode.d7.loss_mask: 0.5532, decode.d7.loss_dice: 0.8214, loss: 19.0869
2022-10-29 14:37:14,704 - mmseg - INFO - Iter [5250/20000]	lr: 2.251e-06, eta: 9:49:42, time: 2.424, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4028, decode.loss_mask: 0.5567, decode.loss_dice: 0.8196, decode.d0.loss_cls: 3.1753, decode.d0.loss_mask: 0.5707, decode.d0.loss_dice: 0.9001, decode.d1.loss_cls: 0.4827, decode.d1.loss_mask: 0.5748, decode.d1.loss_dice: 0.8618, decode.d2.loss_cls: 0.4378, decode.d2.loss_mask: 0.5636, decode.d2.loss_dice: 0.8351, decode.d3.loss_cls: 0.4139, decode.d3.loss_mask: 0.5606, decode.d3.loss_dice: 0.8218, decode.d4.loss_cls: 0.4073, decode.d4.loss_mask: 0.5599, decode.d4.loss_dice: 0.8221, decode.d5.loss_cls: 0.4036, decode.d5.loss_mask: 0.5595, decode.d5.loss_dice: 0.8194, decode.d6.loss_cls: 0.4024, decode.d6.loss_mask: 0.5573, decode.d6.loss_dice: 0.8151, decode.d7.loss_cls: 0.4021, decode.d7.loss_mask: 0.5578, decode.d7.loss_dice: 0.8180, loss: 19.1018
2022-10-29 14:39:14,619 - mmseg - INFO - Iter [5300/20000]	lr: 2.243e-06, eta: 9:47:42, time: 2.398, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4056, decode.loss_mask: 0.5523, decode.loss_dice: 0.8181, decode.d0.loss_cls: 3.1397, decode.d0.loss_mask: 0.5639, decode.d0.loss_dice: 0.9002, decode.d1.loss_cls: 0.4936, decode.d1.loss_mask: 0.5726, decode.d1.loss_dice: 0.8658, decode.d2.loss_cls: 0.4462, decode.d2.loss_mask: 0.5595, decode.d2.loss_dice: 0.8414, decode.d3.loss_cls: 0.4222, decode.d3.loss_mask: 0.5538, decode.d3.loss_dice: 0.8253, decode.d4.loss_cls: 0.4118, decode.d4.loss_mask: 0.5529, decode.d4.loss_dice: 0.8235, decode.d5.loss_cls: 0.4108, decode.d5.loss_mask: 0.5522, decode.d5.loss_dice: 0.8222, decode.d6.loss_cls: 0.4070, decode.d6.loss_mask: 0.5504, decode.d6.loss_dice: 0.8194, decode.d7.loss_cls: 0.4049, decode.d7.loss_mask: 0.5509, decode.d7.loss_dice: 0.8219, loss: 19.0883
2022-10-29 14:41:11,757 - mmseg - INFO - Iter [5350/20000]	lr: 2.236e-06, eta: 9:45:34, time: 2.343, data_time: 0.013, memory: 35422, decode.loss_cls: 0.4005, decode.loss_mask: 0.5613, decode.loss_dice: 0.8231, decode.d0.loss_cls: 3.0720, decode.d0.loss_mask: 0.5760, decode.d0.loss_dice: 0.9028, decode.d1.loss_cls: 0.4832, decode.d1.loss_mask: 0.5790, decode.d1.loss_dice: 0.8685, decode.d2.loss_cls: 0.4301, decode.d2.loss_mask: 0.5698, decode.d2.loss_dice: 0.8439, decode.d3.loss_cls: 0.4110, decode.d3.loss_mask: 0.5640, decode.d3.loss_dice: 0.8318, decode.d4.loss_cls: 0.4085, decode.d4.loss_mask: 0.5624, decode.d4.loss_dice: 0.8283, decode.d5.loss_cls: 0.4021, decode.d5.loss_mask: 0.5611, decode.d5.loss_dice: 0.8273, decode.d6.loss_cls: 0.4013, decode.d6.loss_mask: 0.5600, decode.d6.loss_dice: 0.8203, decode.d7.loss_cls: 0.3999, decode.d7.loss_mask: 0.5613, decode.d7.loss_dice: 0.8228, loss: 19.0726
2022-10-29 14:43:09,919 - mmseg - INFO - Iter [5400/20000]	lr: 2.228e-06, eta: 9:43:30, time: 2.363, data_time: 0.055, memory: 35422, decode.loss_cls: 0.3872, decode.loss_mask: 0.5463, decode.loss_dice: 0.8147, decode.d0.loss_cls: 3.0506, decode.d0.loss_mask: 0.5567, decode.d0.loss_dice: 0.8917, decode.d1.loss_cls: 0.4669, decode.d1.loss_mask: 0.5657, decode.d1.loss_dice: 0.8613, decode.d2.loss_cls: 0.4205, decode.d2.loss_mask: 0.5536, decode.d2.loss_dice: 0.8326, decode.d3.loss_cls: 0.4003, decode.d3.loss_mask: 0.5478, decode.d3.loss_dice: 0.8178, decode.d4.loss_cls: 0.3921, decode.d4.loss_mask: 0.5494, decode.d4.loss_dice: 0.8207, decode.d5.loss_cls: 0.3871, decode.d5.loss_mask: 0.5465, decode.d5.loss_dice: 0.8161, decode.d6.loss_cls: 0.3863, decode.d6.loss_mask: 0.5450, decode.d6.loss_dice: 0.8121, decode.d7.loss_cls: 0.3847, decode.d7.loss_mask: 0.5459, decode.d7.loss_dice: 0.8135, loss: 18.7130
2022-10-29 14:45:08,216 - mmseg - INFO - Iter [5450/20000]	lr: 2.221e-06, eta: 9:41:25, time: 2.366, data_time: 0.011, memory: 35422, decode.loss_cls: 0.3846, decode.loss_mask: 0.5544, decode.loss_dice: 0.8108, decode.d0.loss_cls: 3.0002, decode.d0.loss_mask: 0.5648, decode.d0.loss_dice: 0.8901, decode.d1.loss_cls: 0.4666, decode.d1.loss_mask: 0.5730, decode.d1.loss_dice: 0.8553, decode.d2.loss_cls: 0.4202, decode.d2.loss_mask: 0.5593, decode.d2.loss_dice: 0.8256, decode.d3.loss_cls: 0.4000, decode.d3.loss_mask: 0.5551, decode.d3.loss_dice: 0.8130, decode.d4.loss_cls: 0.3943, decode.d4.loss_mask: 0.5536, decode.d4.loss_dice: 0.8118, decode.d5.loss_cls: 0.3914, decode.d5.loss_mask: 0.5523, decode.d5.loss_dice: 0.8111, decode.d6.loss_cls: 0.3874, decode.d6.loss_mask: 0.5510, decode.d6.loss_dice: 0.8077, decode.d7.loss_cls: 0.3845, decode.d7.loss_mask: 0.5539, decode.d7.loss_dice: 0.8095, loss: 18.6814
2022-10-29 14:47:05,740 - mmseg - INFO - Iter [5500/20000]	lr: 2.213e-06, eta: 9:39:19, time: 2.350, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3831, decode.loss_mask: 0.5494, decode.loss_dice: 0.8069, decode.d0.loss_cls: 2.9442, decode.d0.loss_mask: 0.5618, decode.d0.loss_dice: 0.8860, decode.d1.loss_cls: 0.4673, decode.d1.loss_mask: 0.5661, decode.d1.loss_dice: 0.8518, decode.d2.loss_cls: 0.4208, decode.d2.loss_mask: 0.5525, decode.d2.loss_dice: 0.8189, decode.d3.loss_cls: 0.3972, decode.d3.loss_mask: 0.5502, decode.d3.loss_dice: 0.8107, decode.d4.loss_cls: 0.3899, decode.d4.loss_mask: 0.5498, decode.d4.loss_dice: 0.8096, decode.d5.loss_cls: 0.3866, decode.d5.loss_mask: 0.5489, decode.d5.loss_dice: 0.8090, decode.d6.loss_cls: 0.3828, decode.d6.loss_mask: 0.5484, decode.d6.loss_dice: 0.8037, decode.d7.loss_cls: 0.3845, decode.d7.loss_mask: 0.5471, decode.d7.loss_dice: 0.8077, loss: 18.5349
2022-10-29 14:49:03,323 - mmseg - INFO - Iter [5550/20000]	lr: 2.205e-06, eta: 9:37:14, time: 2.352, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3910, decode.loss_mask: 0.5607, decode.loss_dice: 0.8265, decode.d0.loss_cls: 2.9094, decode.d0.loss_mask: 0.5752, decode.d0.loss_dice: 0.9065, decode.d1.loss_cls: 0.4742, decode.d1.loss_mask: 0.5765, decode.d1.loss_dice: 0.8698, decode.d2.loss_cls: 0.4264, decode.d2.loss_mask: 0.5686, decode.d2.loss_dice: 0.8375, decode.d3.loss_cls: 0.4048, decode.d3.loss_mask: 0.5642, decode.d3.loss_dice: 0.8262, decode.d4.loss_cls: 0.3999, decode.d4.loss_mask: 0.5624, decode.d4.loss_dice: 0.8229, decode.d5.loss_cls: 0.3938, decode.d5.loss_mask: 0.5602, decode.d5.loss_dice: 0.8220, decode.d6.loss_cls: 0.3928, decode.d6.loss_mask: 0.5610, decode.d6.loss_dice: 0.8232, decode.d7.loss_cls: 0.3933, decode.d7.loss_mask: 0.5617, decode.d7.loss_dice: 0.8239, loss: 18.8347
2022-10-29 14:51:05,385 - mmseg - INFO - Iter [5600/20000]	lr: 2.198e-06, eta: 9:35:19, time: 2.441, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3891, decode.loss_mask: 0.5693, decode.loss_dice: 0.8330, decode.d0.loss_cls: 2.8847, decode.d0.loss_mask: 0.5829, decode.d0.loss_dice: 0.9106, decode.d1.loss_cls: 0.4765, decode.d1.loss_mask: 0.5846, decode.d1.loss_dice: 0.8764, decode.d2.loss_cls: 0.4256, decode.d2.loss_mask: 0.5757, decode.d2.loss_dice: 0.8449, decode.d3.loss_cls: 0.4032, decode.d3.loss_mask: 0.5704, decode.d3.loss_dice: 0.8363, decode.d4.loss_cls: 0.3954, decode.d4.loss_mask: 0.5700, decode.d4.loss_dice: 0.8348, decode.d5.loss_cls: 0.3946, decode.d5.loss_mask: 0.5691, decode.d5.loss_dice: 0.8311, decode.d6.loss_cls: 0.3898, decode.d6.loss_mask: 0.5695, decode.d6.loss_dice: 0.8308, decode.d7.loss_cls: 0.3902, decode.d7.loss_mask: 0.5691, decode.d7.loss_dice: 0.8304, loss: 18.9380
2022-10-29 14:53:02,689 - mmseg - INFO - Iter [5650/20000]	lr: 2.190e-06, eta: 9:33:13, time: 2.346, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3774, decode.loss_mask: 0.5535, decode.loss_dice: 0.8180, decode.d0.loss_cls: 2.8294, decode.d0.loss_mask: 0.5642, decode.d0.loss_dice: 0.8906, decode.d1.loss_cls: 0.4631, decode.d1.loss_mask: 0.5697, decode.d1.loss_dice: 0.8615, decode.d2.loss_cls: 0.4179, decode.d2.loss_mask: 0.5598, decode.d2.loss_dice: 0.8302, decode.d3.loss_cls: 0.3959, decode.d3.loss_mask: 0.5555, decode.d3.loss_dice: 0.8218, decode.d4.loss_cls: 0.3869, decode.d4.loss_mask: 0.5543, decode.d4.loss_dice: 0.8205, decode.d5.loss_cls: 0.3826, decode.d5.loss_mask: 0.5541, decode.d5.loss_dice: 0.8177, decode.d6.loss_cls: 0.3784, decode.d6.loss_mask: 0.5529, decode.d6.loss_dice: 0.8179, decode.d7.loss_cls: 0.3813, decode.d7.loss_mask: 0.5539, decode.d7.loss_dice: 0.8165, loss: 18.5252
2022-10-29 14:55:03,439 - mmseg - INFO - Iter [5700/20000]	lr: 2.182e-06, eta: 9:31:16, time: 2.415, data_time: 0.057, memory: 35422, decode.loss_cls: 0.3921, decode.loss_mask: 0.5583, decode.loss_dice: 0.8130, decode.d0.loss_cls: 2.8090, decode.d0.loss_mask: 0.5672, decode.d0.loss_dice: 0.8938, decode.d1.loss_cls: 0.4786, decode.d1.loss_mask: 0.5729, decode.d1.loss_dice: 0.8561, decode.d2.loss_cls: 0.4292, decode.d2.loss_mask: 0.5648, decode.d2.loss_dice: 0.8306, decode.d3.loss_cls: 0.4048, decode.d3.loss_mask: 0.5600, decode.d3.loss_dice: 0.8224, decode.d4.loss_cls: 0.3988, decode.d4.loss_mask: 0.5586, decode.d4.loss_dice: 0.8170, decode.d5.loss_cls: 0.3937, decode.d5.loss_mask: 0.5572, decode.d5.loss_dice: 0.8145, decode.d6.loss_cls: 0.3912, decode.d6.loss_mask: 0.5567, decode.d6.loss_dice: 0.8129, decode.d7.loss_cls: 0.3872, decode.d7.loss_mask: 0.5600, decode.d7.loss_dice: 0.8148, loss: 18.6153
2022-10-29 14:57:03,762 - mmseg - INFO - Iter [5750/20000]	lr: 2.175e-06, eta: 9:29:17, time: 2.406, data_time: 0.017, memory: 35422, decode.loss_cls: 0.3691, decode.loss_mask: 0.5524, decode.loss_dice: 0.8075, decode.d0.loss_cls: 2.7678, decode.d0.loss_mask: 0.5623, decode.d0.loss_dice: 0.8815, decode.d1.loss_cls: 0.4594, decode.d1.loss_mask: 0.5657, decode.d1.loss_dice: 0.8459, decode.d2.loss_cls: 0.4089, decode.d2.loss_mask: 0.5564, decode.d2.loss_dice: 0.8263, decode.d3.loss_cls: 0.3873, decode.d3.loss_mask: 0.5524, decode.d3.loss_dice: 0.8097, decode.d4.loss_cls: 0.3790, decode.d4.loss_mask: 0.5511, decode.d4.loss_dice: 0.8111, decode.d5.loss_cls: 0.3754, decode.d5.loss_mask: 0.5508, decode.d5.loss_dice: 0.8106, decode.d6.loss_cls: 0.3702, decode.d6.loss_mask: 0.5511, decode.d6.loss_dice: 0.8064, decode.d7.loss_cls: 0.3686, decode.d7.loss_mask: 0.5509, decode.d7.loss_dice: 0.8065, loss: 18.2842
2022-10-29 14:58:59,336 - mmseg - INFO - Iter [5800/20000]	lr: 2.167e-06, eta: 9:27:07, time: 2.312, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3825, decode.loss_mask: 0.5455, decode.loss_dice: 0.8037, decode.d0.loss_cls: 2.7546, decode.d0.loss_mask: 0.5599, decode.d0.loss_dice: 0.8867, decode.d1.loss_cls: 0.4709, decode.d1.loss_mask: 0.5630, decode.d1.loss_dice: 0.8563, decode.d2.loss_cls: 0.4196, decode.d2.loss_mask: 0.5514, decode.d2.loss_dice: 0.8234, decode.d3.loss_cls: 0.3968, decode.d3.loss_mask: 0.5463, decode.d3.loss_dice: 0.8093, decode.d4.loss_cls: 0.3923, decode.d4.loss_mask: 0.5457, decode.d4.loss_dice: 0.8073, decode.d5.loss_cls: 0.3855, decode.d5.loss_mask: 0.5472, decode.d5.loss_dice: 0.8076, decode.d6.loss_cls: 0.3829, decode.d6.loss_mask: 0.5443, decode.d6.loss_dice: 0.8039, decode.d7.loss_cls: 0.3806, decode.d7.loss_mask: 0.5450, decode.d7.loss_dice: 0.8050, loss: 18.3172
2022-10-29 15:00:55,639 - mmseg - INFO - Iter [5850/20000]	lr: 2.160e-06, eta: 9:24:58, time: 2.326, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3693, decode.loss_mask: 0.5447, decode.loss_dice: 0.8107, decode.d0.loss_cls: 2.7096, decode.d0.loss_mask: 0.5568, decode.d0.loss_dice: 0.8889, decode.d1.loss_cls: 0.4498, decode.d1.loss_mask: 0.5627, decode.d1.loss_dice: 0.8560, decode.d2.loss_cls: 0.4070, decode.d2.loss_mask: 0.5512, decode.d2.loss_dice: 0.8293, decode.d3.loss_cls: 0.3864, decode.d3.loss_mask: 0.5460, decode.d3.loss_dice: 0.8113, decode.d4.loss_cls: 0.3795, decode.d4.loss_mask: 0.5462, decode.d4.loss_dice: 0.8110, decode.d5.loss_cls: 0.3760, decode.d5.loss_mask: 0.5434, decode.d5.loss_dice: 0.8104, decode.d6.loss_cls: 0.3716, decode.d6.loss_mask: 0.5446, decode.d6.loss_dice: 0.8072, decode.d7.loss_cls: 0.3698, decode.d7.loss_mask: 0.5439, decode.d7.loss_dice: 0.8079, loss: 18.1912
2022-10-29 15:02:51,635 - mmseg - INFO - Iter [5900/20000]	lr: 2.152e-06, eta: 9:22:49, time: 2.320, data_time: 0.014, memory: 35422, decode.loss_cls: 0.3835, decode.loss_mask: 0.5486, decode.loss_dice: 0.8232, decode.d0.loss_cls: 2.6809, decode.d0.loss_mask: 0.5623, decode.d0.loss_dice: 0.9068, decode.d1.loss_cls: 0.4709, decode.d1.loss_mask: 0.5674, decode.d1.loss_dice: 0.8733, decode.d2.loss_cls: 0.4208, decode.d2.loss_mask: 0.5559, decode.d2.loss_dice: 0.8424, decode.d3.loss_cls: 0.3972, decode.d3.loss_mask: 0.5494, decode.d3.loss_dice: 0.8268, decode.d4.loss_cls: 0.3916, decode.d4.loss_mask: 0.5488, decode.d4.loss_dice: 0.8289, decode.d5.loss_cls: 0.3858, decode.d5.loss_mask: 0.5494, decode.d5.loss_dice: 0.8266, decode.d6.loss_cls: 0.3841, decode.d6.loss_mask: 0.5468, decode.d6.loss_dice: 0.8232, decode.d7.loss_cls: 0.3827, decode.d7.loss_mask: 0.5468, decode.d7.loss_dice: 0.8211, loss: 18.4454
2022-10-29 15:04:48,649 - mmseg - INFO - Iter [5950/20000]	lr: 2.144e-06, eta: 9:20:43, time: 2.340, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3809, decode.loss_mask: 0.5549, decode.loss_dice: 0.8126, decode.d0.loss_cls: 2.6213, decode.d0.loss_mask: 0.5680, decode.d0.loss_dice: 0.8918, decode.d1.loss_cls: 0.4621, decode.d1.loss_mask: 0.5728, decode.d1.loss_dice: 0.8596, decode.d2.loss_cls: 0.4146, decode.d2.loss_mask: 0.5610, decode.d2.loss_dice: 0.8307, decode.d3.loss_cls: 0.3922, decode.d3.loss_mask: 0.5566, decode.d3.loss_dice: 0.8167, decode.d4.loss_cls: 0.3861, decode.d4.loss_mask: 0.5552, decode.d4.loss_dice: 0.8163, decode.d5.loss_cls: 0.3827, decode.d5.loss_mask: 0.5554, decode.d5.loss_dice: 0.8139, decode.d6.loss_cls: 0.3796, decode.d6.loss_mask: 0.5525, decode.d6.loss_dice: 0.8105, decode.d7.loss_cls: 0.3785, decode.d7.loss_mask: 0.5553, decode.d7.loss_dice: 0.8120, loss: 18.2938
2022-10-29 15:06:43,742 - mmseg - INFO - Saving checkpoint at 6000 iterations
2022-10-29 15:07:17,186 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-29 15:07:17,187 - mmseg - INFO - Iter [6000/20000]	lr: 2.137e-06, eta: 9:19:51, time: 2.971, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3836, decode.loss_mask: 0.5529, decode.loss_dice: 0.8091, decode.d0.loss_cls: 2.6157, decode.d0.loss_mask: 0.5622, decode.d0.loss_dice: 0.8879, decode.d1.loss_cls: 0.4652, decode.d1.loss_mask: 0.5675, decode.d1.loss_dice: 0.8577, decode.d2.loss_cls: 0.4162, decode.d2.loss_mask: 0.5585, decode.d2.loss_dice: 0.8294, decode.d3.loss_cls: 0.3966, decode.d3.loss_mask: 0.5552, decode.d3.loss_dice: 0.8175, decode.d4.loss_cls: 0.3898, decode.d4.loss_mask: 0.5538, decode.d4.loss_dice: 0.8167, decode.d5.loss_cls: 0.3888, decode.d5.loss_mask: 0.5507, decode.d5.loss_dice: 0.8124, decode.d6.loss_cls: 0.3869, decode.d6.loss_mask: 0.5505, decode.d6.loss_dice: 0.8120, decode.d7.loss_cls: 0.3847, decode.d7.loss_mask: 0.5508, decode.d7.loss_dice: 0.8123, loss: 18.2846
2022-10-29 15:09:16,936 - mmseg - INFO - Iter [6050/20000]	lr: 2.129e-06, eta: 9:17:50, time: 2.395, data_time: 0.057, memory: 35422, decode.loss_cls: 0.3677, decode.loss_mask: 0.5414, decode.loss_dice: 0.7943, decode.d0.loss_cls: 2.5680, decode.d0.loss_mask: 0.5557, decode.d0.loss_dice: 0.8746, decode.d1.loss_cls: 0.4436, decode.d1.loss_mask: 0.5598, decode.d1.loss_dice: 0.8436, decode.d2.loss_cls: 0.4008, decode.d2.loss_mask: 0.5484, decode.d2.loss_dice: 0.8191, decode.d3.loss_cls: 0.3792, decode.d3.loss_mask: 0.5427, decode.d3.loss_dice: 0.8058, decode.d4.loss_cls: 0.3750, decode.d4.loss_mask: 0.5413, decode.d4.loss_dice: 0.8045, decode.d5.loss_cls: 0.3728, decode.d5.loss_mask: 0.5403, decode.d5.loss_dice: 0.7979, decode.d6.loss_cls: 0.3681, decode.d6.loss_mask: 0.5402, decode.d6.loss_dice: 0.7950, decode.d7.loss_cls: 0.3697, decode.d7.loss_mask: 0.5407, decode.d7.loss_dice: 0.7957, loss: 17.8859
2022-10-29 15:11:13,487 - mmseg - INFO - Iter [6100/20000]	lr: 2.121e-06, eta: 9:15:42, time: 2.331, data_time: 0.014, memory: 35422, decode.loss_cls: 0.3813, decode.loss_mask: 0.5522, decode.loss_dice: 0.8184, decode.d0.loss_cls: 2.5611, decode.d0.loss_mask: 0.5668, decode.d0.loss_dice: 0.9029, decode.d1.loss_cls: 0.4575, decode.d1.loss_mask: 0.5713, decode.d1.loss_dice: 0.8675, decode.d2.loss_cls: 0.4210, decode.d2.loss_mask: 0.5591, decode.d2.loss_dice: 0.8413, decode.d3.loss_cls: 0.3974, decode.d3.loss_mask: 0.5538, decode.d3.loss_dice: 0.8281, decode.d4.loss_cls: 0.3938, decode.d4.loss_mask: 0.5520, decode.d4.loss_dice: 0.8234, decode.d5.loss_cls: 0.3871, decode.d5.loss_mask: 0.5517, decode.d5.loss_dice: 0.8217, decode.d6.loss_cls: 0.3851, decode.d6.loss_mask: 0.5507, decode.d6.loss_dice: 0.8198, decode.d7.loss_cls: 0.3825, decode.d7.loss_mask: 0.5532, decode.d7.loss_dice: 0.8192, loss: 18.3201
2022-10-29 15:13:12,435 - mmseg - INFO - Iter [6150/20000]	lr: 2.114e-06, eta: 9:13:40, time: 2.379, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3688, decode.loss_mask: 0.5425, decode.loss_dice: 0.8016, decode.d0.loss_cls: 2.5122, decode.d0.loss_mask: 0.5566, decode.d0.loss_dice: 0.8817, decode.d1.loss_cls: 0.4503, decode.d1.loss_mask: 0.5595, decode.d1.loss_dice: 0.8491, decode.d2.loss_cls: 0.4006, decode.d2.loss_mask: 0.5483, decode.d2.loss_dice: 0.8225, decode.d3.loss_cls: 0.3820, decode.d3.loss_mask: 0.5453, decode.d3.loss_dice: 0.8066, decode.d4.loss_cls: 0.3732, decode.d4.loss_mask: 0.5447, decode.d4.loss_dice: 0.8054, decode.d5.loss_cls: 0.3708, decode.d5.loss_mask: 0.5421, decode.d5.loss_dice: 0.8037, decode.d6.loss_cls: 0.3682, decode.d6.loss_mask: 0.5427, decode.d6.loss_dice: 0.8048, decode.d7.loss_cls: 0.3696, decode.d7.loss_mask: 0.5430, decode.d7.loss_dice: 0.8028, loss: 17.8984
2022-10-29 15:15:09,233 - mmseg - INFO - Iter [6200/20000]	lr: 2.106e-06, eta: 9:11:33, time: 2.336, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3643, decode.loss_mask: 0.5535, decode.loss_dice: 0.8141, decode.d0.loss_cls: 2.5008, decode.d0.loss_mask: 0.5679, decode.d0.loss_dice: 0.8875, decode.d1.loss_cls: 0.4425, decode.d1.loss_mask: 0.5720, decode.d1.loss_dice: 0.8527, decode.d2.loss_cls: 0.3959, decode.d2.loss_mask: 0.5597, decode.d2.loss_dice: 0.8286, decode.d3.loss_cls: 0.3789, decode.d3.loss_mask: 0.5550, decode.d3.loss_dice: 0.8165, decode.d4.loss_cls: 0.3726, decode.d4.loss_mask: 0.5542, decode.d4.loss_dice: 0.8145, decode.d5.loss_cls: 0.3663, decode.d5.loss_mask: 0.5529, decode.d5.loss_dice: 0.8142, decode.d6.loss_cls: 0.3634, decode.d6.loss_mask: 0.5529, decode.d6.loss_dice: 0.8079, decode.d7.loss_cls: 0.3638, decode.d7.loss_mask: 0.5524, decode.d7.loss_dice: 0.8129, loss: 18.0180
2022-10-29 15:17:07,454 - mmseg - INFO - Iter [6250/20000]	lr: 2.099e-06, eta: 9:09:30, time: 2.364, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3863, decode.loss_mask: 0.5560, decode.loss_dice: 0.8132, decode.d0.loss_cls: 2.4903, decode.d0.loss_mask: 0.5730, decode.d0.loss_dice: 0.8992, decode.d1.loss_cls: 0.4704, decode.d1.loss_mask: 0.5750, decode.d1.loss_dice: 0.8648, decode.d2.loss_cls: 0.4228, decode.d2.loss_mask: 0.5628, decode.d2.loss_dice: 0.8296, decode.d3.loss_cls: 0.4014, decode.d3.loss_mask: 0.5584, decode.d3.loss_dice: 0.8168, decode.d4.loss_cls: 0.3961, decode.d4.loss_mask: 0.5580, decode.d4.loss_dice: 0.8139, decode.d5.loss_cls: 0.3883, decode.d5.loss_mask: 0.5567, decode.d5.loss_dice: 0.8151, decode.d6.loss_cls: 0.3862, decode.d6.loss_mask: 0.5558, decode.d6.loss_dice: 0.8123, decode.d7.loss_cls: 0.3864, decode.d7.loss_mask: 0.5567, decode.d7.loss_dice: 0.8144, loss: 18.2600
2022-10-29 15:19:08,243 - mmseg - INFO - Iter [6300/20000]	lr: 2.091e-06, eta: 9:07:32, time: 2.416, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3881, decode.loss_mask: 0.5532, decode.loss_dice: 0.8046, decode.d0.loss_cls: 2.4623, decode.d0.loss_mask: 0.5657, decode.d0.loss_dice: 0.8793, decode.d1.loss_cls: 0.4667, decode.d1.loss_mask: 0.5690, decode.d1.loss_dice: 0.8516, decode.d2.loss_cls: 0.4231, decode.d2.loss_mask: 0.5601, decode.d2.loss_dice: 0.8247, decode.d3.loss_cls: 0.4029, decode.d3.loss_mask: 0.5536, decode.d3.loss_dice: 0.8089, decode.d4.loss_cls: 0.3936, decode.d4.loss_mask: 0.5536, decode.d4.loss_dice: 0.8070, decode.d5.loss_cls: 0.3903, decode.d5.loss_mask: 0.5557, decode.d5.loss_dice: 0.8083, decode.d6.loss_cls: 0.3867, decode.d6.loss_mask: 0.5529, decode.d6.loss_dice: 0.8013, decode.d7.loss_cls: 0.3875, decode.d7.loss_mask: 0.5529, decode.d7.loss_dice: 0.8042, loss: 18.1077
2022-10-29 15:21:08,320 - mmseg - INFO - Iter [6350/20000]	lr: 2.083e-06, eta: 9:05:32, time: 2.402, data_time: 0.056, memory: 35422, decode.loss_cls: 0.3735, decode.loss_mask: 0.5480, decode.loss_dice: 0.7996, decode.d0.loss_cls: 2.4410, decode.d0.loss_mask: 0.5672, decode.d0.loss_dice: 0.8812, decode.d1.loss_cls: 0.4496, decode.d1.loss_mask: 0.5674, decode.d1.loss_dice: 0.8486, decode.d2.loss_cls: 0.4059, decode.d2.loss_mask: 0.5562, decode.d2.loss_dice: 0.8180, decode.d3.loss_cls: 0.3865, decode.d3.loss_mask: 0.5518, decode.d3.loss_dice: 0.7999, decode.d4.loss_cls: 0.3796, decode.d4.loss_mask: 0.5516, decode.d4.loss_dice: 0.8032, decode.d5.loss_cls: 0.3763, decode.d5.loss_mask: 0.5487, decode.d5.loss_dice: 0.8018, decode.d6.loss_cls: 0.3746, decode.d6.loss_mask: 0.5478, decode.d6.loss_dice: 0.7970, decode.d7.loss_cls: 0.3721, decode.d7.loss_mask: 0.5481, decode.d7.loss_dice: 0.7992, loss: 17.8946
2022-10-29 15:23:07,391 - mmseg - INFO - Iter [6400/20000]	lr: 2.076e-06, eta: 9:03:31, time: 2.381, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3601, decode.loss_mask: 0.5457, decode.loss_dice: 0.7927, decode.d0.loss_cls: 2.3962, decode.d0.loss_mask: 0.5592, decode.d0.loss_dice: 0.8703, decode.d1.loss_cls: 0.4318, decode.d1.loss_mask: 0.5659, decode.d1.loss_dice: 0.8448, decode.d2.loss_cls: 0.3905, decode.d2.loss_mask: 0.5527, decode.d2.loss_dice: 0.8117, decode.d3.loss_cls: 0.3720, decode.d3.loss_mask: 0.5488, decode.d3.loss_dice: 0.7997, decode.d4.loss_cls: 0.3645, decode.d4.loss_mask: 0.5484, decode.d4.loss_dice: 0.7988, decode.d5.loss_cls: 0.3609, decode.d5.loss_mask: 0.5461, decode.d5.loss_dice: 0.7967, decode.d6.loss_cls: 0.3611, decode.d6.loss_mask: 0.5453, decode.d6.loss_dice: 0.7918, decode.d7.loss_cls: 0.3590, decode.d7.loss_mask: 0.5457, decode.d7.loss_dice: 0.7942, loss: 17.6547
2022-10-29 15:25:07,449 - mmseg - INFO - Iter [6450/20000]	lr: 2.068e-06, eta: 9:01:31, time: 2.401, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3501, decode.loss_mask: 0.5352, decode.loss_dice: 0.7885, decode.d0.loss_cls: 2.3860, decode.d0.loss_mask: 0.5518, decode.d0.loss_dice: 0.8636, decode.d1.loss_cls: 0.4317, decode.d1.loss_mask: 0.5522, decode.d1.loss_dice: 0.8308, decode.d2.loss_cls: 0.3850, decode.d2.loss_mask: 0.5443, decode.d2.loss_dice: 0.8033, decode.d3.loss_cls: 0.3634, decode.d3.loss_mask: 0.5368, decode.d3.loss_dice: 0.7872, decode.d4.loss_cls: 0.3594, decode.d4.loss_mask: 0.5349, decode.d4.loss_dice: 0.7893, decode.d5.loss_cls: 0.3542, decode.d5.loss_mask: 0.5336, decode.d5.loss_dice: 0.7879, decode.d6.loss_cls: 0.3500, decode.d6.loss_mask: 0.5341, decode.d6.loss_dice: 0.7870, decode.d7.loss_cls: 0.3498, decode.d7.loss_mask: 0.5351, decode.d7.loss_dice: 0.7882, loss: 17.4133
2022-10-29 15:27:08,433 - mmseg - INFO - Iter [6500/20000]	lr: 2.060e-06, eta: 8:59:33, time: 2.420, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3497, decode.loss_mask: 0.5373, decode.loss_dice: 0.8009, decode.d0.loss_cls: 2.3484, decode.d0.loss_mask: 0.5505, decode.d0.loss_dice: 0.8782, decode.d1.loss_cls: 0.4326, decode.d1.loss_mask: 0.5536, decode.d1.loss_dice: 0.8416, decode.d2.loss_cls: 0.3870, decode.d2.loss_mask: 0.5415, decode.d2.loss_dice: 0.8141, decode.d3.loss_cls: 0.3642, decode.d3.loss_mask: 0.5384, decode.d3.loss_dice: 0.8024, decode.d4.loss_cls: 0.3581, decode.d4.loss_mask: 0.5365, decode.d4.loss_dice: 0.8031, decode.d5.loss_cls: 0.3516, decode.d5.loss_mask: 0.5365, decode.d5.loss_dice: 0.8008, decode.d6.loss_cls: 0.3502, decode.d6.loss_mask: 0.5361, decode.d6.loss_dice: 0.7968, decode.d7.loss_cls: 0.3473, decode.d7.loss_mask: 0.5387, decode.d7.loss_dice: 0.7995, loss: 17.4957
2022-10-29 15:29:07,730 - mmseg - INFO - Iter [6550/20000]	lr: 2.053e-06, eta: 8:57:32, time: 2.386, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3587, decode.loss_mask: 0.5376, decode.loss_dice: 0.7916, decode.d0.loss_cls: 2.3457, decode.d0.loss_mask: 0.5522, decode.d0.loss_dice: 0.8740, decode.d1.loss_cls: 0.4357, decode.d1.loss_mask: 0.5565, decode.d1.loss_dice: 0.8383, decode.d2.loss_cls: 0.3893, decode.d2.loss_mask: 0.5449, decode.d2.loss_dice: 0.8094, decode.d3.loss_cls: 0.3693, decode.d3.loss_mask: 0.5393, decode.d3.loss_dice: 0.7946, decode.d4.loss_cls: 0.3672, decode.d4.loss_mask: 0.5370, decode.d4.loss_dice: 0.7951, decode.d5.loss_cls: 0.3633, decode.d5.loss_mask: 0.5381, decode.d5.loss_dice: 0.7930, decode.d6.loss_cls: 0.3601, decode.d6.loss_mask: 0.5373, decode.d6.loss_dice: 0.7868, decode.d7.loss_cls: 0.3574, decode.d7.loss_mask: 0.5377, decode.d7.loss_dice: 0.7915, loss: 17.5016
2022-10-29 15:31:06,195 - mmseg - INFO - Iter [6600/20000]	lr: 2.045e-06, eta: 8:55:30, time: 2.369, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3734, decode.loss_mask: 0.5409, decode.loss_dice: 0.7996, decode.d0.loss_cls: 2.3165, decode.d0.loss_mask: 0.5562, decode.d0.loss_dice: 0.8775, decode.d1.loss_cls: 0.4502, decode.d1.loss_mask: 0.5605, decode.d1.loss_dice: 0.8403, decode.d2.loss_cls: 0.4061, decode.d2.loss_mask: 0.5487, decode.d2.loss_dice: 0.8148, decode.d3.loss_cls: 0.3853, decode.d3.loss_mask: 0.5430, decode.d3.loss_dice: 0.8055, decode.d4.loss_cls: 0.3790, decode.d4.loss_mask: 0.5418, decode.d4.loss_dice: 0.8027, decode.d5.loss_cls: 0.3753, decode.d5.loss_mask: 0.5400, decode.d5.loss_dice: 0.8023, decode.d6.loss_cls: 0.3727, decode.d6.loss_mask: 0.5410, decode.d6.loss_dice: 0.7980, decode.d7.loss_cls: 0.3712, decode.d7.loss_mask: 0.5397, decode.d7.loss_dice: 0.8007, loss: 17.6828
2022-10-29 15:33:02,586 - mmseg - INFO - Iter [6650/20000]	lr: 2.037e-06, eta: 8:53:23, time: 2.328, data_time: 0.058, memory: 35422, decode.loss_cls: 0.3585, decode.loss_mask: 0.5458, decode.loss_dice: 0.7994, decode.d0.loss_cls: 2.2984, decode.d0.loss_mask: 0.5608, decode.d0.loss_dice: 0.8798, decode.d1.loss_cls: 0.4331, decode.d1.loss_mask: 0.5641, decode.d1.loss_dice: 0.8424, decode.d2.loss_cls: 0.3908, decode.d2.loss_mask: 0.5534, decode.d2.loss_dice: 0.8167, decode.d3.loss_cls: 0.3748, decode.d3.loss_mask: 0.5475, decode.d3.loss_dice: 0.8058, decode.d4.loss_cls: 0.3684, decode.d4.loss_mask: 0.5467, decode.d4.loss_dice: 0.8030, decode.d5.loss_cls: 0.3614, decode.d5.loss_mask: 0.5463, decode.d5.loss_dice: 0.8005, decode.d6.loss_cls: 0.3593, decode.d6.loss_mask: 0.5448, decode.d6.loss_dice: 0.7980, decode.d7.loss_cls: 0.3582, decode.d7.loss_mask: 0.5459, decode.d7.loss_dice: 0.8001, loss: 17.6039
2022-10-29 15:35:01,102 - mmseg - INFO - Iter [6700/20000]	lr: 2.030e-06, eta: 8:51:20, time: 2.370, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3453, decode.loss_mask: 0.5319, decode.loss_dice: 0.7785, decode.d0.loss_cls: 2.2643, decode.d0.loss_mask: 0.5487, decode.d0.loss_dice: 0.8586, decode.d1.loss_cls: 0.4203, decode.d1.loss_mask: 0.5514, decode.d1.loss_dice: 0.8227, decode.d2.loss_cls: 0.3777, decode.d2.loss_mask: 0.5383, decode.d2.loss_dice: 0.7958, decode.d3.loss_cls: 0.3586, decode.d3.loss_mask: 0.5348, decode.d3.loss_dice: 0.7862, decode.d4.loss_cls: 0.3534, decode.d4.loss_mask: 0.5332, decode.d4.loss_dice: 0.7825, decode.d5.loss_cls: 0.3471, decode.d5.loss_mask: 0.5324, decode.d5.loss_dice: 0.7794, decode.d6.loss_cls: 0.3430, decode.d6.loss_mask: 0.5317, decode.d6.loss_dice: 0.7785, decode.d7.loss_cls: 0.3424, decode.d7.loss_mask: 0.5325, decode.d7.loss_dice: 0.7784, loss: 17.1477
2022-10-29 15:37:00,646 - mmseg - INFO - Iter [6750/20000]	lr: 2.022e-06, eta: 8:49:19, time: 2.389, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3439, decode.loss_mask: 0.5374, decode.loss_dice: 0.8001, decode.d0.loss_cls: 2.2449, decode.d0.loss_mask: 0.5536, decode.d0.loss_dice: 0.8766, decode.d1.loss_cls: 0.4226, decode.d1.loss_mask: 0.5532, decode.d1.loss_dice: 0.8440, decode.d2.loss_cls: 0.3789, decode.d2.loss_mask: 0.5440, decode.d2.loss_dice: 0.8147, decode.d3.loss_cls: 0.3564, decode.d3.loss_mask: 0.5400, decode.d3.loss_dice: 0.8018, decode.d4.loss_cls: 0.3494, decode.d4.loss_mask: 0.5385, decode.d4.loss_dice: 0.8046, decode.d5.loss_cls: 0.3456, decode.d5.loss_mask: 0.5369, decode.d5.loss_dice: 0.8015, decode.d6.loss_cls: 0.3458, decode.d6.loss_mask: 0.5358, decode.d6.loss_dice: 0.7996, decode.d7.loss_cls: 0.3462, decode.d7.loss_mask: 0.5362, decode.d7.loss_dice: 0.7978, loss: 17.3501
2022-10-29 15:39:02,112 - mmseg - INFO - Iter [6800/20000]	lr: 2.015e-06, eta: 8:47:23, time: 2.431, data_time: 0.015, memory: 35422, decode.loss_cls: 0.3692, decode.loss_mask: 0.5500, decode.loss_dice: 0.8058, decode.d0.loss_cls: 2.2373, decode.d0.loss_mask: 0.5667, decode.d0.loss_dice: 0.8842, decode.d1.loss_cls: 0.4413, decode.d1.loss_mask: 0.5696, decode.d1.loss_dice: 0.8498, decode.d2.loss_cls: 0.3992, decode.d2.loss_mask: 0.5581, decode.d2.loss_dice: 0.8211, decode.d3.loss_cls: 0.3783, decode.d3.loss_mask: 0.5496, decode.d3.loss_dice: 0.8105, decode.d4.loss_cls: 0.3729, decode.d4.loss_mask: 0.5513, decode.d4.loss_dice: 0.8092, decode.d5.loss_cls: 0.3707, decode.d5.loss_mask: 0.5491, decode.d5.loss_dice: 0.8094, decode.d6.loss_cls: 0.3692, decode.d6.loss_mask: 0.5490, decode.d6.loss_dice: 0.8027, decode.d7.loss_cls: 0.3679, decode.d7.loss_mask: 0.5513, decode.d7.loss_dice: 0.8064, loss: 17.6999
2022-10-29 15:41:00,446 - mmseg - INFO - Iter [6850/20000]	lr: 2.007e-06, eta: 8:45:20, time: 2.367, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3625, decode.loss_mask: 0.5340, decode.loss_dice: 0.7896, decode.d0.loss_cls: 2.2427, decode.d0.loss_mask: 0.5505, decode.d0.loss_dice: 0.8703, decode.d1.loss_cls: 0.4388, decode.d1.loss_mask: 0.5529, decode.d1.loss_dice: 0.8351, decode.d2.loss_cls: 0.3949, decode.d2.loss_mask: 0.5419, decode.d2.loss_dice: 0.8060, decode.d3.loss_cls: 0.3750, decode.d3.loss_mask: 0.5350, decode.d3.loss_dice: 0.7972, decode.d4.loss_cls: 0.3699, decode.d4.loss_mask: 0.5345, decode.d4.loss_dice: 0.7966, decode.d5.loss_cls: 0.3629, decode.d5.loss_mask: 0.5339, decode.d5.loss_dice: 0.7937, decode.d6.loss_cls: 0.3606, decode.d6.loss_mask: 0.5343, decode.d6.loss_dice: 0.7905, decode.d7.loss_cls: 0.3595, decode.d7.loss_mask: 0.5339, decode.d7.loss_dice: 0.7929, loss: 17.3895
2022-10-29 15:42:56,988 - mmseg - INFO - Iter [6900/20000]	lr: 1.999e-06, eta: 8:43:14, time: 2.331, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3646, decode.loss_mask: 0.5397, decode.loss_dice: 0.7984, decode.d0.loss_cls: 2.2246, decode.d0.loss_mask: 0.5571, decode.d0.loss_dice: 0.8796, decode.d1.loss_cls: 0.4360, decode.d1.loss_mask: 0.5585, decode.d1.loss_dice: 0.8473, decode.d2.loss_cls: 0.3951, decode.d2.loss_mask: 0.5469, decode.d2.loss_dice: 0.8178, decode.d3.loss_cls: 0.3765, decode.d3.loss_mask: 0.5413, decode.d3.loss_dice: 0.8064, decode.d4.loss_cls: 0.3677, decode.d4.loss_mask: 0.5407, decode.d4.loss_dice: 0.8052, decode.d5.loss_cls: 0.3656, decode.d5.loss_mask: 0.5383, decode.d5.loss_dice: 0.8019, decode.d6.loss_cls: 0.3649, decode.d6.loss_mask: 0.5388, decode.d6.loss_dice: 0.7998, decode.d7.loss_cls: 0.3657, decode.d7.loss_mask: 0.5385, decode.d7.loss_dice: 0.7988, loss: 17.5159
2022-10-29 15:44:54,655 - mmseg - INFO - Iter [6950/20000]	lr: 1.992e-06, eta: 8:41:10, time: 2.353, data_time: 0.017, memory: 35422, decode.loss_cls: 0.3569, decode.loss_mask: 0.5466, decode.loss_dice: 0.7966, decode.d0.loss_cls: 2.1976, decode.d0.loss_mask: 0.5590, decode.d0.loss_dice: 0.8730, decode.d1.loss_cls: 0.4347, decode.d1.loss_mask: 0.5619, decode.d1.loss_dice: 0.8385, decode.d2.loss_cls: 0.3888, decode.d2.loss_mask: 0.5519, decode.d2.loss_dice: 0.8150, decode.d3.loss_cls: 0.3700, decode.d3.loss_mask: 0.5469, decode.d3.loss_dice: 0.8024, decode.d4.loss_cls: 0.3639, decode.d4.loss_mask: 0.5469, decode.d4.loss_dice: 0.8015, decode.d5.loss_cls: 0.3615, decode.d5.loss_mask: 0.5451, decode.d5.loss_dice: 0.7977, decode.d6.loss_cls: 0.3572, decode.d6.loss_mask: 0.5451, decode.d6.loss_dice: 0.7949, decode.d7.loss_cls: 0.3566, decode.d7.loss_mask: 0.5462, decode.d7.loss_dice: 0.7944, loss: 17.4509
2022-10-29 15:46:53,125 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-29 15:46:53,126 - mmseg - INFO - Iter [7000/20000]	lr: 1.984e-06, eta: 8:39:08, time: 2.369, data_time: 0.057, memory: 35422, decode.loss_cls: 0.3491, decode.loss_mask: 0.5279, decode.loss_dice: 0.7959, decode.d0.loss_cls: 2.1947, decode.d0.loss_mask: 0.5438, decode.d0.loss_dice: 0.8721, decode.d1.loss_cls: 0.4273, decode.d1.loss_mask: 0.5448, decode.d1.loss_dice: 0.8434, decode.d2.loss_cls: 0.3811, decode.d2.loss_mask: 0.5353, decode.d2.loss_dice: 0.8152, decode.d3.loss_cls: 0.3592, decode.d3.loss_mask: 0.5318, decode.d3.loss_dice: 0.8024, decode.d4.loss_cls: 0.3518, decode.d4.loss_mask: 0.5310, decode.d4.loss_dice: 0.8030, decode.d5.loss_cls: 0.3489, decode.d5.loss_mask: 0.5281, decode.d5.loss_dice: 0.7976, decode.d6.loss_cls: 0.3456, decode.d6.loss_mask: 0.5290, decode.d6.loss_dice: 0.7962, decode.d7.loss_cls: 0.3496, decode.d7.loss_mask: 0.5282, decode.d7.loss_dice: 0.7958, loss: 17.2284
2022-10-29 15:48:47,541 - mmseg - INFO - Iter [7050/20000]	lr: 1.976e-06, eta: 8:36:58, time: 2.288, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3464, decode.loss_mask: 0.5289, decode.loss_dice: 0.7796, decode.d0.loss_cls: 2.1529, decode.d0.loss_mask: 0.5468, decode.d0.loss_dice: 0.8570, decode.d1.loss_cls: 0.4177, decode.d1.loss_mask: 0.5449, decode.d1.loss_dice: 0.8240, decode.d2.loss_cls: 0.3750, decode.d2.loss_mask: 0.5377, decode.d2.loss_dice: 0.8002, decode.d3.loss_cls: 0.3593, decode.d3.loss_mask: 0.5327, decode.d3.loss_dice: 0.7876, decode.d4.loss_cls: 0.3523, decode.d4.loss_mask: 0.5305, decode.d4.loss_dice: 0.7847, decode.d5.loss_cls: 0.3467, decode.d5.loss_mask: 0.5309, decode.d5.loss_dice: 0.7834, decode.d6.loss_cls: 0.3445, decode.d6.loss_mask: 0.5290, decode.d6.loss_dice: 0.7808, decode.d7.loss_cls: 0.3473, decode.d7.loss_mask: 0.5292, decode.d7.loss_dice: 0.7796, loss: 17.0296
2022-10-29 15:50:44,625 - mmseg - INFO - Iter [7100/20000]	lr: 1.969e-06, eta: 8:34:53, time: 2.342, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3477, decode.loss_mask: 0.5346, decode.loss_dice: 0.7909, decode.d0.loss_cls: 2.1471, decode.d0.loss_mask: 0.5514, decode.d0.loss_dice: 0.8663, decode.d1.loss_cls: 0.4235, decode.d1.loss_mask: 0.5536, decode.d1.loss_dice: 0.8338, decode.d2.loss_cls: 0.3792, decode.d2.loss_mask: 0.5413, decode.d2.loss_dice: 0.8062, decode.d3.loss_cls: 0.3593, decode.d3.loss_mask: 0.5368, decode.d3.loss_dice: 0.7964, decode.d4.loss_cls: 0.3526, decode.d4.loss_mask: 0.5370, decode.d4.loss_dice: 0.7929, decode.d5.loss_cls: 0.3516, decode.d5.loss_mask: 0.5359, decode.d5.loss_dice: 0.7916, decode.d6.loss_cls: 0.3494, decode.d6.loss_mask: 0.5344, decode.d6.loss_dice: 0.7879, decode.d7.loss_cls: 0.3471, decode.d7.loss_mask: 0.5345, decode.d7.loss_dice: 0.7909, loss: 17.1739
2022-10-29 15:52:46,987 - mmseg - INFO - Iter [7150/20000]	lr: 1.961e-06, eta: 8:32:58, time: 2.447, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3559, decode.loss_mask: 0.5391, decode.loss_dice: 0.7951, decode.d0.loss_cls: 2.1525, decode.d0.loss_mask: 0.5556, decode.d0.loss_dice: 0.8714, decode.d1.loss_cls: 0.4308, decode.d1.loss_mask: 0.5563, decode.d1.loss_dice: 0.8435, decode.d2.loss_cls: 0.3866, decode.d2.loss_mask: 0.5462, decode.d2.loss_dice: 0.8163, decode.d3.loss_cls: 0.3658, decode.d3.loss_mask: 0.5422, decode.d3.loss_dice: 0.8010, decode.d4.loss_cls: 0.3581, decode.d4.loss_mask: 0.5405, decode.d4.loss_dice: 0.8042, decode.d5.loss_cls: 0.3579, decode.d5.loss_mask: 0.5380, decode.d5.loss_dice: 0.7974, decode.d6.loss_cls: 0.3557, decode.d6.loss_mask: 0.5377, decode.d6.loss_dice: 0.7945, decode.d7.loss_cls: 0.3546, decode.d7.loss_mask: 0.5390, decode.d7.loss_dice: 0.7956, loss: 17.3315
2022-10-29 15:54:44,547 - mmseg - INFO - Iter [7200/20000]	lr: 1.954e-06, eta: 8:30:55, time: 2.351, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3409, decode.loss_mask: 0.5284, decode.loss_dice: 0.7967, decode.d0.loss_cls: 2.1159, decode.d0.loss_mask: 0.5454, decode.d0.loss_dice: 0.8675, decode.d1.loss_cls: 0.4179, decode.d1.loss_mask: 0.5446, decode.d1.loss_dice: 0.8348, decode.d2.loss_cls: 0.3756, decode.d2.loss_mask: 0.5358, decode.d2.loss_dice: 0.8110, decode.d3.loss_cls: 0.3541, decode.d3.loss_mask: 0.5307, decode.d3.loss_dice: 0.7981, decode.d4.loss_cls: 0.3486, decode.d4.loss_mask: 0.5299, decode.d4.loss_dice: 0.7955, decode.d5.loss_cls: 0.3448, decode.d5.loss_mask: 0.5267, decode.d5.loss_dice: 0.7963, decode.d6.loss_cls: 0.3422, decode.d6.loss_mask: 0.5274, decode.d6.loss_dice: 0.7897, decode.d7.loss_cls: 0.3415, decode.d7.loss_mask: 0.5282, decode.d7.loss_dice: 0.7942, loss: 17.0624
2022-10-29 15:56:43,429 - mmseg - INFO - Iter [7250/20000]	lr: 1.946e-06, eta: 8:28:53, time: 2.378, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3443, decode.loss_mask: 0.5448, decode.loss_dice: 0.7886, decode.d0.loss_cls: 2.0863, decode.d0.loss_mask: 0.5615, decode.d0.loss_dice: 0.8653, decode.d1.loss_cls: 0.4194, decode.d1.loss_mask: 0.5629, decode.d1.loss_dice: 0.8330, decode.d2.loss_cls: 0.3782, decode.d2.loss_mask: 0.5525, decode.d2.loss_dice: 0.8106, decode.d3.loss_cls: 0.3588, decode.d3.loss_mask: 0.5476, decode.d3.loss_dice: 0.7986, decode.d4.loss_cls: 0.3529, decode.d4.loss_mask: 0.5462, decode.d4.loss_dice: 0.7945, decode.d5.loss_cls: 0.3498, decode.d5.loss_mask: 0.5438, decode.d5.loss_dice: 0.7938, decode.d6.loss_cls: 0.3416, decode.d6.loss_mask: 0.5443, decode.d6.loss_dice: 0.7934, decode.d7.loss_cls: 0.3438, decode.d7.loss_mask: 0.5451, decode.d7.loss_dice: 0.7939, loss: 17.1955
2022-10-29 15:58:45,042 - mmseg - INFO - Iter [7300/20000]	lr: 1.938e-06, eta: 8:26:57, time: 2.432, data_time: 0.067, memory: 35422, decode.loss_cls: 0.3531, decode.loss_mask: 0.5335, decode.loss_dice: 0.7934, decode.d0.loss_cls: 2.1035, decode.d0.loss_mask: 0.5497, decode.d0.loss_dice: 0.8766, decode.d1.loss_cls: 0.4292, decode.d1.loss_mask: 0.5507, decode.d1.loss_dice: 0.8414, decode.d2.loss_cls: 0.3855, decode.d2.loss_mask: 0.5407, decode.d2.loss_dice: 0.8133, decode.d3.loss_cls: 0.3663, decode.d3.loss_mask: 0.5333, decode.d3.loss_dice: 0.8002, decode.d4.loss_cls: 0.3608, decode.d4.loss_mask: 0.5323, decode.d4.loss_dice: 0.8006, decode.d5.loss_cls: 0.3550, decode.d5.loss_mask: 0.5320, decode.d5.loss_dice: 0.7979, decode.d6.loss_cls: 0.3536, decode.d6.loss_mask: 0.5318, decode.d6.loss_dice: 0.7961, decode.d7.loss_cls: 0.3530, decode.d7.loss_mask: 0.5339, decode.d7.loss_dice: 0.7928, loss: 17.2101
2022-10-29 16:00:42,250 - mmseg - INFO - Iter [7350/20000]	lr: 1.931e-06, eta: 8:24:53, time: 2.344, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3342, decode.loss_mask: 0.5275, decode.loss_dice: 0.7910, decode.d0.loss_cls: 2.0637, decode.d0.loss_mask: 0.5442, decode.d0.loss_dice: 0.8609, decode.d1.loss_cls: 0.4095, decode.d1.loss_mask: 0.5440, decode.d1.loss_dice: 0.8305, decode.d2.loss_cls: 0.3692, decode.d2.loss_mask: 0.5325, decode.d2.loss_dice: 0.8062, decode.d3.loss_cls: 0.3458, decode.d3.loss_mask: 0.5309, decode.d3.loss_dice: 0.7988, decode.d4.loss_cls: 0.3411, decode.d4.loss_mask: 0.5301, decode.d4.loss_dice: 0.7957, decode.d5.loss_cls: 0.3367, decode.d5.loss_mask: 0.5278, decode.d5.loss_dice: 0.7928, decode.d6.loss_cls: 0.3345, decode.d6.loss_mask: 0.5277, decode.d6.loss_dice: 0.7894, decode.d7.loss_cls: 0.3365, decode.d7.loss_mask: 0.5284, decode.d7.loss_dice: 0.7887, loss: 16.9183
2022-10-29 16:02:40,764 - mmseg - INFO - Iter [7400/20000]	lr: 1.923e-06, eta: 8:22:51, time: 2.370, data_time: 0.022, memory: 35422, decode.loss_cls: 0.3277, decode.loss_mask: 0.5289, decode.loss_dice: 0.7809, decode.d0.loss_cls: 2.0669, decode.d0.loss_mask: 0.5451, decode.d0.loss_dice: 0.8545, decode.d1.loss_cls: 0.4040, decode.d1.loss_mask: 0.5441, decode.d1.loss_dice: 0.8218, decode.d2.loss_cls: 0.3588, decode.d2.loss_mask: 0.5328, decode.d2.loss_dice: 0.7983, decode.d3.loss_cls: 0.3406, decode.d3.loss_mask: 0.5311, decode.d3.loss_dice: 0.7888, decode.d4.loss_cls: 0.3322, decode.d4.loss_mask: 0.5290, decode.d4.loss_dice: 0.7872, decode.d5.loss_cls: 0.3314, decode.d5.loss_mask: 0.5263, decode.d5.loss_dice: 0.7826, decode.d6.loss_cls: 0.3266, decode.d6.loss_mask: 0.5265, decode.d6.loss_dice: 0.7804, decode.d7.loss_cls: 0.3281, decode.d7.loss_mask: 0.5288, decode.d7.loss_dice: 0.7810, loss: 16.7844
2022-10-29 16:04:41,875 - mmseg - INFO - Iter [7450/20000]	lr: 1.915e-06, eta: 8:20:54, time: 2.422, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3378, decode.loss_mask: 0.5279, decode.loss_dice: 0.7888, decode.d0.loss_cls: 2.0750, decode.d0.loss_mask: 0.5452, decode.d0.loss_dice: 0.8655, decode.d1.loss_cls: 0.4115, decode.d1.loss_mask: 0.5456, decode.d1.loss_dice: 0.8318, decode.d2.loss_cls: 0.3698, decode.d2.loss_mask: 0.5340, decode.d2.loss_dice: 0.8063, decode.d3.loss_cls: 0.3522, decode.d3.loss_mask: 0.5286, decode.d3.loss_dice: 0.7900, decode.d4.loss_cls: 0.3468, decode.d4.loss_mask: 0.5290, decode.d4.loss_dice: 0.7904, decode.d5.loss_cls: 0.3411, decode.d5.loss_mask: 0.5265, decode.d5.loss_dice: 0.7908, decode.d6.loss_cls: 0.3386, decode.d6.loss_mask: 0.5257, decode.d6.loss_dice: 0.7880, decode.d7.loss_cls: 0.3372, decode.d7.loss_mask: 0.5273, decode.d7.loss_dice: 0.7876, loss: 16.9391
2022-10-29 16:06:42,158 - mmseg - INFO - Iter [7500/20000]	lr: 1.908e-06, eta: 8:18:55, time: 2.406, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3436, decode.loss_mask: 0.5288, decode.loss_dice: 0.7827, decode.d0.loss_cls: 2.0517, decode.d0.loss_mask: 0.5468, decode.d0.loss_dice: 0.8598, decode.d1.loss_cls: 0.4202, decode.d1.loss_mask: 0.5443, decode.d1.loss_dice: 0.8261, decode.d2.loss_cls: 0.3781, decode.d2.loss_mask: 0.5366, decode.d2.loss_dice: 0.7983, decode.d3.loss_cls: 0.3562, decode.d3.loss_mask: 0.5303, decode.d3.loss_dice: 0.7861, decode.d4.loss_cls: 0.3491, decode.d4.loss_mask: 0.5309, decode.d4.loss_dice: 0.7850, decode.d5.loss_cls: 0.3462, decode.d5.loss_mask: 0.5281, decode.d5.loss_dice: 0.7845, decode.d6.loss_cls: 0.3420, decode.d6.loss_mask: 0.5275, decode.d6.loss_dice: 0.7837, decode.d7.loss_cls: 0.3437, decode.d7.loss_mask: 0.5278, decode.d7.loss_dice: 0.7824, loss: 16.9204
2022-10-29 16:08:39,230 - mmseg - INFO - Iter [7550/20000]	lr: 1.900e-06, eta: 8:16:51, time: 2.341, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3540, decode.loss_mask: 0.5349, decode.loss_dice: 0.7946, decode.d0.loss_cls: 2.0430, decode.d0.loss_mask: 0.5519, decode.d0.loss_dice: 0.8729, decode.d1.loss_cls: 0.4298, decode.d1.loss_mask: 0.5523, decode.d1.loss_dice: 0.8388, decode.d2.loss_cls: 0.3892, decode.d2.loss_mask: 0.5414, decode.d2.loss_dice: 0.8142, decode.d3.loss_cls: 0.3675, decode.d3.loss_mask: 0.5366, decode.d3.loss_dice: 0.8022, decode.d4.loss_cls: 0.3623, decode.d4.loss_mask: 0.5357, decode.d4.loss_dice: 0.7998, decode.d5.loss_cls: 0.3556, decode.d5.loss_mask: 0.5350, decode.d5.loss_dice: 0.7975, decode.d6.loss_cls: 0.3558, decode.d6.loss_mask: 0.5329, decode.d6.loss_dice: 0.7929, decode.d7.loss_cls: 0.3542, decode.d7.loss_mask: 0.5343, decode.d7.loss_dice: 0.7959, loss: 17.1752
2022-10-29 16:10:37,143 - mmseg - INFO - Iter [7600/20000]	lr: 1.892e-06, eta: 8:14:48, time: 2.358, data_time: 0.058, memory: 35422, decode.loss_cls: 0.3346, decode.loss_mask: 0.5263, decode.loss_dice: 0.7696, decode.d0.loss_cls: 2.0225, decode.d0.loss_mask: 0.5425, decode.d0.loss_dice: 0.8465, decode.d1.loss_cls: 0.4152, decode.d1.loss_mask: 0.5413, decode.d1.loss_dice: 0.8082, decode.d2.loss_cls: 0.3698, decode.d2.loss_mask: 0.5311, decode.d2.loss_dice: 0.7848, decode.d3.loss_cls: 0.3457, decode.d3.loss_mask: 0.5286, decode.d3.loss_dice: 0.7757, decode.d4.loss_cls: 0.3399, decode.d4.loss_mask: 0.5267, decode.d4.loss_dice: 0.7751, decode.d5.loss_cls: 0.3375, decode.d5.loss_mask: 0.5248, decode.d5.loss_dice: 0.7711, decode.d6.loss_cls: 0.3345, decode.d6.loss_mask: 0.5246, decode.d6.loss_dice: 0.7673, decode.d7.loss_cls: 0.3340, decode.d7.loss_mask: 0.5255, decode.d7.loss_dice: 0.7708, loss: 16.6741
2022-10-29 16:12:40,141 - mmseg - INFO - Iter [7650/20000]	lr: 1.885e-06, eta: 8:12:54, time: 2.460, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3323, decode.loss_mask: 0.5237, decode.loss_dice: 0.7826, decode.d0.loss_cls: 2.0159, decode.d0.loss_mask: 0.5390, decode.d0.loss_dice: 0.8520, decode.d1.loss_cls: 0.4109, decode.d1.loss_mask: 0.5393, decode.d1.loss_dice: 0.8202, decode.d2.loss_cls: 0.3699, decode.d2.loss_mask: 0.5297, decode.d2.loss_dice: 0.7952, decode.d3.loss_cls: 0.3472, decode.d3.loss_mask: 0.5237, decode.d3.loss_dice: 0.7869, decode.d4.loss_cls: 0.3426, decode.d4.loss_mask: 0.5231, decode.d4.loss_dice: 0.7863, decode.d5.loss_cls: 0.3369, decode.d5.loss_mask: 0.5226, decode.d5.loss_dice: 0.7841, decode.d6.loss_cls: 0.3336, decode.d6.loss_mask: 0.5210, decode.d6.loss_dice: 0.7811, decode.d7.loss_cls: 0.3332, decode.d7.loss_mask: 0.5237, decode.d7.loss_dice: 0.7800, loss: 16.7366
2022-10-29 16:14:38,393 - mmseg - INFO - Iter [7700/20000]	lr: 1.877e-06, eta: 8:10:51, time: 2.365, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3372, decode.loss_mask: 0.5189, decode.loss_dice: 0.7753, decode.d0.loss_cls: 2.0093, decode.d0.loss_mask: 0.5362, decode.d0.loss_dice: 0.8466, decode.d1.loss_cls: 0.4083, decode.d1.loss_mask: 0.5377, decode.d1.loss_dice: 0.8175, decode.d2.loss_cls: 0.3652, decode.d2.loss_mask: 0.5277, decode.d2.loss_dice: 0.7917, decode.d3.loss_cls: 0.3478, decode.d3.loss_mask: 0.5214, decode.d3.loss_dice: 0.7814, decode.d4.loss_cls: 0.3410, decode.d4.loss_mask: 0.5192, decode.d4.loss_dice: 0.7802, decode.d5.loss_cls: 0.3394, decode.d5.loss_mask: 0.5191, decode.d5.loss_dice: 0.7754, decode.d6.loss_cls: 0.3366, decode.d6.loss_mask: 0.5183, decode.d6.loss_dice: 0.7739, decode.d7.loss_cls: 0.3357, decode.d7.loss_mask: 0.5189, decode.d7.loss_dice: 0.7739, loss: 16.6537
2022-10-29 16:16:35,299 - mmseg - INFO - Iter [7750/20000]	lr: 1.870e-06, eta: 8:08:47, time: 2.338, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3402, decode.loss_mask: 0.5273, decode.loss_dice: 0.7728, decode.d0.loss_cls: 1.9825, decode.d0.loss_mask: 0.5445, decode.d0.loss_dice: 0.8492, decode.d1.loss_cls: 0.4087, decode.d1.loss_mask: 0.5431, decode.d1.loss_dice: 0.8218, decode.d2.loss_cls: 0.3688, decode.d2.loss_mask: 0.5318, decode.d2.loss_dice: 0.7979, decode.d3.loss_cls: 0.3508, decode.d3.loss_mask: 0.5288, decode.d3.loss_dice: 0.7805, decode.d4.loss_cls: 0.3439, decode.d4.loss_mask: 0.5284, decode.d4.loss_dice: 0.7766, decode.d5.loss_cls: 0.3421, decode.d5.loss_mask: 0.5276, decode.d5.loss_dice: 0.7758, decode.d6.loss_cls: 0.3371, decode.d6.loss_mask: 0.5261, decode.d6.loss_dice: 0.7723, decode.d7.loss_cls: 0.3396, decode.d7.loss_mask: 0.5271, decode.d7.loss_dice: 0.7718, loss: 16.7170
2022-10-29 16:18:31,477 - mmseg - INFO - Iter [7800/20000]	lr: 1.862e-06, eta: 8:06:42, time: 2.324, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3106, decode.loss_mask: 0.5202, decode.loss_dice: 0.7745, decode.d0.loss_cls: 1.9604, decode.d0.loss_mask: 0.5418, decode.d0.loss_dice: 0.8439, decode.d1.loss_cls: 0.3913, decode.d1.loss_mask: 0.5379, decode.d1.loss_dice: 0.8133, decode.d2.loss_cls: 0.3440, decode.d2.loss_mask: 0.5281, decode.d2.loss_dice: 0.7926, decode.d3.loss_cls: 0.3280, decode.d3.loss_mask: 0.5229, decode.d3.loss_dice: 0.7750, decode.d4.loss_cls: 0.3191, decode.d4.loss_mask: 0.5208, decode.d4.loss_dice: 0.7802, decode.d5.loss_cls: 0.3155, decode.d5.loss_mask: 0.5206, decode.d5.loss_dice: 0.7751, decode.d6.loss_cls: 0.3100, decode.d6.loss_mask: 0.5199, decode.d6.loss_dice: 0.7735, decode.d7.loss_cls: 0.3086, decode.d7.loss_mask: 0.5200, decode.d7.loss_dice: 0.7734, loss: 16.4213
2022-10-29 16:20:29,876 - mmseg - INFO - Iter [7850/20000]	lr: 1.854e-06, eta: 8:04:40, time: 2.368, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3358, decode.loss_mask: 0.5292, decode.loss_dice: 0.7924, decode.d0.loss_cls: 1.9766, decode.d0.loss_mask: 0.5441, decode.d0.loss_dice: 0.8679, decode.d1.loss_cls: 0.4108, decode.d1.loss_mask: 0.5470, decode.d1.loss_dice: 0.8350, decode.d2.loss_cls: 0.3682, decode.d2.loss_mask: 0.5360, decode.d2.loss_dice: 0.8078, decode.d3.loss_cls: 0.3490, decode.d3.loss_mask: 0.5312, decode.d3.loss_dice: 0.7953, decode.d4.loss_cls: 0.3439, decode.d4.loss_mask: 0.5309, decode.d4.loss_dice: 0.7959, decode.d5.loss_cls: 0.3385, decode.d5.loss_mask: 0.5280, decode.d5.loss_dice: 0.7964, decode.d6.loss_cls: 0.3358, decode.d6.loss_mask: 0.5282, decode.d6.loss_dice: 0.7891, decode.d7.loss_cls: 0.3363, decode.d7.loss_mask: 0.5282, decode.d7.loss_dice: 0.7896, loss: 16.8670
2022-10-29 16:22:26,456 - mmseg - INFO - Iter [7900/20000]	lr: 1.847e-06, eta: 8:02:36, time: 2.331, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3483, decode.loss_mask: 0.5246, decode.loss_dice: 0.7823, decode.d0.loss_cls: 1.9952, decode.d0.loss_mask: 0.5409, decode.d0.loss_dice: 0.8622, decode.d1.loss_cls: 0.4210, decode.d1.loss_mask: 0.5405, decode.d1.loss_dice: 0.8303, decode.d2.loss_cls: 0.3811, decode.d2.loss_mask: 0.5296, decode.d2.loss_dice: 0.7970, decode.d3.loss_cls: 0.3597, decode.d3.loss_mask: 0.5274, decode.d3.loss_dice: 0.7877, decode.d4.loss_cls: 0.3523, decode.d4.loss_mask: 0.5259, decode.d4.loss_dice: 0.7878, decode.d5.loss_cls: 0.3514, decode.d5.loss_mask: 0.5243, decode.d5.loss_dice: 0.7870, decode.d6.loss_cls: 0.3466, decode.d6.loss_mask: 0.5236, decode.d6.loss_dice: 0.7822, decode.d7.loss_cls: 0.3470, decode.d7.loss_mask: 0.5250, decode.d7.loss_dice: 0.7831, loss: 16.8638
2022-10-29 16:24:25,926 - mmseg - INFO - Iter [7950/20000]	lr: 1.839e-06, eta: 8:00:36, time: 2.390, data_time: 0.057, memory: 35422, decode.loss_cls: 0.3175, decode.loss_mask: 0.5238, decode.loss_dice: 0.7770, decode.d0.loss_cls: 1.9409, decode.d0.loss_mask: 0.5399, decode.d0.loss_dice: 0.8492, decode.d1.loss_cls: 0.3909, decode.d1.loss_mask: 0.5399, decode.d1.loss_dice: 0.8203, decode.d2.loss_cls: 0.3518, decode.d2.loss_mask: 0.5303, decode.d2.loss_dice: 0.7900, decode.d3.loss_cls: 0.3300, decode.d3.loss_mask: 0.5270, decode.d3.loss_dice: 0.7798, decode.d4.loss_cls: 0.3270, decode.d4.loss_mask: 0.5246, decode.d4.loss_dice: 0.7798, decode.d5.loss_cls: 0.3199, decode.d5.loss_mask: 0.5244, decode.d5.loss_dice: 0.7792, decode.d6.loss_cls: 0.3191, decode.d6.loss_mask: 0.5230, decode.d6.loss_dice: 0.7742, decode.d7.loss_cls: 0.3189, decode.d7.loss_mask: 0.5233, decode.d7.loss_dice: 0.7757, loss: 16.4973
2022-10-29 16:26:26,960 - mmseg - INFO - Saving checkpoint at 8000 iterations
2022-10-29 16:27:00,017 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-29 16:27:00,018 - mmseg - INFO - Iter [8000/20000]	lr: 1.831e-06, eta: 7:59:28, time: 3.082, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3222, decode.loss_mask: 0.5187, decode.loss_dice: 0.7619, decode.d0.loss_cls: 1.9601, decode.d0.loss_mask: 0.5318, decode.d0.loss_dice: 0.8341, decode.d1.loss_cls: 0.3991, decode.d1.loss_mask: 0.5359, decode.d1.loss_dice: 0.8017, decode.d2.loss_cls: 0.3551, decode.d2.loss_mask: 0.5257, decode.d2.loss_dice: 0.7787, decode.d3.loss_cls: 0.3340, decode.d3.loss_mask: 0.5219, decode.d3.loss_dice: 0.7667, decode.d4.loss_cls: 0.3314, decode.d4.loss_mask: 0.5197, decode.d4.loss_dice: 0.7672, decode.d5.loss_cls: 0.3256, decode.d5.loss_mask: 0.5186, decode.d5.loss_dice: 0.7664, decode.d6.loss_cls: 0.3244, decode.d6.loss_mask: 0.5171, decode.d6.loss_dice: 0.7612, decode.d7.loss_cls: 0.3234, decode.d7.loss_mask: 0.5178, decode.d7.loss_dice: 0.7624, loss: 16.3830
2022-10-29 16:28:56,839 - mmseg - INFO - Iter [8050/20000]	lr: 1.824e-06, eta: 7:57:24, time: 2.336, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3208, decode.loss_mask: 0.5196, decode.loss_dice: 0.7773, decode.d0.loss_cls: 1.9362, decode.d0.loss_mask: 0.5364, decode.d0.loss_dice: 0.8554, decode.d1.loss_cls: 0.4029, decode.d1.loss_mask: 0.5354, decode.d1.loss_dice: 0.8205, decode.d2.loss_cls: 0.3550, decode.d2.loss_mask: 0.5259, decode.d2.loss_dice: 0.7894, decode.d3.loss_cls: 0.3354, decode.d3.loss_mask: 0.5208, decode.d3.loss_dice: 0.7802, decode.d4.loss_cls: 0.3284, decode.d4.loss_mask: 0.5205, decode.d4.loss_dice: 0.7799, decode.d5.loss_cls: 0.3240, decode.d5.loss_mask: 0.5181, decode.d5.loss_dice: 0.7798, decode.d6.loss_cls: 0.3214, decode.d6.loss_mask: 0.5189, decode.d6.loss_dice: 0.7763, decode.d7.loss_cls: 0.3187, decode.d7.loss_mask: 0.5199, decode.d7.loss_dice: 0.7784, loss: 16.4956
2022-10-29 16:30:54,861 - mmseg - INFO - Iter [8100/20000]	lr: 1.816e-06, eta: 7:55:21, time: 2.360, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3278, decode.loss_mask: 0.5187, decode.loss_dice: 0.7767, decode.d0.loss_cls: 1.9412, decode.d0.loss_mask: 0.5338, decode.d0.loss_dice: 0.8493, decode.d1.loss_cls: 0.4044, decode.d1.loss_mask: 0.5337, decode.d1.loss_dice: 0.8171, decode.d2.loss_cls: 0.3643, decode.d2.loss_mask: 0.5238, decode.d2.loss_dice: 0.7905, decode.d3.loss_cls: 0.3445, decode.d3.loss_mask: 0.5204, decode.d3.loss_dice: 0.7824, decode.d4.loss_cls: 0.3373, decode.d4.loss_mask: 0.5194, decode.d4.loss_dice: 0.7813, decode.d5.loss_cls: 0.3333, decode.d5.loss_mask: 0.5188, decode.d5.loss_dice: 0.7793, decode.d6.loss_cls: 0.3294, decode.d6.loss_mask: 0.5184, decode.d6.loss_dice: 0.7749, decode.d7.loss_cls: 0.3300, decode.d7.loss_mask: 0.5178, decode.d7.loss_dice: 0.7767, loss: 16.5449
2022-10-29 16:32:49,965 - mmseg - INFO - Iter [8150/20000]	lr: 1.809e-06, eta: 7:53:14, time: 2.302, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3154, decode.loss_mask: 0.5348, decode.loss_dice: 0.7847, decode.d0.loss_cls: 1.9072, decode.d0.loss_mask: 0.5545, decode.d0.loss_dice: 0.8582, decode.d1.loss_cls: 0.3899, decode.d1.loss_mask: 0.5512, decode.d1.loss_dice: 0.8303, decode.d2.loss_cls: 0.3463, decode.d2.loss_mask: 0.5416, decode.d2.loss_dice: 0.8039, decode.d3.loss_cls: 0.3275, decode.d3.loss_mask: 0.5378, decode.d3.loss_dice: 0.7895, decode.d4.loss_cls: 0.3227, decode.d4.loss_mask: 0.5353, decode.d4.loss_dice: 0.7887, decode.d5.loss_cls: 0.3174, decode.d5.loss_mask: 0.5348, decode.d5.loss_dice: 0.7921, decode.d6.loss_cls: 0.3148, decode.d6.loss_mask: 0.5345, decode.d6.loss_dice: 0.7890, decode.d7.loss_cls: 0.3141, decode.d7.loss_mask: 0.5363, decode.d7.loss_dice: 0.7884, loss: 16.6408
2022-10-29 16:34:48,489 - mmseg - INFO - Iter [8200/20000]	lr: 1.801e-06, eta: 7:51:13, time: 2.369, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3223, decode.loss_mask: 0.5249, decode.loss_dice: 0.7783, decode.d0.loss_cls: 1.9254, decode.d0.loss_mask: 0.5396, decode.d0.loss_dice: 0.8510, decode.d1.loss_cls: 0.4005, decode.d1.loss_mask: 0.5416, decode.d1.loss_dice: 0.8235, decode.d2.loss_cls: 0.3591, decode.d2.loss_mask: 0.5303, decode.d2.loss_dice: 0.7952, decode.d3.loss_cls: 0.3366, decode.d3.loss_mask: 0.5264, decode.d3.loss_dice: 0.7818, decode.d4.loss_cls: 0.3303, decode.d4.loss_mask: 0.5257, decode.d4.loss_dice: 0.7820, decode.d5.loss_cls: 0.3286, decode.d5.loss_mask: 0.5234, decode.d5.loss_dice: 0.7774, decode.d6.loss_cls: 0.3251, decode.d6.loss_mask: 0.5234, decode.d6.loss_dice: 0.7774, decode.d7.loss_cls: 0.3225, decode.d7.loss_mask: 0.5239, decode.d7.loss_dice: 0.7758, loss: 16.5520
2022-10-29 16:36:49,729 - mmseg - INFO - Iter [8250/20000]	lr: 1.793e-06, eta: 7:49:15, time: 2.427, data_time: 0.059, memory: 35422, decode.loss_cls: 0.3212, decode.loss_mask: 0.5204, decode.loss_dice: 0.7660, decode.d0.loss_cls: 1.9012, decode.d0.loss_mask: 0.5363, decode.d0.loss_dice: 0.8412, decode.d1.loss_cls: 0.3936, decode.d1.loss_mask: 0.5382, decode.d1.loss_dice: 0.8095, decode.d2.loss_cls: 0.3513, decode.d2.loss_mask: 0.5250, decode.d2.loss_dice: 0.7851, decode.d3.loss_cls: 0.3350, decode.d3.loss_mask: 0.5208, decode.d3.loss_dice: 0.7717, decode.d4.loss_cls: 0.3271, decode.d4.loss_mask: 0.5208, decode.d4.loss_dice: 0.7717, decode.d5.loss_cls: 0.3220, decode.d5.loss_mask: 0.5211, decode.d5.loss_dice: 0.7690, decode.d6.loss_cls: 0.3226, decode.d6.loss_mask: 0.5196, decode.d6.loss_dice: 0.7677, decode.d7.loss_cls: 0.3202, decode.d7.loss_mask: 0.5216, decode.d7.loss_dice: 0.7690, loss: 16.3690
2022-10-29 16:38:47,311 - mmseg - INFO - Iter [8300/20000]	lr: 1.786e-06, eta: 7:47:12, time: 2.351, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3210, decode.loss_mask: 0.5182, decode.loss_dice: 0.7723, decode.d0.loss_cls: 1.8907, decode.d0.loss_mask: 0.5376, decode.d0.loss_dice: 0.8414, decode.d1.loss_cls: 0.3974, decode.d1.loss_mask: 0.5353, decode.d1.loss_dice: 0.8148, decode.d2.loss_cls: 0.3556, decode.d2.loss_mask: 0.5243, decode.d2.loss_dice: 0.7867, decode.d3.loss_cls: 0.3375, decode.d3.loss_mask: 0.5208, decode.d3.loss_dice: 0.7736, decode.d4.loss_cls: 0.3332, decode.d4.loss_mask: 0.5202, decode.d4.loss_dice: 0.7739, decode.d5.loss_cls: 0.3251, decode.d5.loss_mask: 0.5187, decode.d5.loss_dice: 0.7740, decode.d6.loss_cls: 0.3246, decode.d6.loss_mask: 0.5177, decode.d6.loss_dice: 0.7704, decode.d7.loss_cls: 0.3220, decode.d7.loss_mask: 0.5178, decode.d7.loss_dice: 0.7682, loss: 16.3928
2022-10-29 16:40:45,033 - mmseg - INFO - Iter [8350/20000]	lr: 1.778e-06, eta: 7:45:09, time: 2.354, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3078, decode.loss_mask: 0.5038, decode.loss_dice: 0.7737, decode.d0.loss_cls: 1.8873, decode.d0.loss_mask: 0.5220, decode.d0.loss_dice: 0.8436, decode.d1.loss_cls: 0.3856, decode.d1.loss_mask: 0.5193, decode.d1.loss_dice: 0.8156, decode.d2.loss_cls: 0.3404, decode.d2.loss_mask: 0.5117, decode.d2.loss_dice: 0.7917, decode.d3.loss_cls: 0.3223, decode.d3.loss_mask: 0.5054, decode.d3.loss_dice: 0.7791, decode.d4.loss_cls: 0.3147, decode.d4.loss_mask: 0.5052, decode.d4.loss_dice: 0.7792, decode.d5.loss_cls: 0.3126, decode.d5.loss_mask: 0.5040, decode.d5.loss_dice: 0.7771, decode.d6.loss_cls: 0.3101, decode.d6.loss_mask: 0.5030, decode.d6.loss_dice: 0.7734, decode.d7.loss_cls: 0.3072, decode.d7.loss_mask: 0.5023, decode.d7.loss_dice: 0.7742, loss: 16.1722
2022-10-29 16:42:39,493 - mmseg - INFO - Iter [8400/20000]	lr: 1.770e-06, eta: 7:43:02, time: 2.290, data_time: 0.014, memory: 35422, decode.loss_cls: 0.3180, decode.loss_mask: 0.5182, decode.loss_dice: 0.7596, decode.d0.loss_cls: 1.8951, decode.d0.loss_mask: 0.5358, decode.d0.loss_dice: 0.8399, decode.d1.loss_cls: 0.3908, decode.d1.loss_mask: 0.5354, decode.d1.loss_dice: 0.8050, decode.d2.loss_cls: 0.3539, decode.d2.loss_mask: 0.5234, decode.d2.loss_dice: 0.7771, decode.d3.loss_cls: 0.3324, decode.d3.loss_mask: 0.5182, decode.d3.loss_dice: 0.7680, decode.d4.loss_cls: 0.3235, decode.d4.loss_mask: 0.5170, decode.d4.loss_dice: 0.7658, decode.d5.loss_cls: 0.3210, decode.d5.loss_mask: 0.5169, decode.d5.loss_dice: 0.7629, decode.d6.loss_cls: 0.3183, decode.d6.loss_mask: 0.5165, decode.d6.loss_dice: 0.7584, decode.d7.loss_cls: 0.3165, decode.d7.loss_mask: 0.5168, decode.d7.loss_dice: 0.7618, loss: 16.2662
2022-10-29 16:44:38,822 - mmseg - INFO - Iter [8450/20000]	lr: 1.763e-06, eta: 7:41:02, time: 2.387, data_time: 0.014, memory: 35422, decode.loss_cls: 0.3120, decode.loss_mask: 0.5203, decode.loss_dice: 0.7735, decode.d0.loss_cls: 1.8797, decode.d0.loss_mask: 0.5381, decode.d0.loss_dice: 0.8504, decode.d1.loss_cls: 0.3891, decode.d1.loss_mask: 0.5361, decode.d1.loss_dice: 0.8167, decode.d2.loss_cls: 0.3445, decode.d2.loss_mask: 0.5286, decode.d2.loss_dice: 0.7894, decode.d3.loss_cls: 0.3257, decode.d3.loss_mask: 0.5232, decode.d3.loss_dice: 0.7767, decode.d4.loss_cls: 0.3172, decode.d4.loss_mask: 0.5213, decode.d4.loss_dice: 0.7753, decode.d5.loss_cls: 0.3148, decode.d5.loss_mask: 0.5203, decode.d5.loss_dice: 0.7755, decode.d6.loss_cls: 0.3123, decode.d6.loss_mask: 0.5190, decode.d6.loss_dice: 0.7746, decode.d7.loss_cls: 0.3109, decode.d7.loss_mask: 0.5204, decode.d7.loss_dice: 0.7696, loss: 16.3351
2022-10-29 16:46:41,888 - mmseg - INFO - Iter [8500/20000]	lr: 1.755e-06, eta: 7:39:07, time: 2.461, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3080, decode.loss_mask: 0.5174, decode.loss_dice: 0.7693, decode.d0.loss_cls: 1.8578, decode.d0.loss_mask: 0.5352, decode.d0.loss_dice: 0.8400, decode.d1.loss_cls: 0.3738, decode.d1.loss_mask: 0.5359, decode.d1.loss_dice: 0.8063, decode.d2.loss_cls: 0.3400, decode.d2.loss_mask: 0.5260, decode.d2.loss_dice: 0.7827, decode.d3.loss_cls: 0.3199, decode.d3.loss_mask: 0.5219, decode.d3.loss_dice: 0.7720, decode.d4.loss_cls: 0.3142, decode.d4.loss_mask: 0.5200, decode.d4.loss_dice: 0.7690, decode.d5.loss_cls: 0.3116, decode.d5.loss_mask: 0.5179, decode.d5.loss_dice: 0.7670, decode.d6.loss_cls: 0.3090, decode.d6.loss_mask: 0.5167, decode.d6.loss_dice: 0.7687, decode.d7.loss_cls: 0.3079, decode.d7.loss_mask: 0.5185, decode.d7.loss_dice: 0.7669, loss: 16.1937
2022-10-29 16:48:41,708 - mmseg - INFO - Iter [8550/20000]	lr: 1.748e-06, eta: 7:37:07, time: 2.396, data_time: 0.056, memory: 35422, decode.loss_cls: 0.3209, decode.loss_mask: 0.5143, decode.loss_dice: 0.7696, decode.d0.loss_cls: 1.8783, decode.d0.loss_mask: 0.5357, decode.d0.loss_dice: 0.8521, decode.d1.loss_cls: 0.3923, decode.d1.loss_mask: 0.5338, decode.d1.loss_dice: 0.8122, decode.d2.loss_cls: 0.3519, decode.d2.loss_mask: 0.5224, decode.d2.loss_dice: 0.7885, decode.d3.loss_cls: 0.3334, decode.d3.loss_mask: 0.5168, decode.d3.loss_dice: 0.7748, decode.d4.loss_cls: 0.3313, decode.d4.loss_mask: 0.5161, decode.d4.loss_dice: 0.7728, decode.d5.loss_cls: 0.3252, decode.d5.loss_mask: 0.5157, decode.d5.loss_dice: 0.7739, decode.d6.loss_cls: 0.3200, decode.d6.loss_mask: 0.5149, decode.d6.loss_dice: 0.7707, decode.d7.loss_cls: 0.3191, decode.d7.loss_mask: 0.5144, decode.d7.loss_dice: 0.7700, loss: 16.3409
2022-10-29 16:50:40,835 - mmseg - INFO - Iter [8600/20000]	lr: 1.740e-06, eta: 7:35:06, time: 2.383, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3056, decode.loss_mask: 0.5203, decode.loss_dice: 0.7673, decode.d0.loss_cls: 1.8460, decode.d0.loss_mask: 0.5373, decode.d0.loss_dice: 0.8410, decode.d1.loss_cls: 0.3806, decode.d1.loss_mask: 0.5350, decode.d1.loss_dice: 0.8091, decode.d2.loss_cls: 0.3367, decode.d2.loss_mask: 0.5240, decode.d2.loss_dice: 0.7867, decode.d3.loss_cls: 0.3181, decode.d3.loss_mask: 0.5211, decode.d3.loss_dice: 0.7730, decode.d4.loss_cls: 0.3148, decode.d4.loss_mask: 0.5205, decode.d4.loss_dice: 0.7676, decode.d5.loss_cls: 0.3099, decode.d5.loss_mask: 0.5193, decode.d5.loss_dice: 0.7673, decode.d6.loss_cls: 0.3075, decode.d6.loss_mask: 0.5191, decode.d6.loss_dice: 0.7659, decode.d7.loss_cls: 0.3041, decode.d7.loss_mask: 0.5190, decode.d7.loss_dice: 0.7668, loss: 16.1834
2022-10-29 16:52:40,722 - mmseg - INFO - Iter [8650/20000]	lr: 1.732e-06, eta: 7:33:07, time: 2.398, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3047, decode.loss_mask: 0.5089, decode.loss_dice: 0.7669, decode.d0.loss_cls: 1.8367, decode.d0.loss_mask: 0.5256, decode.d0.loss_dice: 0.8332, decode.d1.loss_cls: 0.3746, decode.d1.loss_mask: 0.5271, decode.d1.loss_dice: 0.8021, decode.d2.loss_cls: 0.3376, decode.d2.loss_mask: 0.5168, decode.d2.loss_dice: 0.7773, decode.d3.loss_cls: 0.3200, decode.d3.loss_mask: 0.5105, decode.d3.loss_dice: 0.7682, decode.d4.loss_cls: 0.3136, decode.d4.loss_mask: 0.5090, decode.d4.loss_dice: 0.7678, decode.d5.loss_cls: 0.3073, decode.d5.loss_mask: 0.5089, decode.d5.loss_dice: 0.7673, decode.d6.loss_cls: 0.3063, decode.d6.loss_mask: 0.5086, decode.d6.loss_dice: 0.7627, decode.d7.loss_cls: 0.3016, decode.d7.loss_mask: 0.5090, decode.d7.loss_dice: 0.7648, loss: 16.0372
2022-10-29 16:54:37,326 - mmseg - INFO - Iter [8700/20000]	lr: 1.725e-06, eta: 7:31:03, time: 2.332, data_time: 0.022, memory: 35422, decode.loss_cls: 0.3145, decode.loss_mask: 0.5251, decode.loss_dice: 0.7825, decode.d0.loss_cls: 1.8553, decode.d0.loss_mask: 0.5414, decode.d0.loss_dice: 0.8521, decode.d1.loss_cls: 0.3894, decode.d1.loss_mask: 0.5398, decode.d1.loss_dice: 0.8196, decode.d2.loss_cls: 0.3483, decode.d2.loss_mask: 0.5301, decode.d2.loss_dice: 0.7962, decode.d3.loss_cls: 0.3270, decode.d3.loss_mask: 0.5264, decode.d3.loss_dice: 0.7842, decode.d4.loss_cls: 0.3201, decode.d4.loss_mask: 0.5261, decode.d4.loss_dice: 0.7853, decode.d5.loss_cls: 0.3165, decode.d5.loss_mask: 0.5237, decode.d5.loss_dice: 0.7836, decode.d6.loss_cls: 0.3144, decode.d6.loss_mask: 0.5221, decode.d6.loss_dice: 0.7795, decode.d7.loss_cls: 0.3129, decode.d7.loss_mask: 0.5227, decode.d7.loss_dice: 0.7829, loss: 16.4216
2022-10-29 16:56:42,303 - mmseg - INFO - Iter [8750/20000]	lr: 1.717e-06, eta: 7:29:10, time: 2.500, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3228, decode.loss_mask: 0.5153, decode.loss_dice: 0.7870, decode.d0.loss_cls: 1.8452, decode.d0.loss_mask: 0.5353, decode.d0.loss_dice: 0.8600, decode.d1.loss_cls: 0.3940, decode.d1.loss_mask: 0.5327, decode.d1.loss_dice: 0.8326, decode.d2.loss_cls: 0.3494, decode.d2.loss_mask: 0.5211, decode.d2.loss_dice: 0.8039, decode.d3.loss_cls: 0.3360, decode.d3.loss_mask: 0.5168, decode.d3.loss_dice: 0.7877, decode.d4.loss_cls: 0.3291, decode.d4.loss_mask: 0.5155, decode.d4.loss_dice: 0.7856, decode.d5.loss_cls: 0.3210, decode.d5.loss_mask: 0.5144, decode.d5.loss_dice: 0.7888, decode.d6.loss_cls: 0.3191, decode.d6.loss_mask: 0.5153, decode.d6.loss_dice: 0.7868, decode.d7.loss_cls: 0.3210, decode.d7.loss_mask: 0.5160, decode.d7.loss_dice: 0.7848, loss: 16.4372
2022-10-29 16:58:39,551 - mmseg - INFO - Iter [8800/20000]	lr: 1.709e-06, eta: 7:27:07, time: 2.345, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3089, decode.loss_mask: 0.5130, decode.loss_dice: 0.7561, decode.d0.loss_cls: 1.8463, decode.d0.loss_mask: 0.5315, decode.d0.loss_dice: 0.8303, decode.d1.loss_cls: 0.3801, decode.d1.loss_mask: 0.5278, decode.d1.loss_dice: 0.7970, decode.d2.loss_cls: 0.3413, decode.d2.loss_mask: 0.5170, decode.d2.loss_dice: 0.7708, decode.d3.loss_cls: 0.3247, decode.d3.loss_mask: 0.5138, decode.d3.loss_dice: 0.7594, decode.d4.loss_cls: 0.3166, decode.d4.loss_mask: 0.5144, decode.d4.loss_dice: 0.7598, decode.d5.loss_cls: 0.3121, decode.d5.loss_mask: 0.5121, decode.d5.loss_dice: 0.7594, decode.d6.loss_cls: 0.3105, decode.d6.loss_mask: 0.5111, decode.d6.loss_dice: 0.7549, decode.d7.loss_cls: 0.3104, decode.d7.loss_mask: 0.5117, decode.d7.loss_dice: 0.7551, loss: 16.0461
2022-10-29 17:00:42,687 - mmseg - INFO - Iter [8850/20000]	lr: 1.702e-06, eta: 7:25:11, time: 2.463, data_time: 0.058, memory: 35422, decode.loss_cls: 0.3151, decode.loss_mask: 0.5192, decode.loss_dice: 0.7714, decode.d0.loss_cls: 1.8364, decode.d0.loss_mask: 0.5392, decode.d0.loss_dice: 0.8437, decode.d1.loss_cls: 0.3900, decode.d1.loss_mask: 0.5380, decode.d1.loss_dice: 0.8132, decode.d2.loss_cls: 0.3445, decode.d2.loss_mask: 0.5262, decode.d2.loss_dice: 0.7888, decode.d3.loss_cls: 0.3301, decode.d3.loss_mask: 0.5223, decode.d3.loss_dice: 0.7774, decode.d4.loss_cls: 0.3213, decode.d4.loss_mask: 0.5219, decode.d4.loss_dice: 0.7762, decode.d5.loss_cls: 0.3190, decode.d5.loss_mask: 0.5199, decode.d5.loss_dice: 0.7741, decode.d6.loss_cls: 0.3196, decode.d6.loss_mask: 0.5186, decode.d6.loss_dice: 0.7696, decode.d7.loss_cls: 0.3167, decode.d7.loss_mask: 0.5191, decode.d7.loss_dice: 0.7689, loss: 16.3002
2022-10-29 17:02:39,396 - mmseg - INFO - Iter [8900/20000]	lr: 1.694e-06, eta: 7:23:08, time: 2.333, data_time: 0.014, memory: 35422, decode.loss_cls: 0.3108, decode.loss_mask: 0.5048, decode.loss_dice: 0.7576, decode.d0.loss_cls: 1.8376, decode.d0.loss_mask: 0.5245, decode.d0.loss_dice: 0.8339, decode.d1.loss_cls: 0.3867, decode.d1.loss_mask: 0.5191, decode.d1.loss_dice: 0.7980, decode.d2.loss_cls: 0.3404, decode.d2.loss_mask: 0.5128, decode.d2.loss_dice: 0.7764, decode.d3.loss_cls: 0.3264, decode.d3.loss_mask: 0.5068, decode.d3.loss_dice: 0.7647, decode.d4.loss_cls: 0.3196, decode.d4.loss_mask: 0.5066, decode.d4.loss_dice: 0.7630, decode.d5.loss_cls: 0.3128, decode.d5.loss_mask: 0.5044, decode.d5.loss_dice: 0.7617, decode.d6.loss_cls: 0.3097, decode.d6.loss_mask: 0.5045, decode.d6.loss_dice: 0.7595, decode.d7.loss_cls: 0.3110, decode.d7.loss_mask: 0.5039, decode.d7.loss_dice: 0.7581, loss: 16.0154
2022-10-29 17:04:36,689 - mmseg - INFO - Iter [8950/20000]	lr: 1.686e-06, eta: 7:21:05, time: 2.347, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3009, decode.loss_mask: 0.5098, decode.loss_dice: 0.7670, decode.d0.loss_cls: 1.8241, decode.d0.loss_mask: 0.5326, decode.d0.loss_dice: 0.8444, decode.d1.loss_cls: 0.3764, decode.d1.loss_mask: 0.5267, decode.d1.loss_dice: 0.8095, decode.d2.loss_cls: 0.3356, decode.d2.loss_mask: 0.5173, decode.d2.loss_dice: 0.7833, decode.d3.loss_cls: 0.3131, decode.d3.loss_mask: 0.5127, decode.d3.loss_dice: 0.7721, decode.d4.loss_cls: 0.3070, decode.d4.loss_mask: 0.5120, decode.d4.loss_dice: 0.7709, decode.d5.loss_cls: 0.3042, decode.d5.loss_mask: 0.5105, decode.d5.loss_dice: 0.7675, decode.d6.loss_cls: 0.3032, decode.d6.loss_mask: 0.5098, decode.d6.loss_dice: 0.7655, decode.d7.loss_cls: 0.3002, decode.d7.loss_mask: 0.5107, decode.d7.loss_dice: 0.7672, loss: 16.0542
2022-10-29 17:06:35,367 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-29 17:06:35,369 - mmseg - INFO - Iter [9000/20000]	lr: 1.679e-06, eta: 7:19:04, time: 2.373, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2997, decode.loss_mask: 0.5146, decode.loss_dice: 0.7547, decode.d0.loss_cls: 1.7862, decode.d0.loss_mask: 0.5329, decode.d0.loss_dice: 0.8209, decode.d1.loss_cls: 0.3700, decode.d1.loss_mask: 0.5328, decode.d1.loss_dice: 0.7934, decode.d2.loss_cls: 0.3293, decode.d2.loss_mask: 0.5224, decode.d2.loss_dice: 0.7677, decode.d3.loss_cls: 0.3091, decode.d3.loss_mask: 0.5185, decode.d3.loss_dice: 0.7576, decode.d4.loss_cls: 0.3082, decode.d4.loss_mask: 0.5152, decode.d4.loss_dice: 0.7556, decode.d5.loss_cls: 0.3027, decode.d5.loss_mask: 0.5137, decode.d5.loss_dice: 0.7556, decode.d6.loss_cls: 0.3002, decode.d6.loss_mask: 0.5135, decode.d6.loss_dice: 0.7534, decode.d7.loss_cls: 0.2997, decode.d7.loss_mask: 0.5149, decode.d7.loss_dice: 0.7520, loss: 15.8948
2022-10-29 17:08:33,751 - mmseg - INFO - Iter [9050/20000]	lr: 1.671e-06, eta: 7:17:02, time: 2.368, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3158, decode.loss_mask: 0.5154, decode.loss_dice: 0.7701, decode.d0.loss_cls: 1.8290, decode.d0.loss_mask: 0.5360, decode.d0.loss_dice: 0.8401, decode.d1.loss_cls: 0.3873, decode.d1.loss_mask: 0.5334, decode.d1.loss_dice: 0.8082, decode.d2.loss_cls: 0.3444, decode.d2.loss_mask: 0.5215, decode.d2.loss_dice: 0.7872, decode.d3.loss_cls: 0.3294, decode.d3.loss_mask: 0.5179, decode.d3.loss_dice: 0.7712, decode.d4.loss_cls: 0.3238, decode.d4.loss_mask: 0.5162, decode.d4.loss_dice: 0.7729, decode.d5.loss_cls: 0.3193, decode.d5.loss_mask: 0.5167, decode.d5.loss_dice: 0.7711, decode.d6.loss_cls: 0.3155, decode.d6.loss_mask: 0.5156, decode.d6.loss_dice: 0.7704, decode.d7.loss_cls: 0.3166, decode.d7.loss_mask: 0.5155, decode.d7.loss_dice: 0.7695, loss: 16.2300
2022-10-29 17:10:29,647 - mmseg - INFO - Iter [9100/20000]	lr: 1.664e-06, eta: 7:14:58, time: 2.318, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3104, decode.loss_mask: 0.5218, decode.loss_dice: 0.7707, decode.d0.loss_cls: 1.7940, decode.d0.loss_mask: 0.5388, decode.d0.loss_dice: 0.8427, decode.d1.loss_cls: 0.3831, decode.d1.loss_mask: 0.5352, decode.d1.loss_dice: 0.8075, decode.d2.loss_cls: 0.3424, decode.d2.loss_mask: 0.5260, decode.d2.loss_dice: 0.7894, decode.d3.loss_cls: 0.3263, decode.d3.loss_mask: 0.5191, decode.d3.loss_dice: 0.7723, decode.d4.loss_cls: 0.3165, decode.d4.loss_mask: 0.5198, decode.d4.loss_dice: 0.7748, decode.d5.loss_cls: 0.3132, decode.d5.loss_mask: 0.5197, decode.d5.loss_dice: 0.7708, decode.d6.loss_cls: 0.3082, decode.d6.loss_mask: 0.5199, decode.d6.loss_dice: 0.7696, decode.d7.loss_cls: 0.3064, decode.d7.loss_mask: 0.5206, decode.d7.loss_dice: 0.7709, loss: 16.1903
2022-10-29 17:12:31,623 - mmseg - INFO - Iter [9150/20000]	lr: 1.656e-06, eta: 7:13:01, time: 2.439, data_time: 0.012, memory: 35422, decode.loss_cls: 0.3134, decode.loss_mask: 0.5232, decode.loss_dice: 0.7729, decode.d0.loss_cls: 1.7979, decode.d0.loss_mask: 0.5420, decode.d0.loss_dice: 0.8471, decode.d1.loss_cls: 0.3874, decode.d1.loss_mask: 0.5379, decode.d1.loss_dice: 0.8133, decode.d2.loss_cls: 0.3441, decode.d2.loss_mask: 0.5297, decode.d2.loss_dice: 0.7906, decode.d3.loss_cls: 0.3256, decode.d3.loss_mask: 0.5251, decode.d3.loss_dice: 0.7774, decode.d4.loss_cls: 0.3184, decode.d4.loss_mask: 0.5236, decode.d4.loss_dice: 0.7775, decode.d5.loss_cls: 0.3158, decode.d5.loss_mask: 0.5221, decode.d5.loss_dice: 0.7760, decode.d6.loss_cls: 0.3121, decode.d6.loss_mask: 0.5216, decode.d6.loss_dice: 0.7718, decode.d7.loss_cls: 0.3140, decode.d7.loss_mask: 0.5217, decode.d7.loss_dice: 0.7736, loss: 16.2758
2022-10-29 17:14:35,074 - mmseg - INFO - Iter [9200/20000]	lr: 1.648e-06, eta: 7:11:06, time: 2.468, data_time: 0.059, memory: 35422, decode.loss_cls: 0.2978, decode.loss_mask: 0.4980, decode.loss_dice: 0.7507, decode.d0.loss_cls: 1.7901, decode.d0.loss_mask: 0.5158, decode.d0.loss_dice: 0.8248, decode.d1.loss_cls: 0.3723, decode.d1.loss_mask: 0.5118, decode.d1.loss_dice: 0.7870, decode.d2.loss_cls: 0.3277, decode.d2.loss_mask: 0.5034, decode.d2.loss_dice: 0.7654, decode.d3.loss_cls: 0.3081, decode.d3.loss_mask: 0.5002, decode.d3.loss_dice: 0.7545, decode.d4.loss_cls: 0.3049, decode.d4.loss_mask: 0.4978, decode.d4.loss_dice: 0.7555, decode.d5.loss_cls: 0.2981, decode.d5.loss_mask: 0.4965, decode.d5.loss_dice: 0.7546, decode.d6.loss_cls: 0.2967, decode.d6.loss_mask: 0.4958, decode.d6.loss_dice: 0.7479, decode.d7.loss_cls: 0.2936, decode.d7.loss_mask: 0.4975, decode.d7.loss_dice: 0.7488, loss: 15.6953
2022-10-29 17:16:37,929 - mmseg - INFO - Iter [9250/20000]	lr: 1.641e-06, eta: 7:09:09, time: 2.458, data_time: 0.014, memory: 35422, decode.loss_cls: 0.3051, decode.loss_mask: 0.5074, decode.loss_dice: 0.7678, decode.d0.loss_cls: 1.7975, decode.d0.loss_mask: 0.5267, decode.d0.loss_dice: 0.8356, decode.d1.loss_cls: 0.3803, decode.d1.loss_mask: 0.5233, decode.d1.loss_dice: 0.8088, decode.d2.loss_cls: 0.3340, decode.d2.loss_mask: 0.5152, decode.d2.loss_dice: 0.7825, decode.d3.loss_cls: 0.3187, decode.d3.loss_mask: 0.5095, decode.d3.loss_dice: 0.7736, decode.d4.loss_cls: 0.3122, decode.d4.loss_mask: 0.5094, decode.d4.loss_dice: 0.7714, decode.d5.loss_cls: 0.3066, decode.d5.loss_mask: 0.5082, decode.d5.loss_dice: 0.7665, decode.d6.loss_cls: 0.3054, decode.d6.loss_mask: 0.5066, decode.d6.loss_dice: 0.7663, decode.d7.loss_cls: 0.3026, decode.d7.loss_mask: 0.5079, decode.d7.loss_dice: 0.7670, loss: 16.0162
2022-10-29 17:18:34,957 - mmseg - INFO - Iter [9300/20000]	lr: 1.633e-06, eta: 7:07:07, time: 2.341, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3005, decode.loss_mask: 0.5160, decode.loss_dice: 0.7587, decode.d0.loss_cls: 1.7738, decode.d0.loss_mask: 0.5332, decode.d0.loss_dice: 0.8308, decode.d1.loss_cls: 0.3737, decode.d1.loss_mask: 0.5297, decode.d1.loss_dice: 0.7991, decode.d2.loss_cls: 0.3300, decode.d2.loss_mask: 0.5217, decode.d2.loss_dice: 0.7745, decode.d3.loss_cls: 0.3122, decode.d3.loss_mask: 0.5171, decode.d3.loss_dice: 0.7641, decode.d4.loss_cls: 0.3066, decode.d4.loss_mask: 0.5159, decode.d4.loss_dice: 0.7615, decode.d5.loss_cls: 0.3013, decode.d5.loss_mask: 0.5163, decode.d5.loss_dice: 0.7642, decode.d6.loss_cls: 0.2990, decode.d6.loss_mask: 0.5152, decode.d6.loss_dice: 0.7582, decode.d7.loss_cls: 0.2970, decode.d7.loss_mask: 0.5165, decode.d7.loss_dice: 0.7599, loss: 15.9466
2022-10-29 17:20:33,357 - mmseg - INFO - Iter [9350/20000]	lr: 1.625e-06, eta: 7:05:05, time: 2.365, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3001, decode.loss_mask: 0.5135, decode.loss_dice: 0.7633, decode.d0.loss_cls: 1.7800, decode.d0.loss_mask: 0.5351, decode.d0.loss_dice: 0.8352, decode.d1.loss_cls: 0.3741, decode.d1.loss_mask: 0.5299, decode.d1.loss_dice: 0.8029, decode.d2.loss_cls: 0.3317, decode.d2.loss_mask: 0.5192, decode.d2.loss_dice: 0.7802, decode.d3.loss_cls: 0.3137, decode.d3.loss_mask: 0.5157, decode.d3.loss_dice: 0.7651, decode.d4.loss_cls: 0.3073, decode.d4.loss_mask: 0.5158, decode.d4.loss_dice: 0.7638, decode.d5.loss_cls: 0.3024, decode.d5.loss_mask: 0.5148, decode.d5.loss_dice: 0.7648, decode.d6.loss_cls: 0.2987, decode.d6.loss_mask: 0.5135, decode.d6.loss_dice: 0.7597, decode.d7.loss_cls: 0.3000, decode.d7.loss_mask: 0.5141, decode.d7.loss_dice: 0.7646, loss: 15.9792
2022-10-29 17:22:31,924 - mmseg - INFO - Iter [9400/20000]	lr: 1.618e-06, eta: 7:03:04, time: 2.374, data_time: 0.016, memory: 35422, decode.loss_cls: 0.3009, decode.loss_mask: 0.5129, decode.loss_dice: 0.7689, decode.d0.loss_cls: 1.7610, decode.d0.loss_mask: 0.5327, decode.d0.loss_dice: 0.8438, decode.d1.loss_cls: 0.3708, decode.d1.loss_mask: 0.5285, decode.d1.loss_dice: 0.8097, decode.d2.loss_cls: 0.3296, decode.d2.loss_mask: 0.5201, decode.d2.loss_dice: 0.7836, decode.d3.loss_cls: 0.3156, decode.d3.loss_mask: 0.5141, decode.d3.loss_dice: 0.7725, decode.d4.loss_cls: 0.3113, decode.d4.loss_mask: 0.5115, decode.d4.loss_dice: 0.7689, decode.d5.loss_cls: 0.3079, decode.d5.loss_mask: 0.5106, decode.d5.loss_dice: 0.7680, decode.d6.loss_cls: 0.3033, decode.d6.loss_mask: 0.5106, decode.d6.loss_dice: 0.7647, decode.d7.loss_cls: 0.3012, decode.d7.loss_mask: 0.5127, decode.d7.loss_dice: 0.7660, loss: 16.0015
2022-10-29 17:24:30,623 - mmseg - INFO - Iter [9450/20000]	lr: 1.610e-06, eta: 7:01:03, time: 2.373, data_time: 0.012, memory: 35422, decode.loss_cls: 0.2925, decode.loss_mask: 0.5149, decode.loss_dice: 0.7485, decode.d0.loss_cls: 1.7629, decode.d0.loss_mask: 0.5368, decode.d0.loss_dice: 0.8246, decode.d1.loss_cls: 0.3643, decode.d1.loss_mask: 0.5331, decode.d1.loss_dice: 0.7914, decode.d2.loss_cls: 0.3212, decode.d2.loss_mask: 0.5238, decode.d2.loss_dice: 0.7672, decode.d3.loss_cls: 0.3067, decode.d3.loss_mask: 0.5168, decode.d3.loss_dice: 0.7574, decode.d4.loss_cls: 0.3033, decode.d4.loss_mask: 0.5165, decode.d4.loss_dice: 0.7522, decode.d5.loss_cls: 0.2956, decode.d5.loss_mask: 0.5158, decode.d5.loss_dice: 0.7529, decode.d6.loss_cls: 0.2945, decode.d6.loss_mask: 0.5141, decode.d6.loss_dice: 0.7499, decode.d7.loss_cls: 0.2925, decode.d7.loss_mask: 0.5147, decode.d7.loss_dice: 0.7487, loss: 15.8128
2022-10-29 17:26:29,875 - mmseg - INFO - Iter [9500/20000]	lr: 1.603e-06, eta: 6:59:03, time: 2.386, data_time: 0.056, memory: 35422, decode.loss_cls: 0.2951, decode.loss_mask: 0.4979, decode.loss_dice: 0.7518, decode.d0.loss_cls: 1.7790, decode.d0.loss_mask: 0.5195, decode.d0.loss_dice: 0.8213, decode.d1.loss_cls: 0.3682, decode.d1.loss_mask: 0.5131, decode.d1.loss_dice: 0.7950, decode.d2.loss_cls: 0.3244, decode.d2.loss_mask: 0.5044, decode.d2.loss_dice: 0.7669, decode.d3.loss_cls: 0.3055, decode.d3.loss_mask: 0.5002, decode.d3.loss_dice: 0.7560, decode.d4.loss_cls: 0.2988, decode.d4.loss_mask: 0.4994, decode.d4.loss_dice: 0.7551, decode.d5.loss_cls: 0.2954, decode.d5.loss_mask: 0.4990, decode.d5.loss_dice: 0.7541, decode.d6.loss_cls: 0.2934, decode.d6.loss_mask: 0.4986, decode.d6.loss_dice: 0.7506, decode.d7.loss_cls: 0.2931, decode.d7.loss_mask: 0.4993, decode.d7.loss_dice: 0.7538, loss: 15.6887
2022-10-29 17:28:23,299 - mmseg - INFO - Iter [9550/20000]	lr: 1.595e-06, eta: 6:56:56, time: 2.268, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2879, decode.loss_mask: 0.5152, decode.loss_dice: 0.7661, decode.d0.loss_cls: 1.7325, decode.d0.loss_mask: 0.5347, decode.d0.loss_dice: 0.8352, decode.d1.loss_cls: 0.3514, decode.d1.loss_mask: 0.5296, decode.d1.loss_dice: 0.8037, decode.d2.loss_cls: 0.3167, decode.d2.loss_mask: 0.5214, decode.d2.loss_dice: 0.7797, decode.d3.loss_cls: 0.2996, decode.d3.loss_mask: 0.5177, decode.d3.loss_dice: 0.7703, decode.d4.loss_cls: 0.2946, decode.d4.loss_mask: 0.5156, decode.d4.loss_dice: 0.7692, decode.d5.loss_cls: 0.2915, decode.d5.loss_mask: 0.5149, decode.d5.loss_dice: 0.7670, decode.d6.loss_cls: 0.2865, decode.d6.loss_mask: 0.5137, decode.d6.loss_dice: 0.7643, decode.d7.loss_cls: 0.2881, decode.d7.loss_mask: 0.5132, decode.d7.loss_dice: 0.7645, loss: 15.8447
2022-10-29 17:30:22,907 - mmseg - INFO - Iter [9600/20000]	lr: 1.587e-06, eta: 6:54:57, time: 2.392, data_time: 0.012, memory: 35422, decode.loss_cls: 0.2895, decode.loss_mask: 0.5024, decode.loss_dice: 0.7539, decode.d0.loss_cls: 1.7518, decode.d0.loss_mask: 0.5191, decode.d0.loss_dice: 0.8290, decode.d1.loss_cls: 0.3654, decode.d1.loss_mask: 0.5181, decode.d1.loss_dice: 0.7961, decode.d2.loss_cls: 0.3238, decode.d2.loss_mask: 0.5082, decode.d2.loss_dice: 0.7690, decode.d3.loss_cls: 0.3049, decode.d3.loss_mask: 0.5049, decode.d3.loss_dice: 0.7582, decode.d4.loss_cls: 0.2990, decode.d4.loss_mask: 0.5028, decode.d4.loss_dice: 0.7602, decode.d5.loss_cls: 0.2947, decode.d5.loss_mask: 0.5021, decode.d5.loss_dice: 0.7550, decode.d6.loss_cls: 0.2920, decode.d6.loss_mask: 0.5006, decode.d6.loss_dice: 0.7526, decode.d7.loss_cls: 0.2908, decode.d7.loss_mask: 0.5023, decode.d7.loss_dice: 0.7557, loss: 15.7022
2022-10-29 17:32:13,980 - mmseg - INFO - Iter [9650/20000]	lr: 1.580e-06, eta: 6:52:48, time: 2.221, data_time: 0.012, memory: 35422, decode.loss_cls: 0.2872, decode.loss_mask: 0.5011, decode.loss_dice: 0.7489, decode.d0.loss_cls: 1.7458, decode.d0.loss_mask: 0.5184, decode.d0.loss_dice: 0.8176, decode.d1.loss_cls: 0.3568, decode.d1.loss_mask: 0.5159, decode.d1.loss_dice: 0.7857, decode.d2.loss_cls: 0.3172, decode.d2.loss_mask: 0.5063, decode.d2.loss_dice: 0.7617, decode.d3.loss_cls: 0.2991, decode.d3.loss_mask: 0.5008, decode.d3.loss_dice: 0.7531, decode.d4.loss_cls: 0.2941, decode.d4.loss_mask: 0.5010, decode.d4.loss_dice: 0.7533, decode.d5.loss_cls: 0.2914, decode.d5.loss_mask: 0.4999, decode.d5.loss_dice: 0.7506, decode.d6.loss_cls: 0.2883, decode.d6.loss_mask: 0.5006, decode.d6.loss_dice: 0.7481, decode.d7.loss_cls: 0.2871, decode.d7.loss_mask: 0.5004, decode.d7.loss_dice: 0.7488, loss: 15.5792
2022-10-29 17:34:10,452 - mmseg - INFO - Iter [9700/20000]	lr: 1.572e-06, eta: 6:50:45, time: 2.329, data_time: 0.014, memory: 35422, decode.loss_cls: 0.2853, decode.loss_mask: 0.5002, decode.loss_dice: 0.7471, decode.d0.loss_cls: 1.7539, decode.d0.loss_mask: 0.5210, decode.d0.loss_dice: 0.8148, decode.d1.loss_cls: 0.3579, decode.d1.loss_mask: 0.5165, decode.d1.loss_dice: 0.7860, decode.d2.loss_cls: 0.3160, decode.d2.loss_mask: 0.5083, decode.d2.loss_dice: 0.7648, decode.d3.loss_cls: 0.2977, decode.d3.loss_mask: 0.5041, decode.d3.loss_dice: 0.7519, decode.d4.loss_cls: 0.2939, decode.d4.loss_mask: 0.5022, decode.d4.loss_dice: 0.7505, decode.d5.loss_cls: 0.2888, decode.d5.loss_mask: 0.5019, decode.d5.loss_dice: 0.7489, decode.d6.loss_cls: 0.2867, decode.d6.loss_mask: 0.4997, decode.d6.loss_dice: 0.7459, decode.d7.loss_cls: 0.2852, decode.d7.loss_mask: 0.5009, decode.d7.loss_dice: 0.7472, loss: 15.5774
2022-10-29 17:36:04,312 - mmseg - INFO - Iter [9750/20000]	lr: 1.564e-06, eta: 6:48:39, time: 2.277, data_time: 0.013, memory: 35422, decode.loss_cls: 0.3001, decode.loss_mask: 0.5061, decode.loss_dice: 0.7482, decode.d0.loss_cls: 1.7574, decode.d0.loss_mask: 0.5263, decode.d0.loss_dice: 0.8231, decode.d1.loss_cls: 0.3702, decode.d1.loss_mask: 0.5235, decode.d1.loss_dice: 0.7891, decode.d2.loss_cls: 0.3306, decode.d2.loss_mask: 0.5150, decode.d2.loss_dice: 0.7631, decode.d3.loss_cls: 0.3135, decode.d3.loss_mask: 0.5097, decode.d3.loss_dice: 0.7530, decode.d4.loss_cls: 0.3081, decode.d4.loss_mask: 0.5087, decode.d4.loss_dice: 0.7516, decode.d5.loss_cls: 0.3045, decode.d5.loss_mask: 0.5062, decode.d5.loss_dice: 0.7471, decode.d6.loss_cls: 0.3012, decode.d6.loss_mask: 0.5062, decode.d6.loss_dice: 0.7467, decode.d7.loss_cls: 0.3012, decode.d7.loss_mask: 0.5058, decode.d7.loss_dice: 0.7456, loss: 15.7619
2022-10-29 17:38:00,726 - mmseg - INFO - Iter [9800/20000]	lr: 1.557e-06, eta: 6:46:36, time: 2.328, data_time: 0.073, memory: 35422, decode.loss_cls: 0.2980, decode.loss_mask: 0.5046, decode.loss_dice: 0.7545, decode.d0.loss_cls: 1.7351, decode.d0.loss_mask: 0.5265, decode.d0.loss_dice: 0.8306, decode.d1.loss_cls: 0.3686, decode.d1.loss_mask: 0.5202, decode.d1.loss_dice: 0.7962, decode.d2.loss_cls: 0.3285, decode.d2.loss_mask: 0.5096, decode.d2.loss_dice: 0.7713, decode.d3.loss_cls: 0.3119, decode.d3.loss_mask: 0.5067, decode.d3.loss_dice: 0.7613, decode.d4.loss_cls: 0.3071, decode.d4.loss_mask: 0.5056, decode.d4.loss_dice: 0.7616, decode.d5.loss_cls: 0.3015, decode.d5.loss_mask: 0.5061, decode.d5.loss_dice: 0.7584, decode.d6.loss_cls: 0.2981, decode.d6.loss_mask: 0.5042, decode.d6.loss_dice: 0.7578, decode.d7.loss_cls: 0.2992, decode.d7.loss_mask: 0.5051, decode.d7.loss_dice: 0.7573, loss: 15.7853
2022-10-29 17:40:02,354 - mmseg - INFO - Iter [9850/20000]	lr: 1.549e-06, eta: 6:44:38, time: 2.433, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2955, decode.loss_mask: 0.5140, decode.loss_dice: 0.7546, decode.d0.loss_cls: 1.7288, decode.d0.loss_mask: 0.5371, decode.d0.loss_dice: 0.8264, decode.d1.loss_cls: 0.3634, decode.d1.loss_mask: 0.5297, decode.d1.loss_dice: 0.7920, decode.d2.loss_cls: 0.3241, decode.d2.loss_mask: 0.5189, decode.d2.loss_dice: 0.7701, decode.d3.loss_cls: 0.3067, decode.d3.loss_mask: 0.5151, decode.d3.loss_dice: 0.7561, decode.d4.loss_cls: 0.2993, decode.d4.loss_mask: 0.5138, decode.d4.loss_dice: 0.7570, decode.d5.loss_cls: 0.2981, decode.d5.loss_mask: 0.5130, decode.d5.loss_dice: 0.7530, decode.d6.loss_cls: 0.2951, decode.d6.loss_mask: 0.5128, decode.d6.loss_dice: 0.7526, decode.d7.loss_cls: 0.2969, decode.d7.loss_mask: 0.5129, decode.d7.loss_dice: 0.7515, loss: 15.7887
2022-10-29 17:41:59,238 - mmseg - INFO - Iter [9900/20000]	lr: 1.541e-06, eta: 6:42:36, time: 2.338, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2873, decode.loss_mask: 0.5041, decode.loss_dice: 0.7449, decode.d0.loss_cls: 1.7275, decode.d0.loss_mask: 0.5234, decode.d0.loss_dice: 0.8165, decode.d1.loss_cls: 0.3542, decode.d1.loss_mask: 0.5203, decode.d1.loss_dice: 0.7818, decode.d2.loss_cls: 0.3138, decode.d2.loss_mask: 0.5099, decode.d2.loss_dice: 0.7600, decode.d3.loss_cls: 0.2956, decode.d3.loss_mask: 0.5052, decode.d3.loss_dice: 0.7492, decode.d4.loss_cls: 0.2910, decode.d4.loss_mask: 0.5041, decode.d4.loss_dice: 0.7482, decode.d5.loss_cls: 0.2885, decode.d5.loss_mask: 0.5041, decode.d5.loss_dice: 0.7485, decode.d6.loss_cls: 0.2859, decode.d6.loss_mask: 0.5040, decode.d6.loss_dice: 0.7462, decode.d7.loss_cls: 0.2851, decode.d7.loss_mask: 0.5038, decode.d7.loss_dice: 0.7454, loss: 15.5486
2022-10-29 17:43:59,184 - mmseg - INFO - Iter [9950/20000]	lr: 1.534e-06, eta: 6:40:37, time: 2.399, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2800, decode.loss_mask: 0.5074, decode.loss_dice: 0.7442, decode.d0.loss_cls: 1.7168, decode.d0.loss_mask: 0.5269, decode.d0.loss_dice: 0.8134, decode.d1.loss_cls: 0.3539, decode.d1.loss_mask: 0.5233, decode.d1.loss_dice: 0.7824, decode.d2.loss_cls: 0.3120, decode.d2.loss_mask: 0.5131, decode.d2.loss_dice: 0.7603, decode.d3.loss_cls: 0.2944, decode.d3.loss_mask: 0.5095, decode.d3.loss_dice: 0.7493, decode.d4.loss_cls: 0.2871, decode.d4.loss_mask: 0.5082, decode.d4.loss_dice: 0.7523, decode.d5.loss_cls: 0.2835, decode.d5.loss_mask: 0.5078, decode.d5.loss_dice: 0.7464, decode.d6.loss_cls: 0.2797, decode.d6.loss_mask: 0.5076, decode.d6.loss_dice: 0.7468, decode.d7.loss_cls: 0.2799, decode.d7.loss_mask: 0.5073, decode.d7.loss_dice: 0.7444, loss: 15.5382
2022-10-29 17:45:53,481 - mmseg - INFO - Saving checkpoint at 10000 iterations
2022-10-29 17:46:26,403 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-29 17:46:26,404 - mmseg - INFO - Iter [10000/20000]	lr: 1.526e-06, eta: 6:39:05, time: 2.944, data_time: 0.012, memory: 35422, decode.loss_cls: 0.2901, decode.loss_mask: 0.5044, decode.loss_dice: 0.7460, decode.d0.loss_cls: 1.7096, decode.d0.loss_mask: 0.5241, decode.d0.loss_dice: 0.8194, decode.d1.loss_cls: 0.3593, decode.d1.loss_mask: 0.5200, decode.d1.loss_dice: 0.7871, decode.d2.loss_cls: 0.3185, decode.d2.loss_mask: 0.5112, decode.d2.loss_dice: 0.7639, decode.d3.loss_cls: 0.3001, decode.d3.loss_mask: 0.5067, decode.d3.loss_dice: 0.7504, decode.d4.loss_cls: 0.2949, decode.d4.loss_mask: 0.5049, decode.d4.loss_dice: 0.7482, decode.d5.loss_cls: 0.2913, decode.d5.loss_mask: 0.5042, decode.d5.loss_dice: 0.7486, decode.d6.loss_cls: 0.2882, decode.d6.loss_mask: 0.5042, decode.d6.loss_dice: 0.7486, decode.d7.loss_cls: 0.2899, decode.d7.loss_mask: 0.5042, decode.d7.loss_dice: 0.7469, loss: 15.5848
2022-10-29 17:48:21,975 - mmseg - INFO - Iter [10050/20000]	lr: 1.519e-06, eta: 6:37:01, time: 2.309, data_time: 0.012, memory: 35422, decode.loss_cls: 0.2852, decode.loss_mask: 0.4948, decode.loss_dice: 0.7470, decode.d0.loss_cls: 1.7296, decode.d0.loss_mask: 0.5154, decode.d0.loss_dice: 0.8177, decode.d1.loss_cls: 0.3558, decode.d1.loss_mask: 0.5106, decode.d1.loss_dice: 0.7804, decode.d2.loss_cls: 0.3142, decode.d2.loss_mask: 0.5010, decode.d2.loss_dice: 0.7622, decode.d3.loss_cls: 0.2976, decode.d3.loss_mask: 0.4966, decode.d3.loss_dice: 0.7518, decode.d4.loss_cls: 0.2924, decode.d4.loss_mask: 0.4961, decode.d4.loss_dice: 0.7496, decode.d5.loss_cls: 0.2870, decode.d5.loss_mask: 0.4958, decode.d5.loss_dice: 0.7490, decode.d6.loss_cls: 0.2862, decode.d6.loss_mask: 0.4942, decode.d6.loss_dice: 0.7435, decode.d7.loss_cls: 0.2862, decode.d7.loss_mask: 0.4953, decode.d7.loss_dice: 0.7481, loss: 15.4833
2022-10-29 17:50:19,788 - mmseg - INFO - Iter [10100/20000]	lr: 1.511e-06, eta: 6:35:00, time: 2.359, data_time: 0.016, memory: 35422, decode.loss_cls: 0.2865, decode.loss_mask: 0.4999, decode.loss_dice: 0.7426, decode.d0.loss_cls: 1.7115, decode.d0.loss_mask: 0.5227, decode.d0.loss_dice: 0.8133, decode.d1.loss_cls: 0.3544, decode.d1.loss_mask: 0.5175, decode.d1.loss_dice: 0.7826, decode.d2.loss_cls: 0.3158, decode.d2.loss_mask: 0.5069, decode.d2.loss_dice: 0.7594, decode.d3.loss_cls: 0.2968, decode.d3.loss_mask: 0.5039, decode.d3.loss_dice: 0.7457, decode.d4.loss_cls: 0.2923, decode.d4.loss_mask: 0.5026, decode.d4.loss_dice: 0.7441, decode.d5.loss_cls: 0.2896, decode.d5.loss_mask: 0.5014, decode.d5.loss_dice: 0.7423, decode.d6.loss_cls: 0.2876, decode.d6.loss_mask: 0.4990, decode.d6.loss_dice: 0.7390, decode.d7.loss_cls: 0.2849, decode.d7.loss_mask: 0.5000, decode.d7.loss_dice: 0.7436, loss: 15.4859
2022-10-29 17:52:16,803 - mmseg - INFO - Iter [10150/20000]	lr: 1.503e-06, eta: 6:32:57, time: 2.340, data_time: 0.059, memory: 35422, decode.loss_cls: 0.2968, decode.loss_mask: 0.5040, decode.loss_dice: 0.7513, decode.d0.loss_cls: 1.7198, decode.d0.loss_mask: 0.5266, decode.d0.loss_dice: 0.8228, decode.d1.loss_cls: 0.3664, decode.d1.loss_mask: 0.5185, decode.d1.loss_dice: 0.7909, decode.d2.loss_cls: 0.3250, decode.d2.loss_mask: 0.5102, decode.d2.loss_dice: 0.7657, decode.d3.loss_cls: 0.3081, decode.d3.loss_mask: 0.5066, decode.d3.loss_dice: 0.7579, decode.d4.loss_cls: 0.3030, decode.d4.loss_mask: 0.5052, decode.d4.loss_dice: 0.7551, decode.d5.loss_cls: 0.2981, decode.d5.loss_mask: 0.5044, decode.d5.loss_dice: 0.7535, decode.d6.loss_cls: 0.2967, decode.d6.loss_mask: 0.5039, decode.d6.loss_dice: 0.7526, decode.d7.loss_cls: 0.2955, decode.d7.loss_mask: 0.5043, decode.d7.loss_dice: 0.7514, loss: 15.6945
2022-10-29 17:54:12,283 - mmseg - INFO - Iter [10200/20000]	lr: 1.496e-06, eta: 6:30:53, time: 2.310, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2768, decode.loss_mask: 0.4971, decode.loss_dice: 0.7409, decode.d0.loss_cls: 1.7062, decode.d0.loss_mask: 0.5180, decode.d0.loss_dice: 0.8128, decode.d1.loss_cls: 0.3453, decode.d1.loss_mask: 0.5134, decode.d1.loss_dice: 0.7796, decode.d2.loss_cls: 0.3074, decode.d2.loss_mask: 0.5043, decode.d2.loss_dice: 0.7563, decode.d3.loss_cls: 0.2880, decode.d3.loss_mask: 0.4989, decode.d3.loss_dice: 0.7441, decode.d4.loss_cls: 0.2809, decode.d4.loss_mask: 0.4984, decode.d4.loss_dice: 0.7443, decode.d5.loss_cls: 0.2795, decode.d5.loss_mask: 0.4971, decode.d5.loss_dice: 0.7434, decode.d6.loss_cls: 0.2772, decode.d6.loss_mask: 0.4967, decode.d6.loss_dice: 0.7398, decode.d7.loss_cls: 0.2758, decode.d7.loss_mask: 0.4986, decode.d7.loss_dice: 0.7415, loss: 15.3621
2022-10-29 17:56:09,651 - mmseg - INFO - Iter [10250/20000]	lr: 1.488e-06, eta: 6:28:52, time: 2.347, data_time: 0.020, memory: 35422, decode.loss_cls: 0.2861, decode.loss_mask: 0.4942, decode.loss_dice: 0.7530, decode.d0.loss_cls: 1.7112, decode.d0.loss_mask: 0.5201, decode.d0.loss_dice: 0.8258, decode.d1.loss_cls: 0.3629, decode.d1.loss_mask: 0.5105, decode.d1.loss_dice: 0.7970, decode.d2.loss_cls: 0.3182, decode.d2.loss_mask: 0.5024, decode.d2.loss_dice: 0.7663, decode.d3.loss_cls: 0.3008, decode.d3.loss_mask: 0.4964, decode.d3.loss_dice: 0.7543, decode.d4.loss_cls: 0.2948, decode.d4.loss_mask: 0.4956, decode.d4.loss_dice: 0.7567, decode.d5.loss_cls: 0.2882, decode.d5.loss_mask: 0.4956, decode.d5.loss_dice: 0.7528, decode.d6.loss_cls: 0.2876, decode.d6.loss_mask: 0.4944, decode.d6.loss_dice: 0.7488, decode.d7.loss_cls: 0.2858, decode.d7.loss_mask: 0.4948, decode.d7.loss_dice: 0.7541, loss: 15.5481
2022-10-29 17:58:04,859 - mmseg - INFO - Iter [10300/20000]	lr: 1.480e-06, eta: 6:26:48, time: 2.304, data_time: 0.017, memory: 35422, decode.loss_cls: 0.2854, decode.loss_mask: 0.4969, decode.loss_dice: 0.7435, decode.d0.loss_cls: 1.7145, decode.d0.loss_mask: 0.5186, decode.d0.loss_dice: 0.8122, decode.d1.loss_cls: 0.3570, decode.d1.loss_mask: 0.5134, decode.d1.loss_dice: 0.7813, decode.d2.loss_cls: 0.3164, decode.d2.loss_mask: 0.5051, decode.d2.loss_dice: 0.7568, decode.d3.loss_cls: 0.2966, decode.d3.loss_mask: 0.4993, decode.d3.loss_dice: 0.7471, decode.d4.loss_cls: 0.2918, decode.d4.loss_mask: 0.4983, decode.d4.loss_dice: 0.7462, decode.d5.loss_cls: 0.2889, decode.d5.loss_mask: 0.4971, decode.d5.loss_dice: 0.7453, decode.d6.loss_cls: 0.2862, decode.d6.loss_mask: 0.4973, decode.d6.loss_dice: 0.7427, decode.d7.loss_cls: 0.2841, decode.d7.loss_mask: 0.4971, decode.d7.loss_dice: 0.7420, loss: 15.4608
2022-10-29 18:00:05,064 - mmseg - INFO - Iter [10350/20000]	lr: 1.473e-06, eta: 6:24:49, time: 2.404, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2809, decode.loss_mask: 0.5098, decode.loss_dice: 0.7493, decode.d0.loss_cls: 1.6781, decode.d0.loss_mask: 0.5307, decode.d0.loss_dice: 0.8203, decode.d1.loss_cls: 0.3480, decode.d1.loss_mask: 0.5250, decode.d1.loss_dice: 0.7884, decode.d2.loss_cls: 0.3106, decode.d2.loss_mask: 0.5153, decode.d2.loss_dice: 0.7660, decode.d3.loss_cls: 0.2921, decode.d3.loss_mask: 0.5109, decode.d3.loss_dice: 0.7539, decode.d4.loss_cls: 0.2861, decode.d4.loss_mask: 0.5093, decode.d4.loss_dice: 0.7522, decode.d5.loss_cls: 0.2831, decode.d5.loss_mask: 0.5079, decode.d5.loss_dice: 0.7507, decode.d6.loss_cls: 0.2804, decode.d6.loss_mask: 0.5083, decode.d6.loss_dice: 0.7476, decode.d7.loss_cls: 0.2811, decode.d7.loss_mask: 0.5081, decode.d7.loss_dice: 0.7493, loss: 15.5434
2022-10-29 18:02:08,142 - mmseg - INFO - Iter [10400/20000]	lr: 1.465e-06, eta: 6:22:52, time: 2.462, data_time: 0.014, memory: 35422, decode.loss_cls: 0.2829, decode.loss_mask: 0.5080, decode.loss_dice: 0.7449, decode.d0.loss_cls: 1.6961, decode.d0.loss_mask: 0.5292, decode.d0.loss_dice: 0.8158, decode.d1.loss_cls: 0.3528, decode.d1.loss_mask: 0.5225, decode.d1.loss_dice: 0.7832, decode.d2.loss_cls: 0.3140, decode.d2.loss_mask: 0.5127, decode.d2.loss_dice: 0.7604, decode.d3.loss_cls: 0.2961, decode.d3.loss_mask: 0.5084, decode.d3.loss_dice: 0.7513, decode.d4.loss_cls: 0.2918, decode.d4.loss_mask: 0.5074, decode.d4.loss_dice: 0.7485, decode.d5.loss_cls: 0.2870, decode.d5.loss_mask: 0.5083, decode.d5.loss_dice: 0.7486, decode.d6.loss_cls: 0.2836, decode.d6.loss_mask: 0.5079, decode.d6.loss_dice: 0.7466, decode.d7.loss_cls: 0.2858, decode.d7.loss_mask: 0.5076, decode.d7.loss_dice: 0.7444, loss: 15.5459
2022-10-29 18:04:12,766 - mmseg - INFO - Iter [10450/20000]	lr: 1.458e-06, eta: 6:20:57, time: 2.492, data_time: 0.059, memory: 35422, decode.loss_cls: 0.2785, decode.loss_mask: 0.4958, decode.loss_dice: 0.7403, decode.d0.loss_cls: 1.7036, decode.d0.loss_mask: 0.5120, decode.d0.loss_dice: 0.8098, decode.d1.loss_cls: 0.3494, decode.d1.loss_mask: 0.5115, decode.d1.loss_dice: 0.7770, decode.d2.loss_cls: 0.3092, decode.d2.loss_mask: 0.5024, decode.d2.loss_dice: 0.7582, decode.d3.loss_cls: 0.2903, decode.d3.loss_mask: 0.4984, decode.d3.loss_dice: 0.7461, decode.d4.loss_cls: 0.2862, decode.d4.loss_mask: 0.4971, decode.d4.loss_dice: 0.7466, decode.d5.loss_cls: 0.2827, decode.d5.loss_mask: 0.4949, decode.d5.loss_dice: 0.7404, decode.d6.loss_cls: 0.2800, decode.d6.loss_mask: 0.4939, decode.d6.loss_dice: 0.7405, decode.d7.loss_cls: 0.2782, decode.d7.loss_mask: 0.4942, decode.d7.loss_dice: 0.7425, loss: 15.3598
2022-10-29 18:06:16,788 - mmseg - INFO - Iter [10500/20000]	lr: 1.450e-06, eta: 6:19:01, time: 2.480, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2698, decode.loss_mask: 0.4865, decode.loss_dice: 0.7246, decode.d0.loss_cls: 1.6809, decode.d0.loss_mask: 0.5105, decode.d0.loss_dice: 0.7991, decode.d1.loss_cls: 0.3416, decode.d1.loss_mask: 0.5029, decode.d1.loss_dice: 0.7658, decode.d2.loss_cls: 0.2995, decode.d2.loss_mask: 0.4923, decode.d2.loss_dice: 0.7416, decode.d3.loss_cls: 0.2817, decode.d3.loss_mask: 0.4885, decode.d3.loss_dice: 0.7290, decode.d4.loss_cls: 0.2758, decode.d4.loss_mask: 0.4878, decode.d4.loss_dice: 0.7308, decode.d5.loss_cls: 0.2710, decode.d5.loss_mask: 0.4870, decode.d5.loss_dice: 0.7287, decode.d6.loss_cls: 0.2730, decode.d6.loss_mask: 0.4858, decode.d6.loss_dice: 0.7246, decode.d7.loss_cls: 0.2713, decode.d7.loss_mask: 0.4859, decode.d7.loss_dice: 0.7258, loss: 15.0616
2022-10-29 18:08:12,652 - mmseg - INFO - Iter [10550/20000]	lr: 1.442e-06, eta: 6:16:58, time: 2.317, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2874, decode.loss_mask: 0.5070, decode.loss_dice: 0.7570, decode.d0.loss_cls: 1.7033, decode.d0.loss_mask: 0.5293, decode.d0.loss_dice: 0.8256, decode.d1.loss_cls: 0.3545, decode.d1.loss_mask: 0.5238, decode.d1.loss_dice: 0.7952, decode.d2.loss_cls: 0.3191, decode.d2.loss_mask: 0.5119, decode.d2.loss_dice: 0.7739, decode.d3.loss_cls: 0.3004, decode.d3.loss_mask: 0.5087, decode.d3.loss_dice: 0.7600, decode.d4.loss_cls: 0.2932, decode.d4.loss_mask: 0.5083, decode.d4.loss_dice: 0.7621, decode.d5.loss_cls: 0.2865, decode.d5.loss_mask: 0.5088, decode.d5.loss_dice: 0.7599, decode.d6.loss_cls: 0.2879, decode.d6.loss_mask: 0.5077, decode.d6.loss_dice: 0.7574, decode.d7.loss_cls: 0.2868, decode.d7.loss_mask: 0.5078, decode.d7.loss_dice: 0.7579, loss: 15.6814
2022-10-29 18:10:11,376 - mmseg - INFO - Iter [10600/20000]	lr: 1.435e-06, eta: 6:14:58, time: 2.375, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2803, decode.loss_mask: 0.5006, decode.loss_dice: 0.7535, decode.d0.loss_cls: 1.6945, decode.d0.loss_mask: 0.5227, decode.d0.loss_dice: 0.8287, decode.d1.loss_cls: 0.3514, decode.d1.loss_mask: 0.5166, decode.d1.loss_dice: 0.7949, decode.d2.loss_cls: 0.3114, decode.d2.loss_mask: 0.5078, decode.d2.loss_dice: 0.7681, decode.d3.loss_cls: 0.2916, decode.d3.loss_mask: 0.5023, decode.d3.loss_dice: 0.7596, decode.d4.loss_cls: 0.2853, decode.d4.loss_mask: 0.5013, decode.d4.loss_dice: 0.7581, decode.d5.loss_cls: 0.2822, decode.d5.loss_mask: 0.4997, decode.d5.loss_dice: 0.7553, decode.d6.loss_cls: 0.2800, decode.d6.loss_mask: 0.4980, decode.d6.loss_dice: 0.7541, decode.d7.loss_cls: 0.2806, decode.d7.loss_mask: 0.4996, decode.d7.loss_dice: 0.7544, loss: 15.5329
2022-10-29 18:12:10,142 - mmseg - INFO - Iter [10650/20000]	lr: 1.427e-06, eta: 6:12:57, time: 2.375, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2786, decode.loss_mask: 0.4878, decode.loss_dice: 0.7303, decode.d0.loss_cls: 1.6706, decode.d0.loss_mask: 0.5143, decode.d0.loss_dice: 0.8073, decode.d1.loss_cls: 0.3502, decode.d1.loss_mask: 0.5043, decode.d1.loss_dice: 0.7685, decode.d2.loss_cls: 0.3074, decode.d2.loss_mask: 0.4946, decode.d2.loss_dice: 0.7465, decode.d3.loss_cls: 0.2915, decode.d3.loss_mask: 0.4906, decode.d3.loss_dice: 0.7343, decode.d4.loss_cls: 0.2848, decode.d4.loss_mask: 0.4884, decode.d4.loss_dice: 0.7322, decode.d5.loss_cls: 0.2804, decode.d5.loss_mask: 0.4876, decode.d5.loss_dice: 0.7343, decode.d6.loss_cls: 0.2793, decode.d6.loss_mask: 0.4872, decode.d6.loss_dice: 0.7288, decode.d7.loss_cls: 0.2792, decode.d7.loss_mask: 0.4883, decode.d7.loss_dice: 0.7278, loss: 15.1751
2022-10-29 18:14:07,119 - mmseg - INFO - Iter [10700/20000]	lr: 1.419e-06, eta: 6:10:55, time: 2.340, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2764, decode.loss_mask: 0.4946, decode.loss_dice: 0.7412, decode.d0.loss_cls: 1.6667, decode.d0.loss_mask: 0.5159, decode.d0.loss_dice: 0.8148, decode.d1.loss_cls: 0.3440, decode.d1.loss_mask: 0.5116, decode.d1.loss_dice: 0.7865, decode.d2.loss_cls: 0.3083, decode.d2.loss_mask: 0.5007, decode.d2.loss_dice: 0.7592, decode.d3.loss_cls: 0.2938, decode.d3.loss_mask: 0.4957, decode.d3.loss_dice: 0.7433, decode.d4.loss_cls: 0.2855, decode.d4.loss_mask: 0.4960, decode.d4.loss_dice: 0.7445, decode.d5.loss_cls: 0.2793, decode.d5.loss_mask: 0.4956, decode.d5.loss_dice: 0.7420, decode.d6.loss_cls: 0.2787, decode.d6.loss_mask: 0.4937, decode.d6.loss_dice: 0.7394, decode.d7.loss_cls: 0.2739, decode.d7.loss_mask: 0.4944, decode.d7.loss_dice: 0.7423, loss: 15.3178
2022-10-29 18:16:05,517 - mmseg - INFO - Iter [10750/20000]	lr: 1.412e-06, eta: 6:08:55, time: 2.368, data_time: 0.060, memory: 35422, decode.loss_cls: 0.2805, decode.loss_mask: 0.4951, decode.loss_dice: 0.7455, decode.d0.loss_cls: 1.6901, decode.d0.loss_mask: 0.5152, decode.d0.loss_dice: 0.8169, decode.d1.loss_cls: 0.3497, decode.d1.loss_mask: 0.5097, decode.d1.loss_dice: 0.7880, decode.d2.loss_cls: 0.3104, decode.d2.loss_mask: 0.5011, decode.d2.loss_dice: 0.7606, decode.d3.loss_cls: 0.2943, decode.d3.loss_mask: 0.4969, decode.d3.loss_dice: 0.7481, decode.d4.loss_cls: 0.2884, decode.d4.loss_mask: 0.4967, decode.d4.loss_dice: 0.7489, decode.d5.loss_cls: 0.2832, decode.d5.loss_mask: 0.4958, decode.d5.loss_dice: 0.7462, decode.d6.loss_cls: 0.2801, decode.d6.loss_mask: 0.4951, decode.d6.loss_dice: 0.7442, decode.d7.loss_cls: 0.2817, decode.d7.loss_mask: 0.4950, decode.d7.loss_dice: 0.7450, loss: 15.4025
2022-10-29 18:18:01,633 - mmseg - INFO - Iter [10800/20000]	lr: 1.404e-06, eta: 6:06:52, time: 2.322, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2699, decode.loss_mask: 0.4936, decode.loss_dice: 0.7362, decode.d0.loss_cls: 1.6758, decode.d0.loss_mask: 0.5153, decode.d0.loss_dice: 0.8062, decode.d1.loss_cls: 0.3442, decode.d1.loss_mask: 0.5090, decode.d1.loss_dice: 0.7715, decode.d2.loss_cls: 0.3009, decode.d2.loss_mask: 0.4999, decode.d2.loss_dice: 0.7474, decode.d3.loss_cls: 0.2815, decode.d3.loss_mask: 0.4958, decode.d3.loss_dice: 0.7401, decode.d4.loss_cls: 0.2773, decode.d4.loss_mask: 0.4938, decode.d4.loss_dice: 0.7385, decode.d5.loss_cls: 0.2726, decode.d5.loss_mask: 0.4942, decode.d5.loss_dice: 0.7374, decode.d6.loss_cls: 0.2715, decode.d6.loss_mask: 0.4921, decode.d6.loss_dice: 0.7349, decode.d7.loss_cls: 0.2701, decode.d7.loss_mask: 0.4930, decode.d7.loss_dice: 0.7369, loss: 15.1994
2022-10-29 18:20:00,898 - mmseg - INFO - Iter [10850/20000]	lr: 1.397e-06, eta: 6:04:52, time: 2.385, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2666, decode.loss_mask: 0.4891, decode.loss_dice: 0.7320, decode.d0.loss_cls: 1.6570, decode.d0.loss_mask: 0.5120, decode.d0.loss_dice: 0.7998, decode.d1.loss_cls: 0.3376, decode.d1.loss_mask: 0.5036, decode.d1.loss_dice: 0.7668, decode.d2.loss_cls: 0.2980, decode.d2.loss_mask: 0.4953, decode.d2.loss_dice: 0.7443, decode.d3.loss_cls: 0.2783, decode.d3.loss_mask: 0.4919, decode.d3.loss_dice: 0.7366, decode.d4.loss_cls: 0.2715, decode.d4.loss_mask: 0.4904, decode.d4.loss_dice: 0.7327, decode.d5.loss_cls: 0.2701, decode.d5.loss_mask: 0.4899, decode.d5.loss_dice: 0.7355, decode.d6.loss_cls: 0.2664, decode.d6.loss_mask: 0.4893, decode.d6.loss_dice: 0.7335, decode.d7.loss_cls: 0.2679, decode.d7.loss_mask: 0.4897, decode.d7.loss_dice: 0.7320, loss: 15.0777
2022-10-29 18:22:01,234 - mmseg - INFO - Iter [10900/20000]	lr: 1.389e-06, eta: 6:02:53, time: 2.407, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2704, decode.loss_mask: 0.5004, decode.loss_dice: 0.7432, decode.d0.loss_cls: 1.6408, decode.d0.loss_mask: 0.5205, decode.d0.loss_dice: 0.8138, decode.d1.loss_cls: 0.3374, decode.d1.loss_mask: 0.5162, decode.d1.loss_dice: 0.7826, decode.d2.loss_cls: 0.2966, decode.d2.loss_mask: 0.5064, decode.d2.loss_dice: 0.7570, decode.d3.loss_cls: 0.2816, decode.d3.loss_mask: 0.5018, decode.d3.loss_dice: 0.7470, decode.d4.loss_cls: 0.2760, decode.d4.loss_mask: 0.5014, decode.d4.loss_dice: 0.7456, decode.d5.loss_cls: 0.2720, decode.d5.loss_mask: 0.5000, decode.d5.loss_dice: 0.7448, decode.d6.loss_cls: 0.2689, decode.d6.loss_mask: 0.5001, decode.d6.loss_dice: 0.7421, decode.d7.loss_cls: 0.2700, decode.d7.loss_mask: 0.4999, decode.d7.loss_dice: 0.7423, loss: 15.2791
2022-10-29 18:24:00,162 - mmseg - INFO - Iter [10950/20000]	lr: 1.381e-06, eta: 6:00:53, time: 2.379, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2690, decode.loss_mask: 0.4905, decode.loss_dice: 0.7340, decode.d0.loss_cls: 1.6534, decode.d0.loss_mask: 0.5128, decode.d0.loss_dice: 0.8036, decode.d1.loss_cls: 0.3392, decode.d1.loss_mask: 0.5068, decode.d1.loss_dice: 0.7755, decode.d2.loss_cls: 0.3026, decode.d2.loss_mask: 0.4954, decode.d2.loss_dice: 0.7486, decode.d3.loss_cls: 0.2841, decode.d3.loss_mask: 0.4930, decode.d3.loss_dice: 0.7399, decode.d4.loss_cls: 0.2778, decode.d4.loss_mask: 0.4927, decode.d4.loss_dice: 0.7401, decode.d5.loss_cls: 0.2720, decode.d5.loss_mask: 0.4907, decode.d5.loss_dice: 0.7378, decode.d6.loss_cls: 0.2705, decode.d6.loss_mask: 0.4895, decode.d6.loss_dice: 0.7358, decode.d7.loss_cls: 0.2711, decode.d7.loss_mask: 0.4894, decode.d7.loss_dice: 0.7368, loss: 15.1528
2022-10-29 18:26:02,473 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-29 18:26:02,474 - mmseg - INFO - Iter [11000/20000]	lr: 1.374e-06, eta: 5:58:55, time: 2.446, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2802, decode.loss_mask: 0.4917, decode.loss_dice: 0.7397, decode.d0.loss_cls: 1.6851, decode.d0.loss_mask: 0.5175, decode.d0.loss_dice: 0.8118, decode.d1.loss_cls: 0.3553, decode.d1.loss_mask: 0.5064, decode.d1.loss_dice: 0.7759, decode.d2.loss_cls: 0.3100, decode.d2.loss_mask: 0.4990, decode.d2.loss_dice: 0.7527, decode.d3.loss_cls: 0.2912, decode.d3.loss_mask: 0.4935, decode.d3.loss_dice: 0.7432, decode.d4.loss_cls: 0.2874, decode.d4.loss_mask: 0.4923, decode.d4.loss_dice: 0.7425, decode.d5.loss_cls: 0.2815, decode.d5.loss_mask: 0.4925, decode.d5.loss_dice: 0.7412, decode.d6.loss_cls: 0.2785, decode.d6.loss_mask: 0.4919, decode.d6.loss_dice: 0.7392, decode.d7.loss_cls: 0.2819, decode.d7.loss_mask: 0.4913, decode.d7.loss_dice: 0.7398, loss: 15.3129
2022-10-29 18:27:59,763 - mmseg - INFO - Iter [11050/20000]	lr: 1.366e-06, eta: 5:56:54, time: 2.346, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2800, decode.loss_mask: 0.4999, decode.loss_dice: 0.7539, decode.d0.loss_cls: 1.6653, decode.d0.loss_mask: 0.5236, decode.d0.loss_dice: 0.8221, decode.d1.loss_cls: 0.3540, decode.d1.loss_mask: 0.5154, decode.d1.loss_dice: 0.7922, decode.d2.loss_cls: 0.3090, decode.d2.loss_mask: 0.5054, decode.d2.loss_dice: 0.7697, decode.d3.loss_cls: 0.2905, decode.d3.loss_mask: 0.5014, decode.d3.loss_dice: 0.7576, decode.d4.loss_cls: 0.2865, decode.d4.loss_mask: 0.5004, decode.d4.loss_dice: 0.7564, decode.d5.loss_cls: 0.2827, decode.d5.loss_mask: 0.5000, decode.d5.loss_dice: 0.7578, decode.d6.loss_cls: 0.2824, decode.d6.loss_mask: 0.4997, decode.d6.loss_dice: 0.7514, decode.d7.loss_cls: 0.2810, decode.d7.loss_mask: 0.5004, decode.d7.loss_dice: 0.7544, loss: 15.4931
2022-10-29 18:29:59,225 - mmseg - INFO - Iter [11100/20000]	lr: 1.358e-06, eta: 5:54:54, time: 2.389, data_time: 0.081, memory: 35422, decode.loss_cls: 0.2698, decode.loss_mask: 0.4928, decode.loss_dice: 0.7339, decode.d0.loss_cls: 1.6563, decode.d0.loss_mask: 0.5162, decode.d0.loss_dice: 0.8081, decode.d1.loss_cls: 0.3434, decode.d1.loss_mask: 0.5079, decode.d1.loss_dice: 0.7722, decode.d2.loss_cls: 0.3056, decode.d2.loss_mask: 0.4987, decode.d2.loss_dice: 0.7482, decode.d3.loss_cls: 0.2849, decode.d3.loss_mask: 0.4947, decode.d3.loss_dice: 0.7399, decode.d4.loss_cls: 0.2775, decode.d4.loss_mask: 0.4939, decode.d4.loss_dice: 0.7390, decode.d5.loss_cls: 0.2764, decode.d5.loss_mask: 0.4930, decode.d5.loss_dice: 0.7363, decode.d6.loss_cls: 0.2721, decode.d6.loss_mask: 0.4908, decode.d6.loss_dice: 0.7346, decode.d7.loss_cls: 0.2718, decode.d7.loss_mask: 0.4918, decode.d7.loss_dice: 0.7349, loss: 15.1847
2022-10-29 18:31:58,839 - mmseg - INFO - Iter [11150/20000]	lr: 1.351e-06, eta: 5:52:54, time: 2.392, data_time: 0.012, memory: 35422, decode.loss_cls: 0.2779, decode.loss_mask: 0.4993, decode.loss_dice: 0.7438, decode.d0.loss_cls: 1.6551, decode.d0.loss_mask: 0.5250, decode.d0.loss_dice: 0.8179, decode.d1.loss_cls: 0.3508, decode.d1.loss_mask: 0.5153, decode.d1.loss_dice: 0.7820, decode.d2.loss_cls: 0.3048, decode.d2.loss_mask: 0.5068, decode.d2.loss_dice: 0.7598, decode.d3.loss_cls: 0.2907, decode.d3.loss_mask: 0.5023, decode.d3.loss_dice: 0.7504, decode.d4.loss_cls: 0.2817, decode.d4.loss_mask: 0.5023, decode.d4.loss_dice: 0.7466, decode.d5.loss_cls: 0.2800, decode.d5.loss_mask: 0.5009, decode.d5.loss_dice: 0.7461, decode.d6.loss_cls: 0.2783, decode.d6.loss_mask: 0.4991, decode.d6.loss_dice: 0.7434, decode.d7.loss_cls: 0.2779, decode.d7.loss_mask: 0.4998, decode.d7.loss_dice: 0.7422, loss: 15.3803
2022-10-29 18:34:01,367 - mmseg - INFO - Iter [11200/20000]	lr: 1.343e-06, eta: 5:50:57, time: 2.451, data_time: 0.012, memory: 35422, decode.loss_cls: 0.2696, decode.loss_mask: 0.4992, decode.loss_dice: 0.7474, decode.d0.loss_cls: 1.6535, decode.d0.loss_mask: 0.5211, decode.d0.loss_dice: 0.8182, decode.d1.loss_cls: 0.3405, decode.d1.loss_mask: 0.5143, decode.d1.loss_dice: 0.7806, decode.d2.loss_cls: 0.2999, decode.d2.loss_mask: 0.5058, decode.d2.loss_dice: 0.7622, decode.d3.loss_cls: 0.2811, decode.d3.loss_mask: 0.5001, decode.d3.loss_dice: 0.7510, decode.d4.loss_cls: 0.2783, decode.d4.loss_mask: 0.4994, decode.d4.loss_dice: 0.7508, decode.d5.loss_cls: 0.2713, decode.d5.loss_mask: 0.4987, decode.d5.loss_dice: 0.7508, decode.d6.loss_cls: 0.2710, decode.d6.loss_mask: 0.4986, decode.d6.loss_dice: 0.7473, decode.d7.loss_cls: 0.2714, decode.d7.loss_mask: 0.4982, decode.d7.loss_dice: 0.7491, loss: 15.3295
2022-10-29 18:36:01,515 - mmseg - INFO - Iter [11250/20000]	lr: 1.335e-06, eta: 5:48:58, time: 2.403, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2632, decode.loss_mask: 0.4959, decode.loss_dice: 0.7288, decode.d0.loss_cls: 1.6426, decode.d0.loss_mask: 0.5183, decode.d0.loss_dice: 0.7919, decode.d1.loss_cls: 0.3284, decode.d1.loss_mask: 0.5141, decode.d1.loss_dice: 0.7628, decode.d2.loss_cls: 0.2898, decode.d2.loss_mask: 0.5018, decode.d2.loss_dice: 0.7437, decode.d3.loss_cls: 0.2760, decode.d3.loss_mask: 0.4971, decode.d3.loss_dice: 0.7342, decode.d4.loss_cls: 0.2678, decode.d4.loss_mask: 0.4966, decode.d4.loss_dice: 0.7310, decode.d5.loss_cls: 0.2663, decode.d5.loss_mask: 0.4956, decode.d5.loss_dice: 0.7304, decode.d6.loss_cls: 0.2635, decode.d6.loss_mask: 0.4949, decode.d6.loss_dice: 0.7292, decode.d7.loss_cls: 0.2638, decode.d7.loss_mask: 0.4955, decode.d7.loss_dice: 0.7299, loss: 15.0530
2022-10-29 18:38:00,865 - mmseg - INFO - Iter [11300/20000]	lr: 1.328e-06, eta: 5:46:58, time: 2.387, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2737, decode.loss_mask: 0.4899, decode.loss_dice: 0.7436, decode.d0.loss_cls: 1.6423, decode.d0.loss_mask: 0.5144, decode.d0.loss_dice: 0.8178, decode.d1.loss_cls: 0.3402, decode.d1.loss_mask: 0.5049, decode.d1.loss_dice: 0.7840, decode.d2.loss_cls: 0.3002, decode.d2.loss_mask: 0.4946, decode.d2.loss_dice: 0.7574, decode.d3.loss_cls: 0.2858, decode.d3.loss_mask: 0.4914, decode.d3.loss_dice: 0.7467, decode.d4.loss_cls: 0.2796, decode.d4.loss_mask: 0.4913, decode.d4.loss_dice: 0.7468, decode.d5.loss_cls: 0.2728, decode.d5.loss_mask: 0.4907, decode.d5.loss_dice: 0.7477, decode.d6.loss_cls: 0.2732, decode.d6.loss_mask: 0.4879, decode.d6.loss_dice: 0.7440, decode.d7.loss_cls: 0.2733, decode.d7.loss_mask: 0.4889, decode.d7.loss_dice: 0.7432, loss: 15.2263
2022-10-29 18:40:05,787 - mmseg - INFO - Iter [11350/20000]	lr: 1.320e-06, eta: 5:45:02, time: 2.498, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2580, decode.loss_mask: 0.4877, decode.loss_dice: 0.7275, decode.d0.loss_cls: 1.6449, decode.d0.loss_mask: 0.5111, decode.d0.loss_dice: 0.7945, decode.d1.loss_cls: 0.3314, decode.d1.loss_mask: 0.5020, decode.d1.loss_dice: 0.7627, decode.d2.loss_cls: 0.2889, decode.d2.loss_mask: 0.4946, decode.d2.loss_dice: 0.7408, decode.d3.loss_cls: 0.2717, decode.d3.loss_mask: 0.4897, decode.d3.loss_dice: 0.7300, decode.d4.loss_cls: 0.2669, decode.d4.loss_mask: 0.4901, decode.d4.loss_dice: 0.7298, decode.d5.loss_cls: 0.2638, decode.d5.loss_mask: 0.4879, decode.d5.loss_dice: 0.7287, decode.d6.loss_cls: 0.2590, decode.d6.loss_mask: 0.4879, decode.d6.loss_dice: 0.7260, decode.d7.loss_cls: 0.2593, decode.d7.loss_mask: 0.4877, decode.d7.loss_dice: 0.7271, loss: 14.9497
2022-10-29 18:42:11,797 - mmseg - INFO - Iter [11400/20000]	lr: 1.313e-06, eta: 5:43:07, time: 2.520, data_time: 0.060, memory: 35422, decode.loss_cls: 0.2628, decode.loss_mask: 0.4861, decode.loss_dice: 0.7228, decode.d0.loss_cls: 1.6561, decode.d0.loss_mask: 0.5072, decode.d0.loss_dice: 0.7914, decode.d1.loss_cls: 0.3315, decode.d1.loss_mask: 0.5013, decode.d1.loss_dice: 0.7622, decode.d2.loss_cls: 0.2945, decode.d2.loss_mask: 0.4940, decode.d2.loss_dice: 0.7374, decode.d3.loss_cls: 0.2753, decode.d3.loss_mask: 0.4877, decode.d3.loss_dice: 0.7240, decode.d4.loss_cls: 0.2702, decode.d4.loss_mask: 0.4869, decode.d4.loss_dice: 0.7237, decode.d5.loss_cls: 0.2687, decode.d5.loss_mask: 0.4850, decode.d5.loss_dice: 0.7218, decode.d6.loss_cls: 0.2640, decode.d6.loss_mask: 0.4845, decode.d6.loss_dice: 0.7211, decode.d7.loss_cls: 0.2624, decode.d7.loss_mask: 0.4861, decode.d7.loss_dice: 0.7229, loss: 14.9318
2022-10-29 18:44:10,489 - mmseg - INFO - Iter [11450/20000]	lr: 1.305e-06, eta: 5:41:07, time: 2.374, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2624, decode.loss_mask: 0.4874, decode.loss_dice: 0.7255, decode.d0.loss_cls: 1.6148, decode.d0.loss_mask: 0.5108, decode.d0.loss_dice: 0.7985, decode.d1.loss_cls: 0.3306, decode.d1.loss_mask: 0.5034, decode.d1.loss_dice: 0.7619, decode.d2.loss_cls: 0.2925, decode.d2.loss_mask: 0.4928, decode.d2.loss_dice: 0.7408, decode.d3.loss_cls: 0.2730, decode.d3.loss_mask: 0.4888, decode.d3.loss_dice: 0.7304, decode.d4.loss_cls: 0.2698, decode.d4.loss_mask: 0.4881, decode.d4.loss_dice: 0.7304, decode.d5.loss_cls: 0.2668, decode.d5.loss_mask: 0.4866, decode.d5.loss_dice: 0.7256, decode.d6.loss_cls: 0.2630, decode.d6.loss_mask: 0.4854, decode.d6.loss_dice: 0.7261, decode.d7.loss_cls: 0.2662, decode.d7.loss_mask: 0.4861, decode.d7.loss_dice: 0.7257, loss: 14.9333
2022-10-29 18:46:08,395 - mmseg - INFO - Iter [11500/20000]	lr: 1.297e-06, eta: 5:39:06, time: 2.358, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2551, decode.loss_mask: 0.4840, decode.loss_dice: 0.7255, decode.d0.loss_cls: 1.6388, decode.d0.loss_mask: 0.5075, decode.d0.loss_dice: 0.7886, decode.d1.loss_cls: 0.3290, decode.d1.loss_mask: 0.4989, decode.d1.loss_dice: 0.7594, decode.d2.loss_cls: 0.2852, decode.d2.loss_mask: 0.4896, decode.d2.loss_dice: 0.7418, decode.d3.loss_cls: 0.2682, decode.d3.loss_mask: 0.4858, decode.d3.loss_dice: 0.7298, decode.d4.loss_cls: 0.2606, decode.d4.loss_mask: 0.4861, decode.d4.loss_dice: 0.7300, decode.d5.loss_cls: 0.2567, decode.d5.loss_mask: 0.4845, decode.d5.loss_dice: 0.7285, decode.d6.loss_cls: 0.2534, decode.d6.loss_mask: 0.4837, decode.d6.loss_dice: 0.7243, decode.d7.loss_cls: 0.2536, decode.d7.loss_mask: 0.4839, decode.d7.loss_dice: 0.7250, loss: 14.8573
2022-10-29 18:48:02,269 - mmseg - INFO - Iter [11550/20000]	lr: 1.290e-06, eta: 5:37:02, time: 2.277, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2610, decode.loss_mask: 0.4804, decode.loss_dice: 0.7220, decode.d0.loss_cls: 1.6404, decode.d0.loss_mask: 0.5023, decode.d0.loss_dice: 0.7941, decode.d1.loss_cls: 0.3351, decode.d1.loss_mask: 0.4963, decode.d1.loss_dice: 0.7598, decode.d2.loss_cls: 0.2934, decode.d2.loss_mask: 0.4859, decode.d2.loss_dice: 0.7376, decode.d3.loss_cls: 0.2738, decode.d3.loss_mask: 0.4825, decode.d3.loss_dice: 0.7282, decode.d4.loss_cls: 0.2674, decode.d4.loss_mask: 0.4807, decode.d4.loss_dice: 0.7267, decode.d5.loss_cls: 0.2642, decode.d5.loss_mask: 0.4804, decode.d5.loss_dice: 0.7240, decode.d6.loss_cls: 0.2607, decode.d6.loss_mask: 0.4793, decode.d6.loss_dice: 0.7210, decode.d7.loss_cls: 0.2606, decode.d7.loss_mask: 0.4792, decode.d7.loss_dice: 0.7240, loss: 14.8608
2022-10-29 18:49:58,734 - mmseg - INFO - Iter [11600/20000]	lr: 1.282e-06, eta: 5:35:00, time: 2.329, data_time: 0.012, memory: 35422, decode.loss_cls: 0.2755, decode.loss_mask: 0.4886, decode.loss_dice: 0.7398, decode.d0.loss_cls: 1.6515, decode.d0.loss_mask: 0.5153, decode.d0.loss_dice: 0.8147, decode.d1.loss_cls: 0.3493, decode.d1.loss_mask: 0.5058, decode.d1.loss_dice: 0.7763, decode.d2.loss_cls: 0.3066, decode.d2.loss_mask: 0.4961, decode.d2.loss_dice: 0.7546, decode.d3.loss_cls: 0.2887, decode.d3.loss_mask: 0.4917, decode.d3.loss_dice: 0.7459, decode.d4.loss_cls: 0.2819, decode.d4.loss_mask: 0.4895, decode.d4.loss_dice: 0.7443, decode.d5.loss_cls: 0.2778, decode.d5.loss_mask: 0.4877, decode.d5.loss_dice: 0.7416, decode.d6.loss_cls: 0.2726, decode.d6.loss_mask: 0.4882, decode.d6.loss_dice: 0.7397, decode.d7.loss_cls: 0.2745, decode.d7.loss_mask: 0.4888, decode.d7.loss_dice: 0.7393, loss: 15.2265
2022-10-29 18:52:02,503 - mmseg - INFO - Iter [11650/20000]	lr: 1.274e-06, eta: 5:33:03, time: 2.475, data_time: 0.015, memory: 35422, decode.loss_cls: 0.2666, decode.loss_mask: 0.4904, decode.loss_dice: 0.7282, decode.d0.loss_cls: 1.6097, decode.d0.loss_mask: 0.5129, decode.d0.loss_dice: 0.8022, decode.d1.loss_cls: 0.3379, decode.d1.loss_mask: 0.5065, decode.d1.loss_dice: 0.7693, decode.d2.loss_cls: 0.2940, decode.d2.loss_mask: 0.4972, decode.d2.loss_dice: 0.7465, decode.d3.loss_cls: 0.2754, decode.d3.loss_mask: 0.4925, decode.d3.loss_dice: 0.7358, decode.d4.loss_cls: 0.2739, decode.d4.loss_mask: 0.4904, decode.d4.loss_dice: 0.7337, decode.d5.loss_cls: 0.2683, decode.d5.loss_mask: 0.4909, decode.d5.loss_dice: 0.7300, decode.d6.loss_cls: 0.2680, decode.d6.loss_mask: 0.4890, decode.d6.loss_dice: 0.7292, decode.d7.loss_cls: 0.2667, decode.d7.loss_mask: 0.4892, decode.d7.loss_dice: 0.7275, loss: 15.0221
2022-10-29 18:54:03,832 - mmseg - INFO - Iter [11700/20000]	lr: 1.267e-06, eta: 5:31:05, time: 2.427, data_time: 0.065, memory: 35422, decode.loss_cls: 0.2702, decode.loss_mask: 0.4930, decode.loss_dice: 0.7418, decode.d0.loss_cls: 1.6385, decode.d0.loss_mask: 0.5156, decode.d0.loss_dice: 0.8128, decode.d1.loss_cls: 0.3406, decode.d1.loss_mask: 0.5085, decode.d1.loss_dice: 0.7796, decode.d2.loss_cls: 0.2987, decode.d2.loss_mask: 0.5009, decode.d2.loss_dice: 0.7549, decode.d3.loss_cls: 0.2842, decode.d3.loss_mask: 0.4952, decode.d3.loss_dice: 0.7448, decode.d4.loss_cls: 0.2783, decode.d4.loss_mask: 0.4943, decode.d4.loss_dice: 0.7425, decode.d5.loss_cls: 0.2749, decode.d5.loss_mask: 0.4925, decode.d5.loss_dice: 0.7380, decode.d6.loss_cls: 0.2704, decode.d6.loss_mask: 0.4928, decode.d6.loss_dice: 0.7380, decode.d7.loss_cls: 0.2697, decode.d7.loss_mask: 0.4924, decode.d7.loss_dice: 0.7387, loss: 15.2019
2022-10-29 18:55:59,102 - mmseg - INFO - Iter [11750/20000]	lr: 1.259e-06, eta: 5:29:02, time: 2.305, data_time: 0.013, memory: 35422, decode.loss_cls: 0.2597, decode.loss_mask: 0.4931, decode.loss_dice: 0.7266, decode.d0.loss_cls: 1.6123, decode.d0.loss_mask: 0.5141, decode.d0.loss_dice: 0.7956, decode.d1.loss_cls: 0.3273, decode.d1.loss_mask: 0.5079, decode.d1.loss_dice: 0.7635, decode.d2.loss_cls: 0.2890, decode.d2.loss_mask: 0.4964, decode.d2.loss_dice: 0.7418, decode.d3.loss_cls: 0.2729, decode.d3.loss_mask: 0.4960, decode.d3.loss_dice: 0.7329, decode.d4.loss_cls: 0.2674, decode.d4.loss_mask: 0.4940, decode.d4.loss_dice: 0.7319, decode.d5.loss_cls: 0.2627, decode.d5.loss_mask: 0.4940, decode.d5.loss_dice: 0.7314, decode.d6.loss_cls: 0.2580, decode.d6.loss_mask: 0.4945, decode.d6.loss_dice: 0.7276, decode.d7.loss_cls: 0.2604, decode.d7.loss_mask: 0.4927, decode.d7.loss_dice: 0.7288, loss: 14.9724

2022-10-30 20:38:46,692 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.8 (default, Feb 24 2021, 21:46:12) [GCC 7.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-40GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.2, V11.2.142
GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
PyTorch: 1.9.0a0+df837d0
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2019.0.4 Product Build 20190411 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash N/A)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.2
  - NVCC architecture flags: -gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_86,code=compute_86
  - CuDNN 8.1.1
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.2, CUDNN_VERSION=8.1.1, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, FORCE_FALLBACK_CUDA_MPI=1, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=ON, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.0a0
OpenCV: 4.5.5
MMCV: 1.6.2
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.2
MMSegmentation: 0.20.2+dd8b542
------------------------------------------------------------

2022-10-30 20:38:46,693 - mmseg - INFO - Distributed training: True
2022-10-30 20:38:47,583 - mmseg - INFO - Config:
num_things_classes = 100
num_stuff_classes = 50
num_classes = 150
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoderMask2FormerAug',
    pretrained=
    '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/checkpoint-149/mp_rank_00_model_states_renamed-s14tos16.pt',
    backbone=dict(
        type='BEiTAdapter',
        patch_size=16,
        embed_dim=1408,
        depth=40,
        num_heads=16,
        mlp_ratio=4.363636363636363,
        qkv_bias=True,
        use_abs_pos_emb=True,
        use_rel_pos_bias=False,
        img_size=896,
        init_values=None,
        drop_path_rate=0.5,
        conv_inplane=64,
        n_points=4,
        deform_num_heads=16,
        cffn_ratio=0.25,
        deform_ratio=0.5,
        with_cp=True,
        interaction_indexes=[[0, 9], [10, 19], [20, 29], [30, 39]]),
    decode_head=dict(
        type='Mask2FormerHead',
        in_channels=[1408, 1408, 1408, 1408],
        feat_channels=1024,
        out_channels=1024,
        in_index=[0, 1, 2, 3],
        num_things_classes=100,
        num_stuff_classes=50,
        num_queries=200,
        num_transformer_feat_level=3,
        pixel_decoder=dict(
            type='MSDeformAttnPixelDecoder',
            num_outs=3,
            norm_cfg=dict(type='GN', num_groups=32),
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=1024,
                        num_heads=32,
                        num_levels=3,
                        num_points=4,
                        im2col_step=64,
                        dropout=0.0,
                        batch_first=False,
                        norm_cfg=None,
                        init_cfg=None),
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=1024,
                        feedforward_channels=4096,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True),
                        with_cp=True),
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')),
                init_cfg=None),
            positional_encoding=dict(
                type='SinePositionalEncoding', num_feats=512, normalize=True),
            init_cfg=None),
        enforce_decoder_input_project=False,
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=512, normalize=True),
        transformer_decoder=dict(
            type='DetrTransformerDecoder',
            return_intermediate=True,
            num_layers=8,
            transformerlayers=dict(
                type='DetrTransformerDecoderLayer',
                attn_cfgs=dict(
                    type='MultiheadAttention',
                    embed_dims=1024,
                    num_heads=32,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=False),
                ffn_cfgs=dict(
                    embed_dims=1024,
                    feedforward_channels=4096,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.0,
                    dropout_layer=None,
                    add_identity=True,
                    with_cp=True),
                feedforward_channels=4096,
                operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                 'ffn', 'norm')),
            init_cfg=None),
        loss_cls=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=2.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1
            ]),
        loss_mask=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            reduction='mean',
            loss_weight=5.0),
        loss_dice=dict(
            type='DiceLoss',
            use_sigmoid=True,
            activate=True,
            reduction='mean',
            naive_dice=True,
            eps=1.0,
            loss_weight=5.0)),
    train_cfg=dict(
        num_points=12544,
        oversample_ratio=3.0,
        importance_sample_ratio=0.75,
        assigner=dict(
            type='MaskHungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=2.0),
            mask_cost=dict(
                type='CrossEntropyLossCost', weight=5.0, use_sigmoid=True),
            dice_cost=dict(
                type='DiceCost', weight=5.0, pred_act=True, eps=1.0)),
        sampler=dict(type='MaskPseudoSampler')),
    test_cfg=dict(
        panoptic_on=True,
        semantic_on=False,
        instance_on=True,
        max_per_image=100,
        iou_thr=0.8,
        filter_low_score=True,
        mode='slide',
        crop_size=(896, 896),
        stride=(512, 512)),
    init_cfg=None)
dataset_type = 'ADE20KDataset'
data_root = '/sharefs/yxf/data/seg/ade/ADEChallengeData2016'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (896, 896)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', reduce_zero_label=True),
    dict(type='Resize', img_scale=(3584, 896), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
    dict(type='ToMask'),
    dict(type='DefaultFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(3584, 896),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='ResizeToMultiple', size_divisor=32),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=4,
    train=dict(
        type='ADE20KDataset',
        data_root='/sharefs/yxf/data/seg/ade/ADEChallengeData2016',
        img_dir='images/training',
        ann_dir='annotations/training',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', reduce_zero_label=True),
            dict(type='Resize', img_scale=(3584, 896), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
            dict(type='ToMask'),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
        ]),
    val=dict(
        type='ADE20KDataset',
        data_root='/sharefs/yxf/data/seg/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(3584, 896),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='ResizeToMultiple', size_divisor=32),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='ADE20KDataset',
        data_root='/sharefs/yxf/data/seg/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(3584, 896),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='ResizeToMultiple', size_divisor=32),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_80k_cocostuff164k_ss/lr1e-5_lrd0.95_enc6_dec8/iter_60000.pth'
resume_from = '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95/iter_10000.pth'
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=2.5e-05,
    betas=(0.9, 0.999),
    weight_decay=0.05,
    constructor='LayerDecayOptimizerConstructor',
    paramwise_cfg=dict(num_layers=40, layer_decay_rate=0.95))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=20000)
checkpoint_config = dict(by_epoch=False, interval=2000, max_keep_ckpts=6)
evaluation = dict(
    interval=2000, metric='mIoU', pre_eval=True, save_best='mIoU')
pretrained = '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/checkpoint-149/mp_rank_00_model_states_renamed-s14tos16.pt'
work_dir = '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95'
gpu_ids = range(0, 64)
auto_resume = False

2022-10-30 20:39:15,070 - mmseg - INFO - Set random seed to 708285200, deterministic: False
2022-10-30 20:39:34,963 - mmseg - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: mask_token, norm.weight, norm.bias, lm_head.weight, lm_head.bias

Name of parameter - Initialization information

backbone.cls_token - torch.Size([1, 1, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.pos_embed - torch.Size([1, 3137, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.level_embed - torch.Size([3, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.patch_embed.proj.weight - torch.Size([1408, 3, 16, 16]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.patch_embed.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.0.weight - torch.Size([64, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.6.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.7.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.7.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv2.0.weight - torch.Size([128, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv2.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv2.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv3.0.weight - torch.Size([256, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv4.0.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc1.weight - torch.Size([1408, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc2.weight - torch.Size([1408, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc3.weight - torch.Size([1408, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc3.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc4.weight - torch.Size([1408, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc4.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.up.weight - torch.Size([1408, 1408, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.up.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm3.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm3.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm4.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm4.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.level_encoding.weight - torch.Size([3, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([1024, 1024, 3, 3]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.mask_feature.weight - torch.Size([1024, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.mask_feature.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.post_norm.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.post_norm.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.query_embed.weight - torch.Size([200, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.query_feat.weight - torch.Size([200, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.level_embed.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.cls_embed.weight - torch.Size([151, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.cls_embed.bias - torch.Size([151]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.0.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.2.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.4.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.4.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  
2022-10-30 20:39:38,488 - mmseg - INFO - EncoderDecoderMask2FormerAug(
  (backbone): BEiTAdapter(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1408, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.012820512987673283)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.025641025975346565)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.03846153989434242)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.05128205195069313)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.06410256773233414)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.07692307978868484)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.08974359184503555)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.10256410390138626)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.11538461595773697)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.12820513546466827)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.14102564752101898)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1538461595773697)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1666666716337204)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1794871836900711)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.19230769574642181)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (16): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.20512820780277252)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (17): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.21794871985912323)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (18): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.23076923191547394)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (19): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.24358974397182465)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (20): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.25641027092933655)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (21): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.26923078298568726)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (22): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.28205129504203796)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (23): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.29487180709838867)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (24): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3076923191547394)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (25): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3205128312110901)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (26): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3333333432674408)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (27): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3461538553237915)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (28): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3589743673801422)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (29): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3717948794364929)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (30): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.38461539149284363)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (31): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.39743590354919434)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (32): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.41025641560554504)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (33): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.42307692766189575)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (34): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.43589743971824646)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (35): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.44871795177459717)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (36): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.4615384638309479)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (37): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.4743589758872986)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (38): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.4871794879436493)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (39): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.5)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (spm): SpatialPriorModule(
      (stem): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
        (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      )
      (conv2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv4): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (fc1): Conv2d(64, 1408, kernel_size=(1, 1), stride=(1, 1))
      (fc2): Conv2d(128, 1408, kernel_size=(1, 1), stride=(1, 1))
      (fc3): Conv2d(256, 1408, kernel_size=(1, 1), stride=(1, 1))
      (fc4): Conv2d(256, 1408, kernel_size=(1, 1), stride=(1, 1))
    )
    (interactions): Sequential(
      (0): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (1): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (2): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (3): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (extra_extractors): Sequential(
          (0): Extractor(
            (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
              (value_proj): Linear(in_features=1408, out_features=704, bias=True)
              (output_proj): Linear(in_features=704, out_features=1408, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1408, out_features=352, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
              )
              (act): GELU()
              (fc2): Linear(in_features=352, out_features=1408, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
          (1): Extractor(
            (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
              (value_proj): Linear(in_features=1408, out_features=704, bias=True)
              (output_proj): Linear(in_features=704, out_features=1408, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1408, out_features=352, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
              )
              (act): GELU()
              (fc2): Linear(in_features=352, out_features=1408, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
        )
      )
    )
    (up): ConvTranspose2d(1408, 1408, kernel_size=(2, 2), stride=(2, 2))
    (norm1): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm2): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm3): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm4): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (decode_head): Mask2FormerHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (conv_seg): None
    (dropout): Dropout2d(p=0.1, inplace=False)
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (1): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (2): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (postional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
      (level_encoding): Embedding(3, 1024)
      (lateral_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (output_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
          (activate): ReLU(inplace=True)
        )
      )
      (mask_feature): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    )
    (transformer_decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (6): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (7): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (decoder_input_projs): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
    )
    (decoder_positional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (query_embed): Embedding(200, 1024)
    (query_feat): Embedding(200, 1024)
    (level_embed): Embedding(3, 1024)
    (cls_embed): Linear(in_features=1024, out_features=151, bias=True)
    (mask_embed): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (loss_cls): CrossEntropyLoss(avg_non_ignore=False)
    (loss_mask): CrossEntropyLoss(avg_non_ignore=False)
    (loss_dice): DiceLoss()
  )
)
2022-10-30 20:39:39,383 - mmseg - INFO - Loaded 20210 images
2022-10-30 20:39:45,110 - mmseg - INFO - load checkpoint from local path: /sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95/iter_10000.pth
2022-10-30 20:40:15,941 - mmseg - INFO - resumed from epoch: 32, iter 9999
2022-10-30 20:40:15,945 - mmseg - INFO - Start running, host: fangyuxin@yxf-nsk5n-3317490-worker-0, work_dir: /sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95
2022-10-30 20:40:15,946 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2022-10-30 20:40:15,947 - mmseg - INFO - workflow: [('train', 1)], max: 20000 iters
2022-10-30 20:40:15,947 - mmseg - INFO - Checkpoints will be saved to /sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95 by HardDiskBackend.
2022-10-30 20:41:36,459 - mmseg - INFO - Saving checkpoint at 10000 iterations
2022-10-30 20:42:15,488 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-30 20:42:15,489 - mmseg - INFO - Iter [10000/20000]	lr: 1.526e-06, eta: 426 days, 16:09:36, time: 73.729, data_time: 0.147, memory: 35120, decode.loss_cls: 0.2213, decode.loss_mask: 0.4562, decode.loss_dice: 0.6660, decode.d0.loss_cls: 1.5659, decode.d0.loss_mask: 0.4845, decode.d0.loss_dice: 0.7169, decode.d1.loss_cls: 0.3111, decode.d1.loss_mask: 0.4636, decode.d1.loss_dice: 0.6784, decode.d2.loss_cls: 0.2549, decode.d2.loss_mask: 0.4566, decode.d2.loss_dice: 0.6765, decode.d3.loss_cls: 0.2388, decode.d3.loss_mask: 0.4462, decode.d3.loss_dice: 0.6674, decode.d4.loss_cls: 0.2331, decode.d4.loss_mask: 0.4443, decode.d4.loss_dice: 0.6639, decode.d5.loss_cls: 0.2206, decode.d5.loss_mask: 0.4499, decode.d5.loss_dice: 0.6729, decode.d6.loss_cls: 0.2307, decode.d6.loss_mask: 0.4506, decode.d6.loss_dice: 0.6565, decode.d7.loss_cls: 0.2319, decode.d7.loss_mask: 0.4525, decode.d7.loss_dice: 0.6636, loss: 13.6749
2022-10-30 20:44:17,194 - mmseg - INFO - Iter [10050/20000]	lr: 1.519e-06, eta: 8 days, 14:22:45, time: 2.434, data_time: 0.013, memory: 35579, decode.loss_cls: 0.2795, decode.loss_mask: 0.5012, decode.loss_dice: 0.7436, decode.d0.loss_cls: 1.7106, decode.d0.loss_mask: 0.5180, decode.d0.loss_dice: 0.8109, decode.d1.loss_cls: 0.3441, decode.d1.loss_mask: 0.5163, decode.d1.loss_dice: 0.7798, decode.d2.loss_cls: 0.3099, decode.d2.loss_mask: 0.5076, decode.d2.loss_dice: 0.7562, decode.d3.loss_cls: 0.2910, decode.d3.loss_mask: 0.5029, decode.d3.loss_dice: 0.7484, decode.d4.loss_cls: 0.2888, decode.d4.loss_mask: 0.5032, decode.d4.loss_dice: 0.7456, decode.d5.loss_cls: 0.2813, decode.d5.loss_mask: 0.5009, decode.d5.loss_dice: 0.7442, decode.d6.loss_cls: 0.2775, decode.d6.loss_mask: 0.5012, decode.d6.loss_dice: 0.7448, decode.d7.loss_cls: 0.2782, decode.d7.loss_mask: 0.5011, decode.d7.loss_dice: 0.7432, loss: 15.4300
2022-10-30 20:46:20,345 - mmseg - INFO - Iter [10100/20000]	lr: 1.511e-06, eta: 4 days, 11:02:26, time: 2.463, data_time: 0.013, memory: 35579, decode.loss_cls: 0.2671, decode.loss_mask: 0.4929, decode.loss_dice: 0.7281, decode.d0.loss_cls: 1.7152, decode.d0.loss_mask: 0.5073, decode.d0.loss_dice: 0.7917, decode.d1.loss_cls: 0.3365, decode.d1.loss_mask: 0.5068, decode.d1.loss_dice: 0.7655, decode.d2.loss_cls: 0.2965, decode.d2.loss_mask: 0.4983, decode.d2.loss_dice: 0.7403, decode.d3.loss_cls: 0.2797, decode.d3.loss_mask: 0.4947, decode.d3.loss_dice: 0.7342, decode.d4.loss_cls: 0.2740, decode.d4.loss_mask: 0.4932, decode.d4.loss_dice: 0.7339, decode.d5.loss_cls: 0.2708, decode.d5.loss_mask: 0.4919, decode.d5.loss_dice: 0.7306, decode.d6.loss_cls: 0.2677, decode.d6.loss_mask: 0.4911, decode.d6.loss_dice: 0.7278, decode.d7.loss_cls: 0.2678, decode.d7.loss_mask: 0.4919, decode.d7.loss_dice: 0.7293, loss: 15.1250
2022-10-30 20:48:22,363 - mmseg - INFO - Iter [10150/20000]	lr: 1.503e-06, eta: 3 days, 1:26:42, time: 2.439, data_time: 0.014, memory: 35597, decode.loss_cls: 0.2788, decode.loss_mask: 0.5049, decode.loss_dice: 0.7567, decode.d0.loss_cls: 1.7107, decode.d0.loss_mask: 0.5237, decode.d0.loss_dice: 0.8297, decode.d1.loss_cls: 0.3529, decode.d1.loss_mask: 0.5186, decode.d1.loss_dice: 0.7927, decode.d2.loss_cls: 0.3106, decode.d2.loss_mask: 0.5101, decode.d2.loss_dice: 0.7709, decode.d3.loss_cls: 0.2941, decode.d3.loss_mask: 0.5049, decode.d3.loss_dice: 0.7611, decode.d4.loss_cls: 0.2881, decode.d4.loss_mask: 0.5056, decode.d4.loss_dice: 0.7571, decode.d5.loss_cls: 0.2821, decode.d5.loss_mask: 0.5043, decode.d5.loss_dice: 0.7589, decode.d6.loss_cls: 0.2814, decode.d6.loss_mask: 0.5042, decode.d6.loss_dice: 0.7557, decode.d7.loss_cls: 0.2781, decode.d7.loss_mask: 0.5043, decode.d7.loss_dice: 0.7573, loss: 15.5978
2022-10-30 20:50:28,698 - mmseg - INFO - Iter [10200/20000]	lr: 1.496e-06, eta: 2 days, 8:36:24, time: 2.528, data_time: 0.014, memory: 35597, decode.loss_cls: 0.3053, decode.loss_mask: 0.5063, decode.loss_dice: 0.7489, decode.d0.loss_cls: 1.7117, decode.d0.loss_mask: 0.5274, decode.d0.loss_dice: 0.8257, decode.d1.loss_cls: 0.3791, decode.d1.loss_mask: 0.5225, decode.d1.loss_dice: 0.7876, decode.d2.loss_cls: 0.3379, decode.d2.loss_mask: 0.5127, decode.d2.loss_dice: 0.7657, decode.d3.loss_cls: 0.3183, decode.d3.loss_mask: 0.5089, decode.d3.loss_dice: 0.7557, decode.d4.loss_cls: 0.3127, decode.d4.loss_mask: 0.5064, decode.d4.loss_dice: 0.7554, decode.d5.loss_cls: 0.3067, decode.d5.loss_mask: 0.5070, decode.d5.loss_dice: 0.7523, decode.d6.loss_cls: 0.3055, decode.d6.loss_mask: 0.5061, decode.d6.loss_dice: 0.7501, decode.d7.loss_cls: 0.3034, decode.d7.loss_mask: 0.5065, decode.d7.loss_dice: 0.7508, loss: 15.7766
2022-10-30 20:52:33,079 - mmseg - INFO - Iter [10250/20000]	lr: 1.488e-06, eta: 1 day, 22:26:28, time: 2.488, data_time: 0.015, memory: 35615, decode.loss_cls: 0.3040, decode.loss_mask: 0.5081, decode.loss_dice: 0.7569, decode.d0.loss_cls: 1.7204, decode.d0.loss_mask: 0.5330, decode.d0.loss_dice: 0.8380, decode.d1.loss_cls: 0.3754, decode.d1.loss_mask: 0.5265, decode.d1.loss_dice: 0.8053, decode.d2.loss_cls: 0.3326, decode.d2.loss_mask: 0.5145, decode.d2.loss_dice: 0.7782, decode.d3.loss_cls: 0.3150, decode.d3.loss_mask: 0.5118, decode.d3.loss_dice: 0.7627, decode.d4.loss_cls: 0.3094, decode.d4.loss_mask: 0.5092, decode.d4.loss_dice: 0.7623, decode.d5.loss_cls: 0.3059, decode.d5.loss_mask: 0.5096, decode.d5.loss_dice: 0.7584, decode.d6.loss_cls: 0.3050, decode.d6.loss_mask: 0.5065, decode.d6.loss_dice: 0.7526, decode.d7.loss_cls: 0.3060, decode.d7.loss_mask: 0.5077, decode.d7.loss_dice: 0.7553, loss: 15.8703
2022-10-30 20:54:36,242 - mmseg - INFO - Iter [10300/20000]	lr: 1.480e-06, eta: 1 day, 15:37:50, time: 2.463, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2857, decode.loss_mask: 0.4998, decode.loss_dice: 0.7456, decode.d0.loss_cls: 1.6986, decode.d0.loss_mask: 0.5207, decode.d0.loss_dice: 0.8146, decode.d1.loss_cls: 0.3557, decode.d1.loss_mask: 0.5175, decode.d1.loss_dice: 0.7860, decode.d2.loss_cls: 0.3165, decode.d2.loss_mask: 0.5076, decode.d2.loss_dice: 0.7596, decode.d3.loss_cls: 0.2986, decode.d3.loss_mask: 0.5016, decode.d3.loss_dice: 0.7473, decode.d4.loss_cls: 0.2922, decode.d4.loss_mask: 0.5012, decode.d4.loss_dice: 0.7458, decode.d5.loss_cls: 0.2863, decode.d5.loss_mask: 0.4994, decode.d5.loss_dice: 0.7455, decode.d6.loss_cls: 0.2874, decode.d6.loss_mask: 0.4982, decode.d6.loss_dice: 0.7428, decode.d7.loss_cls: 0.2872, decode.d7.loss_mask: 0.4992, decode.d7.loss_dice: 0.7444, loss: 15.4851
2022-10-30 20:56:42,570 - mmseg - INFO - Iter [10350/20000]	lr: 1.473e-06, eta: 1 day, 10:46:29, time: 2.526, data_time: 0.063, memory: 35615, decode.loss_cls: 0.2703, decode.loss_mask: 0.4999, decode.loss_dice: 0.7399, decode.d0.loss_cls: 1.6836, decode.d0.loss_mask: 0.5182, decode.d0.loss_dice: 0.8143, decode.d1.loss_cls: 0.3381, decode.d1.loss_mask: 0.5177, decode.d1.loss_dice: 0.7850, decode.d2.loss_cls: 0.3012, decode.d2.loss_mask: 0.5051, decode.d2.loss_dice: 0.7554, decode.d3.loss_cls: 0.2829, decode.d3.loss_mask: 0.5008, decode.d3.loss_dice: 0.7452, decode.d4.loss_cls: 0.2771, decode.d4.loss_mask: 0.5002, decode.d4.loss_dice: 0.7438, decode.d5.loss_cls: 0.2745, decode.d5.loss_mask: 0.4984, decode.d5.loss_dice: 0.7410, decode.d6.loss_cls: 0.2690, decode.d6.loss_mask: 0.4986, decode.d6.loss_dice: 0.7383, decode.d7.loss_cls: 0.2703, decode.d7.loss_mask: 0.4979, decode.d7.loss_dice: 0.7377, loss: 15.3044
2022-10-30 20:58:45,812 - mmseg - INFO - Iter [10400/20000]	lr: 1.465e-06, eta: 1 day, 7:06:02, time: 2.465, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2808, decode.loss_mask: 0.5053, decode.loss_dice: 0.7500, decode.d0.loss_cls: 1.6906, decode.d0.loss_mask: 0.5266, decode.d0.loss_dice: 0.8236, decode.d1.loss_cls: 0.3491, decode.d1.loss_mask: 0.5204, decode.d1.loss_dice: 0.7901, decode.d2.loss_cls: 0.3094, decode.d2.loss_mask: 0.5114, decode.d2.loss_dice: 0.7652, decode.d3.loss_cls: 0.2942, decode.d3.loss_mask: 0.5067, decode.d3.loss_dice: 0.7584, decode.d4.loss_cls: 0.2875, decode.d4.loss_mask: 0.5053, decode.d4.loss_dice: 0.7550, decode.d5.loss_cls: 0.2849, decode.d5.loss_mask: 0.5049, decode.d5.loss_dice: 0.7512, decode.d6.loss_cls: 0.2829, decode.d6.loss_mask: 0.5037, decode.d6.loss_dice: 0.7504, decode.d7.loss_cls: 0.2816, decode.d7.loss_mask: 0.5045, decode.d7.loss_dice: 0.7493, loss: 15.5433
2022-10-30 21:00:52,120 - mmseg - INFO - Iter [10450/20000]	lr: 1.458e-06, eta: 1 day, 4:15:06, time: 2.526, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2682, decode.loss_mask: 0.4907, decode.loss_dice: 0.7307, decode.d0.loss_cls: 1.6923, decode.d0.loss_mask: 0.5113, decode.d0.loss_dice: 0.8013, decode.d1.loss_cls: 0.3420, decode.d1.loss_mask: 0.5039, decode.d1.loss_dice: 0.7668, decode.d2.loss_cls: 0.3006, decode.d2.loss_mask: 0.4955, decode.d2.loss_dice: 0.7452, decode.d3.loss_cls: 0.2820, decode.d3.loss_mask: 0.4914, decode.d3.loss_dice: 0.7369, decode.d4.loss_cls: 0.2753, decode.d4.loss_mask: 0.4906, decode.d4.loss_dice: 0.7346, decode.d5.loss_cls: 0.2721, decode.d5.loss_mask: 0.4897, decode.d5.loss_dice: 0.7366, decode.d6.loss_cls: 0.2685, decode.d6.loss_mask: 0.4904, decode.d6.loss_dice: 0.7331, decode.d7.loss_cls: 0.2690, decode.d7.loss_mask: 0.4898, decode.d7.loss_dice: 0.7324, loss: 15.1408
2022-10-30 21:02:56,102 - mmseg - INFO - Iter [10500/20000]	lr: 1.450e-06, eta: 1 day, 1:57:07, time: 2.480, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2931, decode.loss_mask: 0.5039, decode.loss_dice: 0.7535, decode.d0.loss_cls: 1.7017, decode.d0.loss_mask: 0.5263, decode.d0.loss_dice: 0.8248, decode.d1.loss_cls: 0.3614, decode.d1.loss_mask: 0.5210, decode.d1.loss_dice: 0.7945, decode.d2.loss_cls: 0.3232, decode.d2.loss_mask: 0.5113, decode.d2.loss_dice: 0.7668, decode.d3.loss_cls: 0.3052, decode.d3.loss_mask: 0.5063, decode.d3.loss_dice: 0.7550, decode.d4.loss_cls: 0.3015, decode.d4.loss_mask: 0.5049, decode.d4.loss_dice: 0.7557, decode.d5.loss_cls: 0.2937, decode.d5.loss_mask: 0.5060, decode.d5.loss_dice: 0.7542, decode.d6.loss_cls: 0.2917, decode.d6.loss_mask: 0.5041, decode.d6.loss_dice: 0.7531, decode.d7.loss_cls: 0.2909, decode.d7.loss_mask: 0.5039, decode.d7.loss_dice: 0.7519, loss: 15.6597
2022-10-30 21:05:04,067 - mmseg - INFO - Iter [10550/20000]	lr: 1.442e-06, eta: 1 day, 0:04:56, time: 2.559, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2859, decode.loss_mask: 0.4979, decode.loss_dice: 0.7471, decode.d0.loss_cls: 1.6998, decode.d0.loss_mask: 0.5220, decode.d0.loss_dice: 0.8222, decode.d1.loss_cls: 0.3541, decode.d1.loss_mask: 0.5141, decode.d1.loss_dice: 0.7839, decode.d2.loss_cls: 0.3118, decode.d2.loss_mask: 0.5042, decode.d2.loss_dice: 0.7582, decode.d3.loss_cls: 0.2968, decode.d3.loss_mask: 0.4995, decode.d3.loss_dice: 0.7494, decode.d4.loss_cls: 0.2922, decode.d4.loss_mask: 0.4993, decode.d4.loss_dice: 0.7477, decode.d5.loss_cls: 0.2889, decode.d5.loss_mask: 0.4993, decode.d5.loss_dice: 0.7470, decode.d6.loss_cls: 0.2864, decode.d6.loss_mask: 0.4983, decode.d6.loss_dice: 0.7476, decode.d7.loss_cls: 0.2855, decode.d7.loss_mask: 0.4985, decode.d7.loss_dice: 0.7461, loss: 15.4836
2022-10-30 21:07:13,036 - mmseg - INFO - Iter [10600/20000]	lr: 1.435e-06, eta: 22:31:20, time: 2.579, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2783, decode.loss_mask: 0.4993, decode.loss_dice: 0.7476, decode.d0.loss_cls: 1.6742, decode.d0.loss_mask: 0.5224, decode.d0.loss_dice: 0.8218, decode.d1.loss_cls: 0.3409, decode.d1.loss_mask: 0.5152, decode.d1.loss_dice: 0.7890, decode.d2.loss_cls: 0.3051, decode.d2.loss_mask: 0.5056, decode.d2.loss_dice: 0.7656, decode.d3.loss_cls: 0.2904, decode.d3.loss_mask: 0.5012, decode.d3.loss_dice: 0.7555, decode.d4.loss_cls: 0.2843, decode.d4.loss_mask: 0.5022, decode.d4.loss_dice: 0.7542, decode.d5.loss_cls: 0.2811, decode.d5.loss_mask: 0.4997, decode.d5.loss_dice: 0.7532, decode.d6.loss_cls: 0.2797, decode.d6.loss_mask: 0.4997, decode.d6.loss_dice: 0.7492, decode.d7.loss_cls: 0.2792, decode.d7.loss_mask: 0.4991, decode.d7.loss_dice: 0.7518, loss: 15.4456
2022-10-30 21:09:16,146 - mmseg - INFO - Iter [10650/20000]	lr: 1.427e-06, eta: 21:10:23, time: 2.462, data_time: 0.061, memory: 35615, decode.loss_cls: 0.2804, decode.loss_mask: 0.4944, decode.loss_dice: 0.7409, decode.d0.loss_cls: 1.6890, decode.d0.loss_mask: 0.5189, decode.d0.loss_dice: 0.8136, decode.d1.loss_cls: 0.3521, decode.d1.loss_mask: 0.5095, decode.d1.loss_dice: 0.7758, decode.d2.loss_cls: 0.3089, decode.d2.loss_mask: 0.5002, decode.d2.loss_dice: 0.7565, decode.d3.loss_cls: 0.2923, decode.d3.loss_mask: 0.4948, decode.d3.loss_dice: 0.7456, decode.d4.loss_cls: 0.2868, decode.d4.loss_mask: 0.4937, decode.d4.loss_dice: 0.7437, decode.d5.loss_cls: 0.2848, decode.d5.loss_mask: 0.4947, decode.d5.loss_dice: 0.7423, decode.d6.loss_cls: 0.2812, decode.d6.loss_mask: 0.4941, decode.d6.loss_dice: 0.7374, decode.d7.loss_cls: 0.2778, decode.d7.loss_mask: 0.4945, decode.d7.loss_dice: 0.7419, loss: 15.3460
2022-10-30 21:11:16,515 - mmseg - INFO - Iter [10700/20000]	lr: 1.419e-06, eta: 20:00:05, time: 2.407, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2701, decode.loss_mask: 0.4880, decode.loss_dice: 0.7290, decode.d0.loss_cls: 1.6768, decode.d0.loss_mask: 0.5085, decode.d0.loss_dice: 0.7991, decode.d1.loss_cls: 0.3409, decode.d1.loss_mask: 0.5030, decode.d1.loss_dice: 0.7619, decode.d2.loss_cls: 0.3024, decode.d2.loss_mask: 0.4939, decode.d2.loss_dice: 0.7397, decode.d3.loss_cls: 0.2849, decode.d3.loss_mask: 0.4928, decode.d3.loss_dice: 0.7318, decode.d4.loss_cls: 0.2774, decode.d4.loss_mask: 0.4915, decode.d4.loss_dice: 0.7322, decode.d5.loss_cls: 0.2729, decode.d5.loss_mask: 0.4887, decode.d5.loss_dice: 0.7294, decode.d6.loss_cls: 0.2680, decode.d6.loss_mask: 0.4896, decode.d6.loss_dice: 0.7284, decode.d7.loss_cls: 0.2721, decode.d7.loss_mask: 0.4887, decode.d7.loss_dice: 0.7257, loss: 15.0874
2022-10-30 21:13:18,299 - mmseg - INFO - Iter [10750/20000]	lr: 1.412e-06, eta: 18:59:09, time: 2.436, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2704, decode.loss_mask: 0.4962, decode.loss_dice: 0.7379, decode.d0.loss_cls: 1.6556, decode.d0.loss_mask: 0.5177, decode.d0.loss_dice: 0.8127, decode.d1.loss_cls: 0.3443, decode.d1.loss_mask: 0.5089, decode.d1.loss_dice: 0.7743, decode.d2.loss_cls: 0.3037, decode.d2.loss_mask: 0.5000, decode.d2.loss_dice: 0.7507, decode.d3.loss_cls: 0.2871, decode.d3.loss_mask: 0.4950, decode.d3.loss_dice: 0.7411, decode.d4.loss_cls: 0.2792, decode.d4.loss_mask: 0.4959, decode.d4.loss_dice: 0.7409, decode.d5.loss_cls: 0.2732, decode.d5.loss_mask: 0.4964, decode.d5.loss_dice: 0.7415, decode.d6.loss_cls: 0.2727, decode.d6.loss_mask: 0.4951, decode.d6.loss_dice: 0.7381, decode.d7.loss_cls: 0.2704, decode.d7.loss_mask: 0.4957, decode.d7.loss_dice: 0.7373, loss: 15.2318
2022-10-30 21:15:22,201 - mmseg - INFO - Iter [10800/20000]	lr: 1.404e-06, eta: 18:05:59, time: 2.478, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2686, decode.loss_mask: 0.4862, decode.loss_dice: 0.7360, decode.d0.loss_cls: 1.6756, decode.d0.loss_mask: 0.5093, decode.d0.loss_dice: 0.8043, decode.d1.loss_cls: 0.3434, decode.d1.loss_mask: 0.4999, decode.d1.loss_dice: 0.7739, decode.d2.loss_cls: 0.2991, decode.d2.loss_mask: 0.4923, decode.d2.loss_dice: 0.7519, decode.d3.loss_cls: 0.2824, decode.d3.loss_mask: 0.4883, decode.d3.loss_dice: 0.7386, decode.d4.loss_cls: 0.2756, decode.d4.loss_mask: 0.4867, decode.d4.loss_dice: 0.7403, decode.d5.loss_cls: 0.2739, decode.d5.loss_mask: 0.4850, decode.d5.loss_dice: 0.7367, decode.d6.loss_cls: 0.2679, decode.d6.loss_mask: 0.4865, decode.d6.loss_dice: 0.7376, decode.d7.loss_cls: 0.2691, decode.d7.loss_mask: 0.4865, decode.d7.loss_dice: 0.7361, loss: 15.1318
2022-10-30 21:17:33,870 - mmseg - INFO - Iter [10850/20000]	lr: 1.397e-06, eta: 17:20:13, time: 2.632, data_time: 0.023, memory: 35615, decode.loss_cls: 0.2670, decode.loss_mask: 0.4922, decode.loss_dice: 0.7293, decode.d0.loss_cls: 1.6591, decode.d0.loss_mask: 0.5140, decode.d0.loss_dice: 0.7990, decode.d1.loss_cls: 0.3360, decode.d1.loss_mask: 0.5086, decode.d1.loss_dice: 0.7682, decode.d2.loss_cls: 0.2949, decode.d2.loss_mask: 0.5001, decode.d2.loss_dice: 0.7464, decode.d3.loss_cls: 0.2783, decode.d3.loss_mask: 0.4954, decode.d3.loss_dice: 0.7335, decode.d4.loss_cls: 0.2736, decode.d4.loss_mask: 0.4949, decode.d4.loss_dice: 0.7327, decode.d5.loss_cls: 0.2694, decode.d5.loss_mask: 0.4933, decode.d5.loss_dice: 0.7306, decode.d6.loss_cls: 0.2682, decode.d6.loss_mask: 0.4920, decode.d6.loss_dice: 0.7305, decode.d7.loss_cls: 0.2639, decode.d7.loss_mask: 0.4928, decode.d7.loss_dice: 0.7309, loss: 15.0948
2022-10-30 21:19:39,028 - mmseg - INFO - Iter [10900/20000]	lr: 1.389e-06, eta: 16:38:12, time: 2.505, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2856, decode.loss_mask: 0.5044, decode.loss_dice: 0.7528, decode.d0.loss_cls: 1.6859, decode.d0.loss_mask: 0.5274, decode.d0.loss_dice: 0.8315, decode.d1.loss_cls: 0.3530, decode.d1.loss_mask: 0.5204, decode.d1.loss_dice: 0.7986, decode.d2.loss_cls: 0.3137, decode.d2.loss_mask: 0.5130, decode.d2.loss_dice: 0.7743, decode.d3.loss_cls: 0.2956, decode.d3.loss_mask: 0.5077, decode.d3.loss_dice: 0.7616, decode.d4.loss_cls: 0.2902, decode.d4.loss_mask: 0.5063, decode.d4.loss_dice: 0.7604, decode.d5.loss_cls: 0.2887, decode.d5.loss_mask: 0.5053, decode.d5.loss_dice: 0.7581, decode.d6.loss_cls: 0.2831, decode.d6.loss_mask: 0.5049, decode.d6.loss_dice: 0.7544, decode.d7.loss_cls: 0.2833, decode.d7.loss_mask: 0.5044, decode.d7.loss_dice: 0.7554, loss: 15.6201
2022-10-30 21:21:48,932 - mmseg - INFO - Iter [10950/20000]	lr: 1.381e-06, eta: 16:01:07, time: 2.598, data_time: 0.085, memory: 35615, decode.loss_cls: 0.2774, decode.loss_mask: 0.4945, decode.loss_dice: 0.7412, decode.d0.loss_cls: 1.6759, decode.d0.loss_mask: 0.5181, decode.d0.loss_dice: 0.8133, decode.d1.loss_cls: 0.3466, decode.d1.loss_mask: 0.5109, decode.d1.loss_dice: 0.7860, decode.d2.loss_cls: 0.3078, decode.d2.loss_mask: 0.5010, decode.d2.loss_dice: 0.7543, decode.d3.loss_cls: 0.2873, decode.d3.loss_mask: 0.4959, decode.d3.loss_dice: 0.7442, decode.d4.loss_cls: 0.2831, decode.d4.loss_mask: 0.4964, decode.d4.loss_dice: 0.7446, decode.d5.loss_cls: 0.2786, decode.d5.loss_mask: 0.4962, decode.d5.loss_dice: 0.7419, decode.d6.loss_cls: 0.2768, decode.d6.loss_mask: 0.4946, decode.d6.loss_dice: 0.7399, decode.d7.loss_cls: 0.2768, decode.d7.loss_mask: 0.4957, decode.d7.loss_dice: 0.7417, loss: 15.3208
2022-10-30 21:23:52,130 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-30 21:23:52,134 - mmseg - INFO - Iter [11000/20000]	lr: 1.374e-06, eta: 15:26:31, time: 2.463, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2711, decode.loss_mask: 0.5010, decode.loss_dice: 0.7511, decode.d0.loss_cls: 1.6511, decode.d0.loss_mask: 0.5233, decode.d0.loss_dice: 0.8186, decode.d1.loss_cls: 0.3387, decode.d1.loss_mask: 0.5163, decode.d1.loss_dice: 0.7941, decode.d2.loss_cls: 0.2963, decode.d2.loss_mask: 0.5080, decode.d2.loss_dice: 0.7723, decode.d3.loss_cls: 0.2818, decode.d3.loss_mask: 0.5034, decode.d3.loss_dice: 0.7591, decode.d4.loss_cls: 0.2739, decode.d4.loss_mask: 0.5044, decode.d4.loss_dice: 0.7567, decode.d5.loss_cls: 0.2737, decode.d5.loss_mask: 0.5027, decode.d5.loss_dice: 0.7552, decode.d6.loss_cls: 0.2710, decode.d6.loss_mask: 0.5015, decode.d6.loss_dice: 0.7531, decode.d7.loss_cls: 0.2712, decode.d7.loss_mask: 0.5017, decode.d7.loss_dice: 0.7535, loss: 15.4052
2022-10-30 21:25:58,464 - mmseg - INFO - Iter [11050/20000]	lr: 1.366e-06, eta: 14:55:29, time: 2.527, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2687, decode.loss_mask: 0.4837, decode.loss_dice: 0.7233, decode.d0.loss_cls: 1.6535, decode.d0.loss_mask: 0.5066, decode.d0.loss_dice: 0.7961, decode.d1.loss_cls: 0.3379, decode.d1.loss_mask: 0.5003, decode.d1.loss_dice: 0.7601, decode.d2.loss_cls: 0.2966, decode.d2.loss_mask: 0.4919, decode.d2.loss_dice: 0.7389, decode.d3.loss_cls: 0.2802, decode.d3.loss_mask: 0.4858, decode.d3.loss_dice: 0.7279, decode.d4.loss_cls: 0.2762, decode.d4.loss_mask: 0.4841, decode.d4.loss_dice: 0.7262, decode.d5.loss_cls: 0.2702, decode.d5.loss_mask: 0.4843, decode.d5.loss_dice: 0.7278, decode.d6.loss_cls: 0.2684, decode.d6.loss_mask: 0.4843, decode.d6.loss_dice: 0.7228, decode.d7.loss_cls: 0.2665, decode.d7.loss_mask: 0.4833, decode.d7.loss_dice: 0.7266, loss: 14.9722
2022-10-30 21:27:58,840 - mmseg - INFO - Iter [11100/20000]	lr: 1.358e-06, eta: 14:26:15, time: 2.407, data_time: 0.027, memory: 35615, decode.loss_cls: 0.2723, decode.loss_mask: 0.4791, decode.loss_dice: 0.7245, decode.d0.loss_cls: 1.6693, decode.d0.loss_mask: 0.5032, decode.d0.loss_dice: 0.7993, decode.d1.loss_cls: 0.3478, decode.d1.loss_mask: 0.4938, decode.d1.loss_dice: 0.7647, decode.d2.loss_cls: 0.3048, decode.d2.loss_mask: 0.4841, decode.d2.loss_dice: 0.7383, decode.d3.loss_cls: 0.2865, decode.d3.loss_mask: 0.4806, decode.d3.loss_dice: 0.7272, decode.d4.loss_cls: 0.2806, decode.d4.loss_mask: 0.4791, decode.d4.loss_dice: 0.7280, decode.d5.loss_cls: 0.2748, decode.d5.loss_mask: 0.4800, decode.d5.loss_dice: 0.7251, decode.d6.loss_cls: 0.2725, decode.d6.loss_mask: 0.4797, decode.d6.loss_dice: 0.7249, decode.d7.loss_cls: 0.2722, decode.d7.loss_mask: 0.4797, decode.d7.loss_dice: 0.7237, loss: 14.9956
2022-10-30 21:30:01,754 - mmseg - INFO - Iter [11150/20000]	lr: 1.351e-06, eta: 13:59:43, time: 2.458, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2787, decode.loss_mask: 0.4982, decode.loss_dice: 0.7423, decode.d0.loss_cls: 1.6414, decode.d0.loss_mask: 0.5243, decode.d0.loss_dice: 0.8175, decode.d1.loss_cls: 0.3478, decode.d1.loss_mask: 0.5152, decode.d1.loss_dice: 0.7793, decode.d2.loss_cls: 0.3056, decode.d2.loss_mask: 0.5073, decode.d2.loss_dice: 0.7591, decode.d3.loss_cls: 0.2883, decode.d3.loss_mask: 0.5003, decode.d3.loss_dice: 0.7479, decode.d4.loss_cls: 0.2826, decode.d4.loss_mask: 0.4997, decode.d4.loss_dice: 0.7502, decode.d5.loss_cls: 0.2792, decode.d5.loss_mask: 0.4996, decode.d5.loss_dice: 0.7446, decode.d6.loss_cls: 0.2788, decode.d6.loss_mask: 0.4983, decode.d6.loss_dice: 0.7430, decode.d7.loss_cls: 0.2795, decode.d7.loss_mask: 0.4975, decode.d7.loss_dice: 0.7406, loss: 15.3468
2022-10-30 21:32:03,552 - mmseg - INFO - Iter [11200/20000]	lr: 1.343e-06, eta: 13:35:05, time: 2.436, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2796, decode.loss_mask: 0.4884, decode.loss_dice: 0.7335, decode.d0.loss_cls: 1.6619, decode.d0.loss_mask: 0.5128, decode.d0.loss_dice: 0.8074, decode.d1.loss_cls: 0.3511, decode.d1.loss_mask: 0.5047, decode.d1.loss_dice: 0.7735, decode.d2.loss_cls: 0.3120, decode.d2.loss_mask: 0.4961, decode.d2.loss_dice: 0.7495, decode.d3.loss_cls: 0.2958, decode.d3.loss_mask: 0.4923, decode.d3.loss_dice: 0.7392, decode.d4.loss_cls: 0.2879, decode.d4.loss_mask: 0.4909, decode.d4.loss_dice: 0.7376, decode.d5.loss_cls: 0.2834, decode.d5.loss_mask: 0.4898, decode.d5.loss_dice: 0.7359, decode.d6.loss_cls: 0.2793, decode.d6.loss_mask: 0.4886, decode.d6.loss_dice: 0.7332, decode.d7.loss_cls: 0.2791, decode.d7.loss_mask: 0.4881, decode.d7.loss_dice: 0.7316, loss: 15.2232
2022-10-30 21:34:10,385 - mmseg - INFO - Iter [11250/20000]	lr: 1.335e-06, eta: 13:12:51, time: 2.537, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2831, decode.loss_mask: 0.4936, decode.loss_dice: 0.7444, decode.d0.loss_cls: 1.6737, decode.d0.loss_mask: 0.5168, decode.d0.loss_dice: 0.8167, decode.d1.loss_cls: 0.3515, decode.d1.loss_mask: 0.5101, decode.d1.loss_dice: 0.7852, decode.d2.loss_cls: 0.3130, decode.d2.loss_mask: 0.5009, decode.d2.loss_dice: 0.7593, decode.d3.loss_cls: 0.2960, decode.d3.loss_mask: 0.4953, decode.d3.loss_dice: 0.7479, decode.d4.loss_cls: 0.2905, decode.d4.loss_mask: 0.4949, decode.d4.loss_dice: 0.7464, decode.d5.loss_cls: 0.2853, decode.d5.loss_mask: 0.4938, decode.d5.loss_dice: 0.7465, decode.d6.loss_cls: 0.2835, decode.d6.loss_mask: 0.4936, decode.d6.loss_dice: 0.7461, decode.d7.loss_cls: 0.2831, decode.d7.loss_mask: 0.4942, decode.d7.loss_dice: 0.7448, loss: 15.3903
2022-10-30 21:36:17,984 - mmseg - INFO - Iter [11300/20000]	lr: 1.328e-06, eta: 12:52:14, time: 2.552, data_time: 0.059, memory: 35615, decode.loss_cls: 0.2582, decode.loss_mask: 0.4872, decode.loss_dice: 0.7284, decode.d0.loss_cls: 1.6429, decode.d0.loss_mask: 0.5108, decode.d0.loss_dice: 0.8037, decode.d1.loss_cls: 0.3241, decode.d1.loss_mask: 0.5055, decode.d1.loss_dice: 0.7731, decode.d2.loss_cls: 0.2851, decode.d2.loss_mask: 0.4962, decode.d2.loss_dice: 0.7474, decode.d3.loss_cls: 0.2678, decode.d3.loss_mask: 0.4898, decode.d3.loss_dice: 0.7360, decode.d4.loss_cls: 0.2630, decode.d4.loss_mask: 0.4891, decode.d4.loss_dice: 0.7357, decode.d5.loss_cls: 0.2603, decode.d5.loss_mask: 0.4892, decode.d5.loss_dice: 0.7347, decode.d6.loss_cls: 0.2575, decode.d6.loss_mask: 0.4877, decode.d6.loss_dice: 0.7308, decode.d7.loss_cls: 0.2598, decode.d7.loss_mask: 0.4874, decode.d7.loss_dice: 0.7286, loss: 14.9800
2022-10-30 21:38:20,545 - mmseg - INFO - Iter [11350/20000]	lr: 1.320e-06, eta: 12:32:28, time: 2.450, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2606, decode.loss_mask: 0.4899, decode.loss_dice: 0.7336, decode.d0.loss_cls: 1.6175, decode.d0.loss_mask: 0.5119, decode.d0.loss_dice: 0.7988, decode.d1.loss_cls: 0.3289, decode.d1.loss_mask: 0.5061, decode.d1.loss_dice: 0.7684, decode.d2.loss_cls: 0.2916, decode.d2.loss_mask: 0.4955, decode.d2.loss_dice: 0.7473, decode.d3.loss_cls: 0.2735, decode.d3.loss_mask: 0.4942, decode.d3.loss_dice: 0.7381, decode.d4.loss_cls: 0.2672, decode.d4.loss_mask: 0.4916, decode.d4.loss_dice: 0.7378, decode.d5.loss_cls: 0.2624, decode.d5.loss_mask: 0.4898, decode.d5.loss_dice: 0.7379, decode.d6.loss_cls: 0.2602, decode.d6.loss_mask: 0.4900, decode.d6.loss_dice: 0.7335, decode.d7.loss_cls: 0.2606, decode.d7.loss_mask: 0.4896, decode.d7.loss_dice: 0.7319, loss: 15.0082
2022-10-30 21:40:24,234 - mmseg - INFO - Iter [11400/20000]	lr: 1.313e-06, eta: 12:14:03, time: 2.472, data_time: 0.027, memory: 35615, decode.loss_cls: 0.2541, decode.loss_mask: 0.4766, decode.loss_dice: 0.7247, decode.d0.loss_cls: 1.6377, decode.d0.loss_mask: 0.4960, decode.d0.loss_dice: 0.7891, decode.d1.loss_cls: 0.3233, decode.d1.loss_mask: 0.4904, decode.d1.loss_dice: 0.7596, decode.d2.loss_cls: 0.2828, decode.d2.loss_mask: 0.4821, decode.d2.loss_dice: 0.7377, decode.d3.loss_cls: 0.2685, decode.d3.loss_mask: 0.4772, decode.d3.loss_dice: 0.7273, decode.d4.loss_cls: 0.2620, decode.d4.loss_mask: 0.4767, decode.d4.loss_dice: 0.7282, decode.d5.loss_cls: 0.2582, decode.d5.loss_mask: 0.4764, decode.d5.loss_dice: 0.7269, decode.d6.loss_cls: 0.2558, decode.d6.loss_mask: 0.4757, decode.d6.loss_dice: 0.7230, decode.d7.loss_cls: 0.2543, decode.d7.loss_mask: 0.4763, decode.d7.loss_dice: 0.7236, loss: 14.7644
2022-10-30 21:42:26,026 - mmseg - INFO - Iter [11450/20000]	lr: 1.305e-06, eta: 11:56:37, time: 2.439, data_time: 0.031, memory: 35615, decode.loss_cls: 0.2660, decode.loss_mask: 0.4917, decode.loss_dice: 0.7413, decode.d0.loss_cls: 1.6477, decode.d0.loss_mask: 0.5136, decode.d0.loss_dice: 0.8116, decode.d1.loss_cls: 0.3387, decode.d1.loss_mask: 0.5069, decode.d1.loss_dice: 0.7803, decode.d2.loss_cls: 0.2968, decode.d2.loss_mask: 0.5003, decode.d2.loss_dice: 0.7552, decode.d3.loss_cls: 0.2791, decode.d3.loss_mask: 0.4940, decode.d3.loss_dice: 0.7454, decode.d4.loss_cls: 0.2762, decode.d4.loss_mask: 0.4940, decode.d4.loss_dice: 0.7455, decode.d5.loss_cls: 0.2703, decode.d5.loss_mask: 0.4927, decode.d5.loss_dice: 0.7467, decode.d6.loss_cls: 0.2666, decode.d6.loss_mask: 0.4919, decode.d6.loss_dice: 0.7437, decode.d7.loss_cls: 0.2680, decode.d7.loss_mask: 0.4919, decode.d7.loss_dice: 0.7424, loss: 15.1983
2022-10-30 21:44:28,326 - mmseg - INFO - Iter [11500/20000]	lr: 1.297e-06, eta: 11:40:14, time: 2.446, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2663, decode.loss_mask: 0.4983, decode.loss_dice: 0.7349, decode.d0.loss_cls: 1.6298, decode.d0.loss_mask: 0.5227, decode.d0.loss_dice: 0.8045, decode.d1.loss_cls: 0.3324, decode.d1.loss_mask: 0.5143, decode.d1.loss_dice: 0.7785, decode.d2.loss_cls: 0.2954, decode.d2.loss_mask: 0.5039, decode.d2.loss_dice: 0.7540, decode.d3.loss_cls: 0.2792, decode.d3.loss_mask: 0.4997, decode.d3.loss_dice: 0.7421, decode.d4.loss_cls: 0.2746, decode.d4.loss_mask: 0.5001, decode.d4.loss_dice: 0.7401, decode.d5.loss_cls: 0.2715, decode.d5.loss_mask: 0.4990, decode.d5.loss_dice: 0.7385, decode.d6.loss_cls: 0.2682, decode.d6.loss_mask: 0.4984, decode.d6.loss_dice: 0.7371, decode.d7.loss_cls: 0.2689, decode.d7.loss_mask: 0.4989, decode.d7.loss_dice: 0.7344, loss: 15.1858
2022-10-30 21:46:29,397 - mmseg - INFO - Iter [11550/20000]	lr: 1.290e-06, eta: 11:24:40, time: 2.421, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2602, decode.loss_mask: 0.4874, decode.loss_dice: 0.7391, decode.d0.loss_cls: 1.6293, decode.d0.loss_mask: 0.5101, decode.d0.loss_dice: 0.8073, decode.d1.loss_cls: 0.3286, decode.d1.loss_mask: 0.5026, decode.d1.loss_dice: 0.7764, decode.d2.loss_cls: 0.2899, decode.d2.loss_mask: 0.4939, decode.d2.loss_dice: 0.7553, decode.d3.loss_cls: 0.2748, decode.d3.loss_mask: 0.4900, decode.d3.loss_dice: 0.7464, decode.d4.loss_cls: 0.2683, decode.d4.loss_mask: 0.4895, decode.d4.loss_dice: 0.7445, decode.d5.loss_cls: 0.2635, decode.d5.loss_mask: 0.4899, decode.d5.loss_dice: 0.7425, decode.d6.loss_cls: 0.2610, decode.d6.loss_mask: 0.4866, decode.d6.loss_dice: 0.7394, decode.d7.loss_cls: 0.2589, decode.d7.loss_mask: 0.4874, decode.d7.loss_dice: 0.7420, loss: 15.0648
2022-10-30 21:48:35,818 - mmseg - INFO - Iter [11600/20000]	lr: 1.282e-06, eta: 11:10:25, time: 2.528, data_time: 0.064, memory: 35615, decode.loss_cls: 0.2554, decode.loss_mask: 0.4733, decode.loss_dice: 0.7203, decode.d0.loss_cls: 1.6368, decode.d0.loss_mask: 0.4935, decode.d0.loss_dice: 0.7878, decode.d1.loss_cls: 0.3243, decode.d1.loss_mask: 0.4872, decode.d1.loss_dice: 0.7525, decode.d2.loss_cls: 0.2834, decode.d2.loss_mask: 0.4786, decode.d2.loss_dice: 0.7361, decode.d3.loss_cls: 0.2674, decode.d3.loss_mask: 0.4748, decode.d3.loss_dice: 0.7218, decode.d4.loss_cls: 0.2625, decode.d4.loss_mask: 0.4736, decode.d4.loss_dice: 0.7239, decode.d5.loss_cls: 0.2573, decode.d5.loss_mask: 0.4725, decode.d5.loss_dice: 0.7218, decode.d6.loss_cls: 0.2553, decode.d6.loss_mask: 0.4725, decode.d6.loss_dice: 0.7177, decode.d7.loss_cls: 0.2548, decode.d7.loss_mask: 0.4730, decode.d7.loss_dice: 0.7194, loss: 14.6971
2022-10-30 21:50:39,942 - mmseg - INFO - Iter [11650/20000]	lr: 1.274e-06, eta: 10:56:42, time: 2.482, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2622, decode.loss_mask: 0.4856, decode.loss_dice: 0.7382, decode.d0.loss_cls: 1.6229, decode.d0.loss_mask: 0.5070, decode.d0.loss_dice: 0.8039, decode.d1.loss_cls: 0.3326, decode.d1.loss_mask: 0.5024, decode.d1.loss_dice: 0.7767, decode.d2.loss_cls: 0.2928, decode.d2.loss_mask: 0.4928, decode.d2.loss_dice: 0.7545, decode.d3.loss_cls: 0.2790, decode.d3.loss_mask: 0.4877, decode.d3.loss_dice: 0.7411, decode.d4.loss_cls: 0.2693, decode.d4.loss_mask: 0.4880, decode.d4.loss_dice: 0.7434, decode.d5.loss_cls: 0.2647, decode.d5.loss_mask: 0.4871, decode.d5.loss_dice: 0.7386, decode.d6.loss_cls: 0.2631, decode.d6.loss_mask: 0.4861, decode.d6.loss_dice: 0.7393, decode.d7.loss_cls: 0.2616, decode.d7.loss_mask: 0.4854, decode.d7.loss_dice: 0.7376, loss: 15.0437
2022-10-30 21:52:45,027 - mmseg - INFO - Iter [11700/20000]	lr: 1.267e-06, eta: 10:43:45, time: 2.502, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2546, decode.loss_mask: 0.4810, decode.loss_dice: 0.7249, decode.d0.loss_cls: 1.6104, decode.d0.loss_mask: 0.5041, decode.d0.loss_dice: 0.7887, decode.d1.loss_cls: 0.3183, decode.d1.loss_mask: 0.4955, decode.d1.loss_dice: 0.7575, decode.d2.loss_cls: 0.2788, decode.d2.loss_mask: 0.4875, decode.d2.loss_dice: 0.7379, decode.d3.loss_cls: 0.2671, decode.d3.loss_mask: 0.4830, decode.d3.loss_dice: 0.7318, decode.d4.loss_cls: 0.2616, decode.d4.loss_mask: 0.4833, decode.d4.loss_dice: 0.7273, decode.d5.loss_cls: 0.2559, decode.d5.loss_mask: 0.4824, decode.d5.loss_dice: 0.7258, decode.d6.loss_cls: 0.2541, decode.d6.loss_mask: 0.4812, decode.d6.loss_dice: 0.7269, decode.d7.loss_cls: 0.2536, decode.d7.loss_mask: 0.4815, decode.d7.loss_dice: 0.7269, loss: 14.7813
2022-10-30 21:54:49,200 - mmseg - INFO - Iter [11750/20000]	lr: 1.259e-06, eta: 10:31:21, time: 2.483, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2611, decode.loss_mask: 0.4765, decode.loss_dice: 0.7165, decode.d0.loss_cls: 1.6321, decode.d0.loss_mask: 0.5009, decode.d0.loss_dice: 0.7873, decode.d1.loss_cls: 0.3321, decode.d1.loss_mask: 0.4916, decode.d1.loss_dice: 0.7540, decode.d2.loss_cls: 0.2908, decode.d2.loss_mask: 0.4830, decode.d2.loss_dice: 0.7345, decode.d3.loss_cls: 0.2753, decode.d3.loss_mask: 0.4799, decode.d3.loss_dice: 0.7238, decode.d4.loss_cls: 0.2665, decode.d4.loss_mask: 0.4797, decode.d4.loss_dice: 0.7229, decode.d5.loss_cls: 0.2618, decode.d5.loss_mask: 0.4778, decode.d5.loss_dice: 0.7205, decode.d6.loss_cls: 0.2593, decode.d6.loss_mask: 0.4774, decode.d6.loss_dice: 0.7173, decode.d7.loss_cls: 0.2612, decode.d7.loss_mask: 0.4768, decode.d7.loss_dice: 0.7197, loss: 14.7802
2022-10-30 21:57:01,921 - mmseg - INFO - Iter [11800/20000]	lr: 1.252e-06, eta: 10:20:11, time: 2.654, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2543, decode.loss_mask: 0.4905, decode.loss_dice: 0.7319, decode.d0.loss_cls: 1.6100, decode.d0.loss_mask: 0.5147, decode.d0.loss_dice: 0.8016, decode.d1.loss_cls: 0.3221, decode.d1.loss_mask: 0.5066, decode.d1.loss_dice: 0.7690, decode.d2.loss_cls: 0.2856, decode.d2.loss_mask: 0.4977, decode.d2.loss_dice: 0.7475, decode.d3.loss_cls: 0.2643, decode.d3.loss_mask: 0.4941, decode.d3.loss_dice: 0.7411, decode.d4.loss_cls: 0.2601, decode.d4.loss_mask: 0.4921, decode.d4.loss_dice: 0.7377, decode.d5.loss_cls: 0.2569, decode.d5.loss_mask: 0.4914, decode.d5.loss_dice: 0.7338, decode.d6.loss_cls: 0.2573, decode.d6.loss_mask: 0.4897, decode.d6.loss_dice: 0.7321, decode.d7.loss_cls: 0.2550, decode.d7.loss_mask: 0.4907, decode.d7.loss_dice: 0.7330, loss: 14.9608
2022-10-30 21:59:04,077 - mmseg - INFO - Iter [11850/20000]	lr: 1.244e-06, eta: 10:08:43, time: 2.443, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2642, decode.loss_mask: 0.4855, decode.loss_dice: 0.7326, decode.d0.loss_cls: 1.6216, decode.d0.loss_mask: 0.5106, decode.d0.loss_dice: 0.8049, decode.d1.loss_cls: 0.3376, decode.d1.loss_mask: 0.5022, decode.d1.loss_dice: 0.7679, decode.d2.loss_cls: 0.2936, decode.d2.loss_mask: 0.4934, decode.d2.loss_dice: 0.7446, decode.d3.loss_cls: 0.2765, decode.d3.loss_mask: 0.4887, decode.d3.loss_dice: 0.7351, decode.d4.loss_cls: 0.2707, decode.d4.loss_mask: 0.4873, decode.d4.loss_dice: 0.7359, decode.d5.loss_cls: 0.2687, decode.d5.loss_mask: 0.4858, decode.d5.loss_dice: 0.7321, decode.d6.loss_cls: 0.2664, decode.d6.loss_mask: 0.4844, decode.d6.loss_dice: 0.7303, decode.d7.loss_cls: 0.2642, decode.d7.loss_mask: 0.4856, decode.d7.loss_dice: 0.7303, loss: 15.0008
2022-10-30 22:01:15,580 - mmseg - INFO - Iter [11900/20000]	lr: 1.236e-06, eta: 9:58:24, time: 2.628, data_time: 0.057, memory: 35615, decode.loss_cls: 0.2572, decode.loss_mask: 0.4800, decode.loss_dice: 0.7275, decode.d0.loss_cls: 1.6142, decode.d0.loss_mask: 0.5034, decode.d0.loss_dice: 0.7951, decode.d1.loss_cls: 0.3208, decode.d1.loss_mask: 0.4958, decode.d1.loss_dice: 0.7649, decode.d2.loss_cls: 0.2822, decode.d2.loss_mask: 0.4854, decode.d2.loss_dice: 0.7420, decode.d3.loss_cls: 0.2683, decode.d3.loss_mask: 0.4811, decode.d3.loss_dice: 0.7332, decode.d4.loss_cls: 0.2635, decode.d4.loss_mask: 0.4813, decode.d4.loss_dice: 0.7287, decode.d5.loss_cls: 0.2599, decode.d5.loss_mask: 0.4804, decode.d5.loss_dice: 0.7311, decode.d6.loss_cls: 0.2574, decode.d6.loss_mask: 0.4793, decode.d6.loss_dice: 0.7281, decode.d7.loss_cls: 0.2571, decode.d7.loss_mask: 0.4797, decode.d7.loss_dice: 0.7292, loss: 14.8269
2022-10-30 22:03:26,206 - mmseg - INFO - Iter [11950/20000]	lr: 1.229e-06, eta: 9:48:27, time: 2.614, data_time: 0.015, memory: 35615, decode.loss_cls: 0.2560, decode.loss_mask: 0.4865, decode.loss_dice: 0.7262, decode.d0.loss_cls: 1.6166, decode.d0.loss_mask: 0.5128, decode.d0.loss_dice: 0.7964, decode.d1.loss_cls: 0.3194, decode.d1.loss_mask: 0.5044, decode.d1.loss_dice: 0.7690, decode.d2.loss_cls: 0.2860, decode.d2.loss_mask: 0.4942, decode.d2.loss_dice: 0.7437, decode.d3.loss_cls: 0.2675, decode.d3.loss_mask: 0.4919, decode.d3.loss_dice: 0.7326, decode.d4.loss_cls: 0.2608, decode.d4.loss_mask: 0.4910, decode.d4.loss_dice: 0.7355, decode.d5.loss_cls: 0.2581, decode.d5.loss_mask: 0.4876, decode.d5.loss_dice: 0.7303, decode.d6.loss_cls: 0.2550, decode.d6.loss_mask: 0.4871, decode.d6.loss_dice: 0.7274, decode.d7.loss_cls: 0.2545, decode.d7.loss_mask: 0.4880, decode.d7.loss_dice: 0.7270, loss: 14.9052
2022-10-30 22:05:34,087 - mmseg - INFO - Saving checkpoint at 12000 iterations
2022-10-30 22:06:06,142 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-30 22:06:06,143 - mmseg - INFO - Iter [12000/20000]	lr: 1.221e-06, eta: 9:40:50, time: 3.199, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2499, decode.loss_mask: 0.4901, decode.loss_dice: 0.7181, decode.d0.loss_cls: 1.5950, decode.d0.loss_mask: 0.5140, decode.d0.loss_dice: 0.7909, decode.d1.loss_cls: 0.3220, decode.d1.loss_mask: 0.5045, decode.d1.loss_dice: 0.7570, decode.d2.loss_cls: 0.2807, decode.d2.loss_mask: 0.4958, decode.d2.loss_dice: 0.7356, decode.d3.loss_cls: 0.2634, decode.d3.loss_mask: 0.4925, decode.d3.loss_dice: 0.7254, decode.d4.loss_cls: 0.2574, decode.d4.loss_mask: 0.4906, decode.d4.loss_dice: 0.7233, decode.d5.loss_cls: 0.2542, decode.d5.loss_mask: 0.4910, decode.d5.loss_dice: 0.7226, decode.d6.loss_cls: 0.2533, decode.d6.loss_mask: 0.4900, decode.d6.loss_dice: 0.7218, decode.d7.loss_cls: 0.2516, decode.d7.loss_mask: 0.4907, decode.d7.loss_dice: 0.7195, loss: 14.8011
2022-10-30 22:08:11,383 - mmseg - INFO - Iter [12050/20000]	lr: 1.213e-06, eta: 9:31:14, time: 2.504, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2583, decode.loss_mask: 0.4856, decode.loss_dice: 0.7255, decode.d0.loss_cls: 1.6039, decode.d0.loss_mask: 0.5121, decode.d0.loss_dice: 0.7970, decode.d1.loss_cls: 0.3244, decode.d1.loss_mask: 0.5022, decode.d1.loss_dice: 0.7612, decode.d2.loss_cls: 0.2896, decode.d2.loss_mask: 0.4928, decode.d2.loss_dice: 0.7397, decode.d3.loss_cls: 0.2680, decode.d3.loss_mask: 0.4874, decode.d3.loss_dice: 0.7296, decode.d4.loss_cls: 0.2611, decode.d4.loss_mask: 0.4868, decode.d4.loss_dice: 0.7296, decode.d5.loss_cls: 0.2604, decode.d5.loss_mask: 0.4865, decode.d5.loss_dice: 0.7264, decode.d6.loss_cls: 0.2580, decode.d6.loss_mask: 0.4856, decode.d6.loss_dice: 0.7270, decode.d7.loss_cls: 0.2588, decode.d7.loss_mask: 0.4854, decode.d7.loss_dice: 0.7262, loss: 14.8689
2022-10-30 22:10:11,799 - mmseg - INFO - Iter [12100/20000]	lr: 1.206e-06, eta: 9:21:40, time: 2.409, data_time: 0.015, memory: 35615, decode.loss_cls: 0.2624, decode.loss_mask: 0.4751, decode.loss_dice: 0.7327, decode.d0.loss_cls: 1.6242, decode.d0.loss_mask: 0.5015, decode.d0.loss_dice: 0.8075, decode.d1.loss_cls: 0.3310, decode.d1.loss_mask: 0.4916, decode.d1.loss_dice: 0.7717, decode.d2.loss_cls: 0.2915, decode.d2.loss_mask: 0.4820, decode.d2.loss_dice: 0.7488, decode.d3.loss_cls: 0.2750, decode.d3.loss_mask: 0.4781, decode.d3.loss_dice: 0.7376, decode.d4.loss_cls: 0.2676, decode.d4.loss_mask: 0.4773, decode.d4.loss_dice: 0.7381, decode.d5.loss_cls: 0.2653, decode.d5.loss_mask: 0.4759, decode.d5.loss_dice: 0.7369, decode.d6.loss_cls: 0.2615, decode.d6.loss_mask: 0.4745, decode.d6.loss_dice: 0.7329, decode.d7.loss_cls: 0.2623, decode.d7.loss_mask: 0.4750, decode.d7.loss_dice: 0.7335, loss: 14.9115
2022-10-30 22:12:09,582 - mmseg - INFO - Iter [12150/20000]	lr: 1.198e-06, eta: 9:12:19, time: 2.356, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2502, decode.loss_mask: 0.4824, decode.loss_dice: 0.7273, decode.d0.loss_cls: 1.5855, decode.d0.loss_mask: 0.5049, decode.d0.loss_dice: 0.7958, decode.d1.loss_cls: 0.3161, decode.d1.loss_mask: 0.4979, decode.d1.loss_dice: 0.7635, decode.d2.loss_cls: 0.2811, decode.d2.loss_mask: 0.4892, decode.d2.loss_dice: 0.7414, decode.d3.loss_cls: 0.2642, decode.d3.loss_mask: 0.4834, decode.d3.loss_dice: 0.7299, decode.d4.loss_cls: 0.2572, decode.d4.loss_mask: 0.4832, decode.d4.loss_dice: 0.7307, decode.d5.loss_cls: 0.2529, decode.d5.loss_mask: 0.4829, decode.d5.loss_dice: 0.7289, decode.d6.loss_cls: 0.2505, decode.d6.loss_mask: 0.4818, decode.d6.loss_dice: 0.7251, decode.d7.loss_cls: 0.2514, decode.d7.loss_mask: 0.4816, decode.d7.loss_dice: 0.7249, loss: 14.7637
2022-10-30 22:14:08,779 - mmseg - INFO - Iter [12200/20000]	lr: 1.190e-06, eta: 9:03:22, time: 2.384, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2575, decode.loss_mask: 0.4822, decode.loss_dice: 0.7200, decode.d0.loss_cls: 1.6100, decode.d0.loss_mask: 0.5067, decode.d0.loss_dice: 0.7870, decode.d1.loss_cls: 0.3241, decode.d1.loss_mask: 0.4974, decode.d1.loss_dice: 0.7557, decode.d2.loss_cls: 0.2824, decode.d2.loss_mask: 0.4872, decode.d2.loss_dice: 0.7375, decode.d3.loss_cls: 0.2688, decode.d3.loss_mask: 0.4845, decode.d3.loss_dice: 0.7246, decode.d4.loss_cls: 0.2637, decode.d4.loss_mask: 0.4832, decode.d4.loss_dice: 0.7219, decode.d5.loss_cls: 0.2606, decode.d5.loss_mask: 0.4830, decode.d5.loss_dice: 0.7198, decode.d6.loss_cls: 0.2595, decode.d6.loss_mask: 0.4813, decode.d6.loss_dice: 0.7187, decode.d7.loss_cls: 0.2582, decode.d7.loss_mask: 0.4815, decode.d7.loss_dice: 0.7195, loss: 14.7764
2022-10-30 22:16:16,706 - mmseg - INFO - Iter [12250/20000]	lr: 1.183e-06, eta: 8:55:14, time: 2.558, data_time: 0.057, memory: 35615, decode.loss_cls: 0.2595, decode.loss_mask: 0.4772, decode.loss_dice: 0.7199, decode.d0.loss_cls: 1.6041, decode.d0.loss_mask: 0.5002, decode.d0.loss_dice: 0.7886, decode.d1.loss_cls: 0.3314, decode.d1.loss_mask: 0.4929, decode.d1.loss_dice: 0.7579, decode.d2.loss_cls: 0.2895, decode.d2.loss_mask: 0.4830, decode.d2.loss_dice: 0.7347, decode.d3.loss_cls: 0.2709, decode.d3.loss_mask: 0.4799, decode.d3.loss_dice: 0.7243, decode.d4.loss_cls: 0.2627, decode.d4.loss_mask: 0.4805, decode.d4.loss_dice: 0.7258, decode.d5.loss_cls: 0.2627, decode.d5.loss_mask: 0.4784, decode.d5.loss_dice: 0.7209, decode.d6.loss_cls: 0.2609, decode.d6.loss_mask: 0.4781, decode.d6.loss_dice: 0.7206, decode.d7.loss_cls: 0.2598, decode.d7.loss_mask: 0.4777, decode.d7.loss_dice: 0.7199, loss: 14.7619
2022-10-30 22:18:22,137 - mmseg - INFO - Iter [12300/20000]	lr: 1.175e-06, eta: 8:47:13, time: 2.509, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2601, decode.loss_mask: 0.4893, decode.loss_dice: 0.7239, decode.d0.loss_cls: 1.6166, decode.d0.loss_mask: 0.5124, decode.d0.loss_dice: 0.7949, decode.d1.loss_cls: 0.3341, decode.d1.loss_mask: 0.5014, decode.d1.loss_dice: 0.7601, decode.d2.loss_cls: 0.2931, decode.d2.loss_mask: 0.4949, decode.d2.loss_dice: 0.7380, decode.d3.loss_cls: 0.2738, decode.d3.loss_mask: 0.4908, decode.d3.loss_dice: 0.7303, decode.d4.loss_cls: 0.2668, decode.d4.loss_mask: 0.4897, decode.d4.loss_dice: 0.7287, decode.d5.loss_cls: 0.2625, decode.d5.loss_mask: 0.4898, decode.d5.loss_dice: 0.7276, decode.d6.loss_cls: 0.2614, decode.d6.loss_mask: 0.4888, decode.d6.loss_dice: 0.7261, decode.d7.loss_cls: 0.2612, decode.d7.loss_mask: 0.4891, decode.d7.loss_dice: 0.7236, loss: 14.9289
2022-10-30 22:20:26,168 - mmseg - INFO - Iter [12350/20000]	lr: 1.168e-06, eta: 8:39:23, time: 2.481, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2571, decode.loss_mask: 0.4890, decode.loss_dice: 0.7226, decode.d0.loss_cls: 1.6063, decode.d0.loss_mask: 0.5120, decode.d0.loss_dice: 0.7978, decode.d1.loss_cls: 0.3279, decode.d1.loss_mask: 0.5034, decode.d1.loss_dice: 0.7621, decode.d2.loss_cls: 0.2871, decode.d2.loss_mask: 0.4934, decode.d2.loss_dice: 0.7377, decode.d3.loss_cls: 0.2683, decode.d3.loss_mask: 0.4905, decode.d3.loss_dice: 0.7272, decode.d4.loss_cls: 0.2658, decode.d4.loss_mask: 0.4889, decode.d4.loss_dice: 0.7275, decode.d5.loss_cls: 0.2600, decode.d5.loss_mask: 0.4885, decode.d5.loss_dice: 0.7248, decode.d6.loss_cls: 0.2584, decode.d6.loss_mask: 0.4878, decode.d6.loss_dice: 0.7223, decode.d7.loss_cls: 0.2572, decode.d7.loss_mask: 0.4886, decode.d7.loss_dice: 0.7224, loss: 14.8746
2022-10-30 22:22:29,800 - mmseg - INFO - Iter [12400/20000]	lr: 1.160e-06, eta: 8:31:46, time: 2.473, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2534, decode.loss_mask: 0.4772, decode.loss_dice: 0.7178, decode.d0.loss_cls: 1.5961, decode.d0.loss_mask: 0.5019, decode.d0.loss_dice: 0.7902, decode.d1.loss_cls: 0.3188, decode.d1.loss_mask: 0.4922, decode.d1.loss_dice: 0.7528, decode.d2.loss_cls: 0.2802, decode.d2.loss_mask: 0.4827, decode.d2.loss_dice: 0.7313, decode.d3.loss_cls: 0.2659, decode.d3.loss_mask: 0.4795, decode.d3.loss_dice: 0.7259, decode.d4.loss_cls: 0.2611, decode.d4.loss_mask: 0.4784, decode.d4.loss_dice: 0.7224, decode.d5.loss_cls: 0.2551, decode.d5.loss_mask: 0.4786, decode.d5.loss_dice: 0.7226, decode.d6.loss_cls: 0.2518, decode.d6.loss_mask: 0.4778, decode.d6.loss_dice: 0.7195, decode.d7.loss_cls: 0.2540, decode.d7.loss_mask: 0.4770, decode.d7.loss_dice: 0.7184, loss: 14.6827
2022-10-30 22:24:33,206 - mmseg - INFO - Iter [12450/20000]	lr: 1.152e-06, eta: 8:24:22, time: 2.468, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2531, decode.loss_mask: 0.4775, decode.loss_dice: 0.7292, decode.d0.loss_cls: 1.5985, decode.d0.loss_mask: 0.5004, decode.d0.loss_dice: 0.7968, decode.d1.loss_cls: 0.3243, decode.d1.loss_mask: 0.4938, decode.d1.loss_dice: 0.7639, decode.d2.loss_cls: 0.2817, decode.d2.loss_mask: 0.4839, decode.d2.loss_dice: 0.7430, decode.d3.loss_cls: 0.2650, decode.d3.loss_mask: 0.4796, decode.d3.loss_dice: 0.7324, decode.d4.loss_cls: 0.2609, decode.d4.loss_mask: 0.4779, decode.d4.loss_dice: 0.7315, decode.d5.loss_cls: 0.2543, decode.d5.loss_mask: 0.4772, decode.d5.loss_dice: 0.7290, decode.d6.loss_cls: 0.2540, decode.d6.loss_mask: 0.4772, decode.d6.loss_dice: 0.7296, decode.d7.loss_cls: 0.2521, decode.d7.loss_mask: 0.4776, decode.d7.loss_dice: 0.7305, loss: 14.7747
2022-10-30 22:26:37,871 - mmseg - INFO - Iter [12500/20000]	lr: 1.145e-06, eta: 8:17:14, time: 2.493, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2457, decode.loss_mask: 0.4793, decode.loss_dice: 0.7168, decode.d0.loss_cls: 1.5768, decode.d0.loss_mask: 0.5039, decode.d0.loss_dice: 0.7798, decode.d1.loss_cls: 0.3099, decode.d1.loss_mask: 0.4948, decode.d1.loss_dice: 0.7562, decode.d2.loss_cls: 0.2739, decode.d2.loss_mask: 0.4859, decode.d2.loss_dice: 0.7285, decode.d3.loss_cls: 0.2587, decode.d3.loss_mask: 0.4806, decode.d3.loss_dice: 0.7208, decode.d4.loss_cls: 0.2528, decode.d4.loss_mask: 0.4805, decode.d4.loss_dice: 0.7190, decode.d5.loss_cls: 0.2482, decode.d5.loss_mask: 0.4803, decode.d5.loss_dice: 0.7167, decode.d6.loss_cls: 0.2459, decode.d6.loss_mask: 0.4796, decode.d6.loss_dice: 0.7165, decode.d7.loss_cls: 0.2453, decode.d7.loss_mask: 0.4794, decode.d7.loss_dice: 0.7160, loss: 14.5919
2022-10-30 22:28:45,287 - mmseg - INFO - Iter [12550/20000]	lr: 1.137e-06, eta: 8:10:26, time: 2.548, data_time: 0.056, memory: 35615, decode.loss_cls: 0.2598, decode.loss_mask: 0.4839, decode.loss_dice: 0.7344, decode.d0.loss_cls: 1.5913, decode.d0.loss_mask: 0.5096, decode.d0.loss_dice: 0.8069, decode.d1.loss_cls: 0.3259, decode.d1.loss_mask: 0.5004, decode.d1.loss_dice: 0.7798, decode.d2.loss_cls: 0.2897, decode.d2.loss_mask: 0.4912, decode.d2.loss_dice: 0.7520, decode.d3.loss_cls: 0.2735, decode.d3.loss_mask: 0.4865, decode.d3.loss_dice: 0.7406, decode.d4.loss_cls: 0.2656, decode.d4.loss_mask: 0.4864, decode.d4.loss_dice: 0.7414, decode.d5.loss_cls: 0.2611, decode.d5.loss_mask: 0.4839, decode.d5.loss_dice: 0.7405, decode.d6.loss_cls: 0.2582, decode.d6.loss_mask: 0.4843, decode.d6.loss_dice: 0.7382, decode.d7.loss_cls: 0.2592, decode.d7.loss_mask: 0.4854, decode.d7.loss_dice: 0.7377, loss: 14.9672
2022-10-30 22:30:56,316 - mmseg - INFO - Iter [12600/20000]	lr: 1.129e-06, eta: 8:04:00, time: 2.621, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2492, decode.loss_mask: 0.4781, decode.loss_dice: 0.7181, decode.d0.loss_cls: 1.5708, decode.d0.loss_mask: 0.5034, decode.d0.loss_dice: 0.7855, decode.d1.loss_cls: 0.3159, decode.d1.loss_mask: 0.4921, decode.d1.loss_dice: 0.7548, decode.d2.loss_cls: 0.2806, decode.d2.loss_mask: 0.4827, decode.d2.loss_dice: 0.7306, decode.d3.loss_cls: 0.2595, decode.d3.loss_mask: 0.4806, decode.d3.loss_dice: 0.7219, decode.d4.loss_cls: 0.2559, decode.d4.loss_mask: 0.4795, decode.d4.loss_dice: 0.7202, decode.d5.loss_cls: 0.2508, decode.d5.loss_mask: 0.4783, decode.d5.loss_dice: 0.7197, decode.d6.loss_cls: 0.2488, decode.d6.loss_mask: 0.4791, decode.d6.loss_dice: 0.7163, decode.d7.loss_cls: 0.2508, decode.d7.loss_mask: 0.4775, decode.d7.loss_dice: 0.7164, loss: 14.6170
2022-10-30 22:33:00,426 - mmseg - INFO - Iter [12650/20000]	lr: 1.122e-06, eta: 7:57:24, time: 2.482, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2436, decode.loss_mask: 0.4755, decode.loss_dice: 0.7191, decode.d0.loss_cls: 1.5778, decode.d0.loss_mask: 0.5030, decode.d0.loss_dice: 0.7880, decode.d1.loss_cls: 0.3135, decode.d1.loss_mask: 0.4897, decode.d1.loss_dice: 0.7573, decode.d2.loss_cls: 0.2722, decode.d2.loss_mask: 0.4823, decode.d2.loss_dice: 0.7352, decode.d3.loss_cls: 0.2540, decode.d3.loss_mask: 0.4783, decode.d3.loss_dice: 0.7246, decode.d4.loss_cls: 0.2479, decode.d4.loss_mask: 0.4767, decode.d4.loss_dice: 0.7262, decode.d5.loss_cls: 0.2470, decode.d5.loss_mask: 0.4755, decode.d5.loss_dice: 0.7217, decode.d6.loss_cls: 0.2422, decode.d6.loss_mask: 0.4752, decode.d6.loss_dice: 0.7200, decode.d7.loss_cls: 0.2403, decode.d7.loss_mask: 0.4746, decode.d7.loss_dice: 0.7203, loss: 14.5817
2022-10-30 22:35:02,538 - mmseg - INFO - Iter [12700/20000]	lr: 1.114e-06, eta: 7:50:52, time: 2.443, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2427, decode.loss_mask: 0.4765, decode.loss_dice: 0.7199, decode.d0.loss_cls: 1.5841, decode.d0.loss_mask: 0.5008, decode.d0.loss_dice: 0.7880, decode.d1.loss_cls: 0.3146, decode.d1.loss_mask: 0.4887, decode.d1.loss_dice: 0.7503, decode.d2.loss_cls: 0.2725, decode.d2.loss_mask: 0.4817, decode.d2.loss_dice: 0.7311, decode.d3.loss_cls: 0.2564, decode.d3.loss_mask: 0.4789, decode.d3.loss_dice: 0.7244, decode.d4.loss_cls: 0.2498, decode.d4.loss_mask: 0.4789, decode.d4.loss_dice: 0.7236, decode.d5.loss_cls: 0.2442, decode.d5.loss_mask: 0.4773, decode.d5.loss_dice: 0.7228, decode.d6.loss_cls: 0.2400, decode.d6.loss_mask: 0.4777, decode.d6.loss_dice: 0.7224, decode.d7.loss_cls: 0.2418, decode.d7.loss_mask: 0.4770, decode.d7.loss_dice: 0.7204, loss: 14.5866
2022-10-30 22:37:03,918 - mmseg - INFO - Iter [12750/20000]	lr: 1.107e-06, eta: 7:44:29, time: 2.428, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2509, decode.loss_mask: 0.4791, decode.loss_dice: 0.7190, decode.d0.loss_cls: 1.5941, decode.d0.loss_mask: 0.5043, decode.d0.loss_dice: 0.7868, decode.d1.loss_cls: 0.3203, decode.d1.loss_mask: 0.4941, decode.d1.loss_dice: 0.7582, decode.d2.loss_cls: 0.2784, decode.d2.loss_mask: 0.4841, decode.d2.loss_dice: 0.7336, decode.d3.loss_cls: 0.2653, decode.d3.loss_mask: 0.4809, decode.d3.loss_dice: 0.7237, decode.d4.loss_cls: 0.2559, decode.d4.loss_mask: 0.4804, decode.d4.loss_dice: 0.7219, decode.d5.loss_cls: 0.2547, decode.d5.loss_mask: 0.4799, decode.d5.loss_dice: 0.7193, decode.d6.loss_cls: 0.2547, decode.d6.loss_mask: 0.4777, decode.d6.loss_dice: 0.7188, decode.d7.loss_cls: 0.2515, decode.d7.loss_mask: 0.4785, decode.d7.loss_dice: 0.7195, loss: 14.6857
2022-10-30 22:39:09,413 - mmseg - INFO - Iter [12800/20000]	lr: 1.099e-06, eta: 7:38:25, time: 2.510, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2467, decode.loss_mask: 0.4834, decode.loss_dice: 0.7129, decode.d0.loss_cls: 1.5839, decode.d0.loss_mask: 0.5075, decode.d0.loss_dice: 0.7783, decode.d1.loss_cls: 0.3157, decode.d1.loss_mask: 0.4987, decode.d1.loss_dice: 0.7497, decode.d2.loss_cls: 0.2757, decode.d2.loss_mask: 0.4892, decode.d2.loss_dice: 0.7278, decode.d3.loss_cls: 0.2591, decode.d3.loss_mask: 0.4855, decode.d3.loss_dice: 0.7179, decode.d4.loss_cls: 0.2508, decode.d4.loss_mask: 0.4853, decode.d4.loss_dice: 0.7160, decode.d5.loss_cls: 0.2477, decode.d5.loss_mask: 0.4848, decode.d5.loss_dice: 0.7134, decode.d6.loss_cls: 0.2459, decode.d6.loss_mask: 0.4842, decode.d6.loss_dice: 0.7137, decode.d7.loss_cls: 0.2458, decode.d7.loss_mask: 0.4829, decode.d7.loss_dice: 0.7123, loss: 14.6146
2022-10-30 22:41:15,171 - mmseg - INFO - Iter [12850/20000]	lr: 1.091e-06, eta: 7:32:30, time: 2.515, data_time: 0.060, memory: 35615, decode.loss_cls: 0.2505, decode.loss_mask: 0.4792, decode.loss_dice: 0.7158, decode.d0.loss_cls: 1.5757, decode.d0.loss_mask: 0.5011, decode.d0.loss_dice: 0.7877, decode.d1.loss_cls: 0.3213, decode.d1.loss_mask: 0.4930, decode.d1.loss_dice: 0.7504, decode.d2.loss_cls: 0.2821, decode.d2.loss_mask: 0.4847, decode.d2.loss_dice: 0.7305, decode.d3.loss_cls: 0.2640, decode.d3.loss_mask: 0.4806, decode.d3.loss_dice: 0.7205, decode.d4.loss_cls: 0.2561, decode.d4.loss_mask: 0.4791, decode.d4.loss_dice: 0.7205, decode.d5.loss_cls: 0.2519, decode.d5.loss_mask: 0.4793, decode.d5.loss_dice: 0.7177, decode.d6.loss_cls: 0.2475, decode.d6.loss_mask: 0.4789, decode.d6.loss_dice: 0.7167, decode.d7.loss_cls: 0.2496, decode.d7.loss_mask: 0.4782, decode.d7.loss_dice: 0.7155, loss: 14.6281
2022-10-30 22:43:17,468 - mmseg - INFO - Iter [12900/20000]	lr: 1.084e-06, eta: 7:26:35, time: 2.446, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2436, decode.loss_mask: 0.4699, decode.loss_dice: 0.7085, decode.d0.loss_cls: 1.5799, decode.d0.loss_mask: 0.4953, decode.d0.loss_dice: 0.7738, decode.d1.loss_cls: 0.3125, decode.d1.loss_mask: 0.4851, decode.d1.loss_dice: 0.7462, decode.d2.loss_cls: 0.2733, decode.d2.loss_mask: 0.4771, decode.d2.loss_dice: 0.7235, decode.d3.loss_cls: 0.2552, decode.d3.loss_mask: 0.4742, decode.d3.loss_dice: 0.7131, decode.d4.loss_cls: 0.2495, decode.d4.loss_mask: 0.4722, decode.d4.loss_dice: 0.7119, decode.d5.loss_cls: 0.2452, decode.d5.loss_mask: 0.4702, decode.d5.loss_dice: 0.7066, decode.d6.loss_cls: 0.2444, decode.d6.loss_mask: 0.4702, decode.d6.loss_dice: 0.7062, decode.d7.loss_cls: 0.2458, decode.d7.loss_mask: 0.4703, decode.d7.loss_dice: 0.7069, loss: 14.4308
2022-10-30 22:45:19,192 - mmseg - INFO - Iter [12950/20000]	lr: 1.076e-06, eta: 7:20:46, time: 2.434, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2373, decode.loss_mask: 0.4860, decode.loss_dice: 0.7133, decode.d0.loss_cls: 1.5570, decode.d0.loss_mask: 0.5083, decode.d0.loss_dice: 0.7815, decode.d1.loss_cls: 0.3091, decode.d1.loss_mask: 0.5011, decode.d1.loss_dice: 0.7478, decode.d2.loss_cls: 0.2688, decode.d2.loss_mask: 0.4908, decode.d2.loss_dice: 0.7270, decode.d3.loss_cls: 0.2544, decode.d3.loss_mask: 0.4863, decode.d3.loss_dice: 0.7180, decode.d4.loss_cls: 0.2470, decode.d4.loss_mask: 0.4866, decode.d4.loss_dice: 0.7183, decode.d5.loss_cls: 0.2405, decode.d5.loss_mask: 0.4850, decode.d5.loss_dice: 0.7166, decode.d6.loss_cls: 0.2389, decode.d6.loss_mask: 0.4846, decode.d6.loss_dice: 0.7122, decode.d7.loss_cls: 0.2382, decode.d7.loss_mask: 0.4858, decode.d7.loss_dice: 0.7136, loss: 14.5538
2022-10-30 22:47:24,411 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-30 22:47:24,412 - mmseg - INFO - Iter [13000/20000]	lr: 1.068e-06, eta: 7:15:13, time: 2.504, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2351, decode.loss_mask: 0.4732, decode.loss_dice: 0.7151, decode.d0.loss_cls: 1.5765, decode.d0.loss_mask: 0.4991, decode.d0.loss_dice: 0.7844, decode.d1.loss_cls: 0.3058, decode.d1.loss_mask: 0.4881, decode.d1.loss_dice: 0.7488, decode.d2.loss_cls: 0.2654, decode.d2.loss_mask: 0.4801, decode.d2.loss_dice: 0.7278, decode.d3.loss_cls: 0.2488, decode.d3.loss_mask: 0.4748, decode.d3.loss_dice: 0.7169, decode.d4.loss_cls: 0.2425, decode.d4.loss_mask: 0.4755, decode.d4.loss_dice: 0.7172, decode.d5.loss_cls: 0.2379, decode.d5.loss_mask: 0.4733, decode.d5.loss_dice: 0.7142, decode.d6.loss_cls: 0.2375, decode.d6.loss_mask: 0.4731, decode.d6.loss_dice: 0.7113, decode.d7.loss_cls: 0.2340, decode.d7.loss_mask: 0.4736, decode.d7.loss_dice: 0.7137, loss: 14.4436
2022-10-30 22:49:26,086 - mmseg - INFO - Iter [13050/20000]	lr: 1.061e-06, eta: 7:09:39, time: 2.433, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2507, decode.loss_mask: 0.4754, decode.loss_dice: 0.7135, decode.d0.loss_cls: 1.5717, decode.d0.loss_mask: 0.4973, decode.d0.loss_dice: 0.7791, decode.d1.loss_cls: 0.3174, decode.d1.loss_mask: 0.4901, decode.d1.loss_dice: 0.7480, decode.d2.loss_cls: 0.2750, decode.d2.loss_mask: 0.4810, decode.d2.loss_dice: 0.7277, decode.d3.loss_cls: 0.2592, decode.d3.loss_mask: 0.4765, decode.d3.loss_dice: 0.7191, decode.d4.loss_cls: 0.2538, decode.d4.loss_mask: 0.4764, decode.d4.loss_dice: 0.7176, decode.d5.loss_cls: 0.2516, decode.d5.loss_mask: 0.4759, decode.d5.loss_dice: 0.7152, decode.d6.loss_cls: 0.2480, decode.d6.loss_mask: 0.4754, decode.d6.loss_dice: 0.7127, decode.d7.loss_cls: 0.2489, decode.d7.loss_mask: 0.4759, decode.d7.loss_dice: 0.7148, loss: 14.5480
2022-10-30 22:51:34,020 - mmseg - INFO - Iter [13100/20000]	lr: 1.053e-06, eta: 7:04:26, time: 2.559, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2407, decode.loss_mask: 0.4772, decode.loss_dice: 0.7167, decode.d0.loss_cls: 1.5870, decode.d0.loss_mask: 0.5017, decode.d0.loss_dice: 0.7832, decode.d1.loss_cls: 0.3104, decode.d1.loss_mask: 0.4925, decode.d1.loss_dice: 0.7511, decode.d2.loss_cls: 0.2730, decode.d2.loss_mask: 0.4823, decode.d2.loss_dice: 0.7302, decode.d3.loss_cls: 0.2553, decode.d3.loss_mask: 0.4795, decode.d3.loss_dice: 0.7185, decode.d4.loss_cls: 0.2468, decode.d4.loss_mask: 0.4790, decode.d4.loss_dice: 0.7198, decode.d5.loss_cls: 0.2439, decode.d5.loss_mask: 0.4778, decode.d5.loss_dice: 0.7177, decode.d6.loss_cls: 0.2417, decode.d6.loss_mask: 0.4770, decode.d6.loss_dice: 0.7168, decode.d7.loss_cls: 0.2410, decode.d7.loss_mask: 0.4772, decode.d7.loss_dice: 0.7170, loss: 14.5554
2022-10-30 22:53:37,540 - mmseg - INFO - Iter [13150/20000]	lr: 1.046e-06, eta: 6:59:08, time: 2.470, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2450, decode.loss_mask: 0.4708, decode.loss_dice: 0.7180, decode.d0.loss_cls: 1.5659, decode.d0.loss_mask: 0.4965, decode.d0.loss_dice: 0.7854, decode.d1.loss_cls: 0.3107, decode.d1.loss_mask: 0.4857, decode.d1.loss_dice: 0.7510, decode.d2.loss_cls: 0.2724, decode.d2.loss_mask: 0.4755, decode.d2.loss_dice: 0.7280, decode.d3.loss_cls: 0.2580, decode.d3.loss_mask: 0.4734, decode.d3.loss_dice: 0.7197, decode.d4.loss_cls: 0.2521, decode.d4.loss_mask: 0.4707, decode.d4.loss_dice: 0.7214, decode.d5.loss_cls: 0.2461, decode.d5.loss_mask: 0.4708, decode.d5.loss_dice: 0.7184, decode.d6.loss_cls: 0.2467, decode.d6.loss_mask: 0.4711, decode.d6.loss_dice: 0.7178, decode.d7.loss_cls: 0.2453, decode.d7.loss_mask: 0.4704, decode.d7.loss_dice: 0.7152, loss: 14.5019
2022-10-30 22:55:44,390 - mmseg - INFO - Iter [13200/20000]	lr: 1.038e-06, eta: 6:54:04, time: 2.537, data_time: 0.073, memory: 35615, decode.loss_cls: 0.2421, decode.loss_mask: 0.4708, decode.loss_dice: 0.7170, decode.d0.loss_cls: 1.5705, decode.d0.loss_mask: 0.4943, decode.d0.loss_dice: 0.7849, decode.d1.loss_cls: 0.3159, decode.d1.loss_mask: 0.4880, decode.d1.loss_dice: 0.7514, decode.d2.loss_cls: 0.2747, decode.d2.loss_mask: 0.4779, decode.d2.loss_dice: 0.7322, decode.d3.loss_cls: 0.2568, decode.d3.loss_mask: 0.4732, decode.d3.loss_dice: 0.7216, decode.d4.loss_cls: 0.2508, decode.d4.loss_mask: 0.4728, decode.d4.loss_dice: 0.7214, decode.d5.loss_cls: 0.2486, decode.d5.loss_mask: 0.4704, decode.d5.loss_dice: 0.7197, decode.d6.loss_cls: 0.2451, decode.d6.loss_mask: 0.4711, decode.d6.loss_dice: 0.7181, decode.d7.loss_cls: 0.2443, decode.d7.loss_mask: 0.4704, decode.d7.loss_dice: 0.7146, loss: 14.5184
2022-10-30 22:57:49,789 - mmseg - INFO - Iter [13250/20000]	lr: 1.030e-06, eta: 6:49:03, time: 2.508, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2396, decode.loss_mask: 0.4683, decode.loss_dice: 0.7070, decode.d0.loss_cls: 1.5780, decode.d0.loss_mask: 0.4924, decode.d0.loss_dice: 0.7703, decode.d1.loss_cls: 0.3077, decode.d1.loss_mask: 0.4806, decode.d1.loss_dice: 0.7454, decode.d2.loss_cls: 0.2726, decode.d2.loss_mask: 0.4735, decode.d2.loss_dice: 0.7191, decode.d3.loss_cls: 0.2532, decode.d3.loss_mask: 0.4705, decode.d3.loss_dice: 0.7097, decode.d4.loss_cls: 0.2478, decode.d4.loss_mask: 0.4686, decode.d4.loss_dice: 0.7098, decode.d5.loss_cls: 0.2448, decode.d5.loss_mask: 0.4672, decode.d5.loss_dice: 0.7061, decode.d6.loss_cls: 0.2413, decode.d6.loss_mask: 0.4677, decode.d6.loss_dice: 0.7054, decode.d7.loss_cls: 0.2398, decode.d7.loss_mask: 0.4677, decode.d7.loss_dice: 0.7045, loss: 14.3587
2022-10-30 22:59:53,870 - mmseg - INFO - Iter [13300/20000]	lr: 1.023e-06, eta: 6:44:04, time: 2.479, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2430, decode.loss_mask: 0.4695, decode.loss_dice: 0.7196, decode.d0.loss_cls: 1.5802, decode.d0.loss_mask: 0.4953, decode.d0.loss_dice: 0.7913, decode.d1.loss_cls: 0.3117, decode.d1.loss_mask: 0.4853, decode.d1.loss_dice: 0.7570, decode.d2.loss_cls: 0.2742, decode.d2.loss_mask: 0.4760, decode.d2.loss_dice: 0.7342, decode.d3.loss_cls: 0.2562, decode.d3.loss_mask: 0.4731, decode.d3.loss_dice: 0.7246, decode.d4.loss_cls: 0.2518, decode.d4.loss_mask: 0.4715, decode.d4.loss_dice: 0.7219, decode.d5.loss_cls: 0.2440, decode.d5.loss_mask: 0.4708, decode.d5.loss_dice: 0.7196, decode.d6.loss_cls: 0.2428, decode.d6.loss_mask: 0.4695, decode.d6.loss_dice: 0.7205, decode.d7.loss_cls: 0.2411, decode.d7.loss_mask: 0.4701, decode.d7.loss_dice: 0.7217, loss: 14.5367
2022-10-30 23:01:59,617 - mmseg - INFO - Iter [13350/20000]	lr: 1.015e-06, eta: 6:39:13, time: 2.517, data_time: 0.016, memory: 35615, decode.loss_cls: 0.2374, decode.loss_mask: 0.4789, decode.loss_dice: 0.7258, decode.d0.loss_cls: 1.5574, decode.d0.loss_mask: 0.5004, decode.d0.loss_dice: 0.7924, decode.d1.loss_cls: 0.3043, decode.d1.loss_mask: 0.4927, decode.d1.loss_dice: 0.7636, decode.d2.loss_cls: 0.2664, decode.d2.loss_mask: 0.4850, decode.d2.loss_dice: 0.7418, decode.d3.loss_cls: 0.2516, decode.d3.loss_mask: 0.4806, decode.d3.loss_dice: 0.7339, decode.d4.loss_cls: 0.2431, decode.d4.loss_mask: 0.4798, decode.d4.loss_dice: 0.7317, decode.d5.loss_cls: 0.2417, decode.d5.loss_mask: 0.4790, decode.d5.loss_dice: 0.7282, decode.d6.loss_cls: 0.2384, decode.d6.loss_mask: 0.4788, decode.d6.loss_dice: 0.7257, decode.d7.loss_cls: 0.2377, decode.d7.loss_mask: 0.4792, decode.d7.loss_dice: 0.7249, loss: 14.6003
2022-10-30 23:04:08,256 - mmseg - INFO - Iter [13400/20000]	lr: 1.007e-06, eta: 6:34:32, time: 2.563, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2371, decode.loss_mask: 0.4733, decode.loss_dice: 0.7149, decode.d0.loss_cls: 1.5466, decode.d0.loss_mask: 0.4976, decode.d0.loss_dice: 0.7820, decode.d1.loss_cls: 0.3070, decode.d1.loss_mask: 0.4868, decode.d1.loss_dice: 0.7502, decode.d2.loss_cls: 0.2695, decode.d2.loss_mask: 0.4776, decode.d2.loss_dice: 0.7295, decode.d3.loss_cls: 0.2466, decode.d3.loss_mask: 0.4754, decode.d3.loss_dice: 0.7186, decode.d4.loss_cls: 0.2426, decode.d4.loss_mask: 0.4748, decode.d4.loss_dice: 0.7187, decode.d5.loss_cls: 0.2406, decode.d5.loss_mask: 0.4736, decode.d5.loss_dice: 0.7159, decode.d6.loss_cls: 0.2379, decode.d6.loss_mask: 0.4716, decode.d6.loss_dice: 0.7160, decode.d7.loss_cls: 0.2373, decode.d7.loss_mask: 0.4735, decode.d7.loss_dice: 0.7157, loss: 14.4308
2022-10-30 23:06:15,687 - mmseg - INFO - Iter [13450/20000]	lr: 9.997e-07, eta: 6:29:55, time: 2.559, data_time: 0.023, memory: 35615, decode.loss_cls: 0.2365, decode.loss_mask: 0.4749, decode.loss_dice: 0.7025, decode.d0.loss_cls: 1.5416, decode.d0.loss_mask: 0.4962, decode.d0.loss_dice: 0.7706, decode.d1.loss_cls: 0.3017, decode.d1.loss_mask: 0.4879, decode.d1.loss_dice: 0.7335, decode.d2.loss_cls: 0.2638, decode.d2.loss_mask: 0.4807, decode.d2.loss_dice: 0.7149, decode.d3.loss_cls: 0.2505, decode.d3.loss_mask: 0.4764, decode.d3.loss_dice: 0.7060, decode.d4.loss_cls: 0.2429, decode.d4.loss_mask: 0.4755, decode.d4.loss_dice: 0.7055, decode.d5.loss_cls: 0.2398, decode.d5.loss_mask: 0.4745, decode.d5.loss_dice: 0.7047, decode.d6.loss_cls: 0.2392, decode.d6.loss_mask: 0.4734, decode.d6.loss_dice: 0.7010, decode.d7.loss_cls: 0.2371, decode.d7.loss_mask: 0.4739, decode.d7.loss_dice: 0.7027, loss: 14.3078
2022-10-30 23:08:19,270 - mmseg - INFO - Iter [13500/20000]	lr: 9.921e-07, eta: 6:25:15, time: 2.472, data_time: 0.071, memory: 35615, decode.loss_cls: 0.2420, decode.loss_mask: 0.4694, decode.loss_dice: 0.7082, decode.d0.loss_cls: 1.5591, decode.d0.loss_mask: 0.4969, decode.d0.loss_dice: 0.7810, decode.d1.loss_cls: 0.3141, decode.d1.loss_mask: 0.4846, decode.d1.loss_dice: 0.7445, decode.d2.loss_cls: 0.2701, decode.d2.loss_mask: 0.4764, decode.d2.loss_dice: 0.7250, decode.d3.loss_cls: 0.2545, decode.d3.loss_mask: 0.4731, decode.d3.loss_dice: 0.7131, decode.d4.loss_cls: 0.2492, decode.d4.loss_mask: 0.4716, decode.d4.loss_dice: 0.7128, decode.d5.loss_cls: 0.2437, decode.d5.loss_mask: 0.4713, decode.d5.loss_dice: 0.7082, decode.d6.loss_cls: 0.2436, decode.d6.loss_mask: 0.4697, decode.d6.loss_dice: 0.7087, decode.d7.loss_cls: 0.2407, decode.d7.loss_mask: 0.4694, decode.d7.loss_dice: 0.7082, loss: 14.4088
2022-10-30 23:10:23,996 - mmseg - INFO - Iter [13550/20000]	lr: 9.845e-07, eta: 6:20:40, time: 2.494, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2395, decode.loss_mask: 0.4689, decode.loss_dice: 0.7053, decode.d0.loss_cls: 1.5662, decode.d0.loss_mask: 0.4922, decode.d0.loss_dice: 0.7708, decode.d1.loss_cls: 0.3117, decode.d1.loss_mask: 0.4820, decode.d1.loss_dice: 0.7384, decode.d2.loss_cls: 0.2671, decode.d2.loss_mask: 0.4737, decode.d2.loss_dice: 0.7193, decode.d3.loss_cls: 0.2508, decode.d3.loss_mask: 0.4704, decode.d3.loss_dice: 0.7103, decode.d4.loss_cls: 0.2443, decode.d4.loss_mask: 0.4696, decode.d4.loss_dice: 0.7096, decode.d5.loss_cls: 0.2409, decode.d5.loss_mask: 0.4701, decode.d5.loss_dice: 0.7053, decode.d6.loss_cls: 0.2408, decode.d6.loss_mask: 0.4689, decode.d6.loss_dice: 0.7061, decode.d7.loss_cls: 0.2421, decode.d7.loss_mask: 0.4687, decode.d7.loss_dice: 0.7042, loss: 14.3372
2022-10-30 23:12:27,491 - mmseg - INFO - Iter [13600/20000]	lr: 9.768e-07, eta: 6:16:08, time: 2.470, data_time: 0.023, memory: 35615, decode.loss_cls: 0.2334, decode.loss_mask: 0.4700, decode.loss_dice: 0.7069, decode.d0.loss_cls: 1.5560, decode.d0.loss_mask: 0.4934, decode.d0.loss_dice: 0.7724, decode.d1.loss_cls: 0.3030, decode.d1.loss_mask: 0.4839, decode.d1.loss_dice: 0.7420, decode.d2.loss_cls: 0.2622, decode.d2.loss_mask: 0.4757, decode.d2.loss_dice: 0.7220, decode.d3.loss_cls: 0.2458, decode.d3.loss_mask: 0.4722, decode.d3.loss_dice: 0.7134, decode.d4.loss_cls: 0.2404, decode.d4.loss_mask: 0.4715, decode.d4.loss_dice: 0.7100, decode.d5.loss_cls: 0.2351, decode.d5.loss_mask: 0.4708, decode.d5.loss_dice: 0.7089, decode.d6.loss_cls: 0.2342, decode.d6.loss_mask: 0.4705, decode.d6.loss_dice: 0.7080, decode.d7.loss_cls: 0.2339, decode.d7.loss_mask: 0.4695, decode.d7.loss_dice: 0.7076, loss: 14.3125
2022-10-30 23:14:24,977 - mmseg - INFO - Iter [13650/20000]	lr: 9.692e-07, eta: 6:11:30, time: 2.350, data_time: 0.017, memory: 35615, decode.loss_cls: 0.2336, decode.loss_mask: 0.4780, decode.loss_dice: 0.7116, decode.d0.loss_cls: 1.5392, decode.d0.loss_mask: 0.5008, decode.d0.loss_dice: 0.7763, decode.d1.loss_cls: 0.3010, decode.d1.loss_mask: 0.4936, decode.d1.loss_dice: 0.7509, decode.d2.loss_cls: 0.2610, decode.d2.loss_mask: 0.4837, decode.d2.loss_dice: 0.7285, decode.d3.loss_cls: 0.2430, decode.d3.loss_mask: 0.4805, decode.d3.loss_dice: 0.7174, decode.d4.loss_cls: 0.2379, decode.d4.loss_mask: 0.4788, decode.d4.loss_dice: 0.7178, decode.d5.loss_cls: 0.2343, decode.d5.loss_mask: 0.4777, decode.d5.loss_dice: 0.7182, decode.d6.loss_cls: 0.2295, decode.d6.loss_mask: 0.4784, decode.d6.loss_dice: 0.7147, decode.d7.loss_cls: 0.2312, decode.d7.loss_mask: 0.4776, decode.d7.loss_dice: 0.7152, loss: 14.4103
2022-10-30 23:16:26,308 - mmseg - INFO - Iter [13700/20000]	lr: 9.616e-07, eta: 6:07:02, time: 2.427, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2408, decode.loss_mask: 0.4620, decode.loss_dice: 0.7077, decode.d0.loss_cls: 1.5448, decode.d0.loss_mask: 0.4885, decode.d0.loss_dice: 0.7792, decode.d1.loss_cls: 0.3061, decode.d1.loss_mask: 0.4800, decode.d1.loss_dice: 0.7453, decode.d2.loss_cls: 0.2666, decode.d2.loss_mask: 0.4698, decode.d2.loss_dice: 0.7244, decode.d3.loss_cls: 0.2509, decode.d3.loss_mask: 0.4663, decode.d3.loss_dice: 0.7125, decode.d4.loss_cls: 0.2471, decode.d4.loss_mask: 0.4632, decode.d4.loss_dice: 0.7157, decode.d5.loss_cls: 0.2405, decode.d5.loss_mask: 0.4642, decode.d5.loss_dice: 0.7133, decode.d6.loss_cls: 0.2397, decode.d6.loss_mask: 0.4635, decode.d6.loss_dice: 0.7106, decode.d7.loss_cls: 0.2386, decode.d7.loss_mask: 0.4627, decode.d7.loss_dice: 0.7108, loss: 14.3146
2022-10-30 23:18:34,614 - mmseg - INFO - Iter [13750/20000]	lr: 9.540e-07, eta: 6:02:50, time: 2.566, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2249, decode.loss_mask: 0.4726, decode.loss_dice: 0.7028, decode.d0.loss_cls: 1.5416, decode.d0.loss_mask: 0.4968, decode.d0.loss_dice: 0.7632, decode.d1.loss_cls: 0.2892, decode.d1.loss_mask: 0.4867, decode.d1.loss_dice: 0.7343, decode.d2.loss_cls: 0.2535, decode.d2.loss_mask: 0.4792, decode.d2.loss_dice: 0.7172, decode.d3.loss_cls: 0.2372, decode.d3.loss_mask: 0.4746, decode.d3.loss_dice: 0.7049, decode.d4.loss_cls: 0.2322, decode.d4.loss_mask: 0.4744, decode.d4.loss_dice: 0.7064, decode.d5.loss_cls: 0.2304, decode.d5.loss_mask: 0.4726, decode.d5.loss_dice: 0.7041, decode.d6.loss_cls: 0.2282, decode.d6.loss_mask: 0.4720, decode.d6.loss_dice: 0.7006, decode.d7.loss_cls: 0.2248, decode.d7.loss_mask: 0.4722, decode.d7.loss_dice: 0.7042, loss: 14.2008
2022-10-30 23:20:43,427 - mmseg - INFO - Iter [13800/20000]	lr: 9.463e-07, eta: 5:58:41, time: 2.576, data_time: 0.057, memory: 35615, decode.loss_cls: 0.2353, decode.loss_mask: 0.4842, decode.loss_dice: 0.7158, decode.d0.loss_cls: 1.5257, decode.d0.loss_mask: 0.5092, decode.d0.loss_dice: 0.7840, decode.d1.loss_cls: 0.3081, decode.d1.loss_mask: 0.5003, decode.d1.loss_dice: 0.7524, decode.d2.loss_cls: 0.2673, decode.d2.loss_mask: 0.4896, decode.d2.loss_dice: 0.7334, decode.d3.loss_cls: 0.2491, decode.d3.loss_mask: 0.4870, decode.d3.loss_dice: 0.7225, decode.d4.loss_cls: 0.2431, decode.d4.loss_mask: 0.4860, decode.d4.loss_dice: 0.7239, decode.d5.loss_cls: 0.2416, decode.d5.loss_mask: 0.4847, decode.d5.loss_dice: 0.7225, decode.d6.loss_cls: 0.2408, decode.d6.loss_mask: 0.4833, decode.d6.loss_dice: 0.7186, decode.d7.loss_cls: 0.2376, decode.d7.loss_mask: 0.4833, decode.d7.loss_dice: 0.7166, loss: 14.5457
2022-10-30 23:22:49,929 - mmseg - INFO - Iter [13850/20000]	lr: 9.387e-07, eta: 5:54:33, time: 2.530, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2255, decode.loss_mask: 0.4645, decode.loss_dice: 0.6967, decode.d0.loss_cls: 1.5320, decode.d0.loss_mask: 0.4860, decode.d0.loss_dice: 0.7576, decode.d1.loss_cls: 0.2959, decode.d1.loss_mask: 0.4764, decode.d1.loss_dice: 0.7289, decode.d2.loss_cls: 0.2556, decode.d2.loss_mask: 0.4696, decode.d2.loss_dice: 0.7099, decode.d3.loss_cls: 0.2386, decode.d3.loss_mask: 0.4646, decode.d3.loss_dice: 0.7003, decode.d4.loss_cls: 0.2344, decode.d4.loss_mask: 0.4649, decode.d4.loss_dice: 0.6994, decode.d5.loss_cls: 0.2282, decode.d5.loss_mask: 0.4650, decode.d5.loss_dice: 0.6982, decode.d6.loss_cls: 0.2252, decode.d6.loss_mask: 0.4650, decode.d6.loss_dice: 0.6971, decode.d7.loss_cls: 0.2259, decode.d7.loss_mask: 0.4638, decode.d7.loss_dice: 0.6968, loss: 14.0662
2022-10-30 23:24:58,111 - mmseg - INFO - Iter [13900/20000]	lr: 9.311e-07, eta: 5:50:30, time: 2.564, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2353, decode.loss_mask: 0.4681, decode.loss_dice: 0.7074, decode.d0.loss_cls: 1.5706, decode.d0.loss_mask: 0.4914, decode.d0.loss_dice: 0.7711, decode.d1.loss_cls: 0.3083, decode.d1.loss_mask: 0.4808, decode.d1.loss_dice: 0.7429, decode.d2.loss_cls: 0.2642, decode.d2.loss_mask: 0.4734, decode.d2.loss_dice: 0.7215, decode.d3.loss_cls: 0.2479, decode.d3.loss_mask: 0.4697, decode.d3.loss_dice: 0.7105, decode.d4.loss_cls: 0.2419, decode.d4.loss_mask: 0.4693, decode.d4.loss_dice: 0.7115, decode.d5.loss_cls: 0.2390, decode.d5.loss_mask: 0.4678, decode.d5.loss_dice: 0.7088, decode.d6.loss_cls: 0.2368, decode.d6.loss_mask: 0.4676, decode.d6.loss_dice: 0.7069, decode.d7.loss_cls: 0.2331, decode.d7.loss_mask: 0.4696, decode.d7.loss_dice: 0.7089, loss: 14.3243
2022-10-30 23:27:08,512 - mmseg - INFO - Iter [13950/20000]	lr: 9.234e-07, eta: 5:46:33, time: 2.607, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2344, decode.loss_mask: 0.4713, decode.loss_dice: 0.7165, decode.d0.loss_cls: 1.5347, decode.d0.loss_mask: 0.4950, decode.d0.loss_dice: 0.7843, decode.d1.loss_cls: 0.3095, decode.d1.loss_mask: 0.4851, decode.d1.loss_dice: 0.7488, decode.d2.loss_cls: 0.2652, decode.d2.loss_mask: 0.4778, decode.d2.loss_dice: 0.7307, decode.d3.loss_cls: 0.2472, decode.d3.loss_mask: 0.4737, decode.d3.loss_dice: 0.7223, decode.d4.loss_cls: 0.2410, decode.d4.loss_mask: 0.4723, decode.d4.loss_dice: 0.7214, decode.d5.loss_cls: 0.2378, decode.d5.loss_mask: 0.4719, decode.d5.loss_dice: 0.7203, decode.d6.loss_cls: 0.2362, decode.d6.loss_mask: 0.4709, decode.d6.loss_dice: 0.7176, decode.d7.loss_cls: 0.2377, decode.d7.loss_mask: 0.4722, decode.d7.loss_dice: 0.7159, loss: 14.4117
2022-10-30 23:29:11,594 - mmseg - INFO - Saving checkpoint at 14000 iterations
2022-10-30 23:29:45,132 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-30 23:29:45,134 - mmseg - INFO - Iter [14000/20000]	lr: 9.158e-07, eta: 5:43:18, time: 3.133, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2371, decode.loss_mask: 0.4675, decode.loss_dice: 0.7178, decode.d0.loss_cls: 1.5425, decode.d0.loss_mask: 0.4916, decode.d0.loss_dice: 0.7862, decode.d1.loss_cls: 0.3062, decode.d1.loss_mask: 0.4822, decode.d1.loss_dice: 0.7555, decode.d2.loss_cls: 0.2662, decode.d2.loss_mask: 0.4741, decode.d2.loss_dice: 0.7340, decode.d3.loss_cls: 0.2496, decode.d3.loss_mask: 0.4710, decode.d3.loss_dice: 0.7241, decode.d4.loss_cls: 0.2440, decode.d4.loss_mask: 0.4698, decode.d4.loss_dice: 0.7241, decode.d5.loss_cls: 0.2397, decode.d5.loss_mask: 0.4684, decode.d5.loss_dice: 0.7223, decode.d6.loss_cls: 0.2384, decode.d6.loss_mask: 0.4673, decode.d6.loss_dice: 0.7234, decode.d7.loss_cls: 0.2362, decode.d7.loss_mask: 0.4677, decode.d7.loss_dice: 0.7223, loss: 14.4293
2022-10-30 23:31:48,279 - mmseg - INFO - Iter [14050/20000]	lr: 9.082e-07, eta: 5:39:16, time: 2.463, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2318, decode.loss_mask: 0.4683, decode.loss_dice: 0.7170, decode.d0.loss_cls: 1.5341, decode.d0.loss_mask: 0.4902, decode.d0.loss_dice: 0.7784, decode.d1.loss_cls: 0.2977, decode.d1.loss_mask: 0.4820, decode.d1.loss_dice: 0.7522, decode.d2.loss_cls: 0.2606, decode.d2.loss_mask: 0.4712, decode.d2.loss_dice: 0.7344, decode.d3.loss_cls: 0.2412, decode.d3.loss_mask: 0.4699, decode.d3.loss_dice: 0.7218, decode.d4.loss_cls: 0.2368, decode.d4.loss_mask: 0.4696, decode.d4.loss_dice: 0.7187, decode.d5.loss_cls: 0.2335, decode.d5.loss_mask: 0.4683, decode.d5.loss_dice: 0.7169, decode.d6.loss_cls: 0.2320, decode.d6.loss_mask: 0.4681, decode.d6.loss_dice: 0.7175, decode.d7.loss_cls: 0.2304, decode.d7.loss_mask: 0.4683, decode.d7.loss_dice: 0.7168, loss: 14.3277
2022-10-30 23:33:48,234 - mmseg - INFO - Iter [14100/20000]	lr: 9.005e-07, eta: 5:35:11, time: 2.399, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2365, decode.loss_mask: 0.4668, decode.loss_dice: 0.6985, decode.d0.loss_cls: 1.5565, decode.d0.loss_mask: 0.4945, decode.d0.loss_dice: 0.7696, decode.d1.loss_cls: 0.3011, decode.d1.loss_mask: 0.4815, decode.d1.loss_dice: 0.7352, decode.d2.loss_cls: 0.2646, decode.d2.loss_mask: 0.4747, decode.d2.loss_dice: 0.7135, decode.d3.loss_cls: 0.2457, decode.d3.loss_mask: 0.4699, decode.d3.loss_dice: 0.7062, decode.d4.loss_cls: 0.2408, decode.d4.loss_mask: 0.4681, decode.d4.loss_dice: 0.7046, decode.d5.loss_cls: 0.2344, decode.d5.loss_mask: 0.4667, decode.d5.loss_dice: 0.7033, decode.d6.loss_cls: 0.2364, decode.d6.loss_mask: 0.4654, decode.d6.loss_dice: 0.7011, decode.d7.loss_cls: 0.2345, decode.d7.loss_mask: 0.4660, decode.d7.loss_dice: 0.6995, loss: 14.2357
2022-10-30 23:35:52,364 - mmseg - INFO - Iter [14150/20000]	lr: 8.929e-07, eta: 5:31:15, time: 2.483, data_time: 0.068, memory: 35615, decode.loss_cls: 0.2295, decode.loss_mask: 0.4630, decode.loss_dice: 0.7002, decode.d0.loss_cls: 1.5593, decode.d0.loss_mask: 0.4907, decode.d0.loss_dice: 0.7653, decode.d1.loss_cls: 0.2995, decode.d1.loss_mask: 0.4798, decode.d1.loss_dice: 0.7361, decode.d2.loss_cls: 0.2632, decode.d2.loss_mask: 0.4699, decode.d2.loss_dice: 0.7154, decode.d3.loss_cls: 0.2426, decode.d3.loss_mask: 0.4663, decode.d3.loss_dice: 0.7054, decode.d4.loss_cls: 0.2375, decode.d4.loss_mask: 0.4645, decode.d4.loss_dice: 0.7049, decode.d5.loss_cls: 0.2325, decode.d5.loss_mask: 0.4630, decode.d5.loss_dice: 0.7033, decode.d6.loss_cls: 0.2285, decode.d6.loss_mask: 0.4627, decode.d6.loss_dice: 0.7001, decode.d7.loss_cls: 0.2296, decode.d7.loss_mask: 0.4627, decode.d7.loss_dice: 0.7015, loss: 14.1771
2022-10-30 23:37:54,437 - mmseg - INFO - Iter [14200/20000]	lr: 8.853e-07, eta: 5:27:19, time: 2.441, data_time: 0.016, memory: 35615, decode.loss_cls: 0.2233, decode.loss_mask: 0.4689, decode.loss_dice: 0.7099, decode.d0.loss_cls: 1.5341, decode.d0.loss_mask: 0.4918, decode.d0.loss_dice: 0.7719, decode.d1.loss_cls: 0.2919, decode.d1.loss_mask: 0.4817, decode.d1.loss_dice: 0.7421, decode.d2.loss_cls: 0.2522, decode.d2.loss_mask: 0.4723, decode.d2.loss_dice: 0.7238, decode.d3.loss_cls: 0.2350, decode.d3.loss_mask: 0.4697, decode.d3.loss_dice: 0.7156, decode.d4.loss_cls: 0.2303, decode.d4.loss_mask: 0.4688, decode.d4.loss_dice: 0.7148, decode.d5.loss_cls: 0.2242, decode.d5.loss_mask: 0.4694, decode.d5.loss_dice: 0.7128, decode.d6.loss_cls: 0.2251, decode.d6.loss_mask: 0.4689, decode.d6.loss_dice: 0.7097, decode.d7.loss_cls: 0.2212, decode.d7.loss_mask: 0.4705, decode.d7.loss_dice: 0.7114, loss: 14.2115
2022-10-30 23:39:54,967 - mmseg - INFO - Iter [14250/20000]	lr: 8.776e-07, eta: 5:23:24, time: 2.411, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2318, decode.loss_mask: 0.4768, decode.loss_dice: 0.7103, decode.d0.loss_cls: 1.5336, decode.d0.loss_mask: 0.5001, decode.d0.loss_dice: 0.7785, decode.d1.loss_cls: 0.3006, decode.d1.loss_mask: 0.4914, decode.d1.loss_dice: 0.7448, decode.d2.loss_cls: 0.2622, decode.d2.loss_mask: 0.4818, decode.d2.loss_dice: 0.7234, decode.d3.loss_cls: 0.2448, decode.d3.loss_mask: 0.4802, decode.d3.loss_dice: 0.7151, decode.d4.loss_cls: 0.2402, decode.d4.loss_mask: 0.4796, decode.d4.loss_dice: 0.7130, decode.d5.loss_cls: 0.2343, decode.d5.loss_mask: 0.4774, decode.d5.loss_dice: 0.7101, decode.d6.loss_cls: 0.2325, decode.d6.loss_mask: 0.4760, decode.d6.loss_dice: 0.7106, decode.d7.loss_cls: 0.2318, decode.d7.loss_mask: 0.4758, decode.d7.loss_dice: 0.7096, loss: 14.3666
2022-10-30 23:42:04,169 - mmseg - INFO - Iter [14300/20000]	lr: 8.700e-07, eta: 5:19:43, time: 2.584, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2303, decode.loss_mask: 0.4664, decode.loss_dice: 0.7021, decode.d0.loss_cls: 1.5178, decode.d0.loss_mask: 0.4923, decode.d0.loss_dice: 0.7713, decode.d1.loss_cls: 0.2949, decode.d1.loss_mask: 0.4789, decode.d1.loss_dice: 0.7384, decode.d2.loss_cls: 0.2580, decode.d2.loss_mask: 0.4721, decode.d2.loss_dice: 0.7143, decode.d3.loss_cls: 0.2417, decode.d3.loss_mask: 0.4681, decode.d3.loss_dice: 0.7060, decode.d4.loss_cls: 0.2367, decode.d4.loss_mask: 0.4674, decode.d4.loss_dice: 0.7060, decode.d5.loss_cls: 0.2318, decode.d5.loss_mask: 0.4666, decode.d5.loss_dice: 0.7034, decode.d6.loss_cls: 0.2314, decode.d6.loss_mask: 0.4664, decode.d6.loss_dice: 0.7024, decode.d7.loss_cls: 0.2292, decode.d7.loss_mask: 0.4669, decode.d7.loss_dice: 0.7021, loss: 14.1629
2022-10-30 23:44:05,594 - mmseg - INFO - Iter [14350/20000]	lr: 8.624e-07, eta: 5:15:54, time: 2.428, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2347, decode.loss_mask: 0.4738, decode.loss_dice: 0.7141, decode.d0.loss_cls: 1.5246, decode.d0.loss_mask: 0.5010, decode.d0.loss_dice: 0.7791, decode.d1.loss_cls: 0.2990, decode.d1.loss_mask: 0.4902, decode.d1.loss_dice: 0.7492, decode.d2.loss_cls: 0.2607, decode.d2.loss_mask: 0.4813, decode.d2.loss_dice: 0.7302, decode.d3.loss_cls: 0.2445, decode.d3.loss_mask: 0.4771, decode.d3.loss_dice: 0.7194, decode.d4.loss_cls: 0.2402, decode.d4.loss_mask: 0.4748, decode.d4.loss_dice: 0.7176, decode.d5.loss_cls: 0.2358, decode.d5.loss_mask: 0.4750, decode.d5.loss_dice: 0.7158, decode.d6.loss_cls: 0.2338, decode.d6.loss_mask: 0.4738, decode.d6.loss_dice: 0.7129, decode.d7.loss_cls: 0.2339, decode.d7.loss_mask: 0.4743, decode.d7.loss_dice: 0.7126, loss: 14.3796
2022-10-30 23:46:10,542 - mmseg - INFO - Iter [14400/20000]	lr: 8.548e-07, eta: 5:12:12, time: 2.499, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2320, decode.loss_mask: 0.4648, decode.loss_dice: 0.7077, decode.d0.loss_cls: 1.5283, decode.d0.loss_mask: 0.4929, decode.d0.loss_dice: 0.7756, decode.d1.loss_cls: 0.3008, decode.d1.loss_mask: 0.4799, decode.d1.loss_dice: 0.7427, decode.d2.loss_cls: 0.2609, decode.d2.loss_mask: 0.4705, decode.d2.loss_dice: 0.7231, decode.d3.loss_cls: 0.2455, decode.d3.loss_mask: 0.4669, decode.d3.loss_dice: 0.7123, decode.d4.loss_cls: 0.2404, decode.d4.loss_mask: 0.4658, decode.d4.loss_dice: 0.7120, decode.d5.loss_cls: 0.2352, decode.d5.loss_mask: 0.4652, decode.d5.loss_dice: 0.7094, decode.d6.loss_cls: 0.2339, decode.d6.loss_mask: 0.4641, decode.d6.loss_dice: 0.7076, decode.d7.loss_cls: 0.2317, decode.d7.loss_mask: 0.4646, decode.d7.loss_dice: 0.7071, loss: 14.2407
2022-10-30 23:48:17,536 - mmseg - INFO - Iter [14450/20000]	lr: 8.471e-07, eta: 5:08:34, time: 2.540, data_time: 0.063, memory: 35615, decode.loss_cls: 0.2211, decode.loss_mask: 0.4519, decode.loss_dice: 0.6973, decode.d0.loss_cls: 1.5458, decode.d0.loss_mask: 0.4740, decode.d0.loss_dice: 0.7573, decode.d1.loss_cls: 0.2948, decode.d1.loss_mask: 0.4637, decode.d1.loss_dice: 0.7309, decode.d2.loss_cls: 0.2498, decode.d2.loss_mask: 0.4584, decode.d2.loss_dice: 0.7114, decode.d3.loss_cls: 0.2331, decode.d3.loss_mask: 0.4550, decode.d3.loss_dice: 0.7034, decode.d4.loss_cls: 0.2266, decode.d4.loss_mask: 0.4534, decode.d4.loss_dice: 0.7020, decode.d5.loss_cls: 0.2252, decode.d5.loss_mask: 0.4517, decode.d5.loss_dice: 0.6984, decode.d6.loss_cls: 0.2238, decode.d6.loss_mask: 0.4516, decode.d6.loss_dice: 0.6933, decode.d7.loss_cls: 0.2224, decode.d7.loss_mask: 0.4518, decode.d7.loss_dice: 0.6960, loss: 13.9443
2022-10-30 23:50:22,708 - mmseg - INFO - Iter [14500/20000]	lr: 8.395e-07, eta: 5:04:57, time: 2.504, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2244, decode.loss_mask: 0.4612, decode.loss_dice: 0.6991, decode.d0.loss_cls: 1.5356, decode.d0.loss_mask: 0.4849, decode.d0.loss_dice: 0.7634, decode.d1.loss_cls: 0.2981, decode.d1.loss_mask: 0.4760, decode.d1.loss_dice: 0.7322, decode.d2.loss_cls: 0.2539, decode.d2.loss_mask: 0.4684, decode.d2.loss_dice: 0.7149, decode.d3.loss_cls: 0.2371, decode.d3.loss_mask: 0.4639, decode.d3.loss_dice: 0.7038, decode.d4.loss_cls: 0.2327, decode.d4.loss_mask: 0.4626, decode.d4.loss_dice: 0.7014, decode.d5.loss_cls: 0.2275, decode.d5.loss_mask: 0.4626, decode.d5.loss_dice: 0.6981, decode.d6.loss_cls: 0.2283, decode.d6.loss_mask: 0.4604, decode.d6.loss_dice: 0.6970, decode.d7.loss_cls: 0.2233, decode.d7.loss_mask: 0.4620, decode.d7.loss_dice: 0.6994, loss: 14.0719
2022-10-30 23:52:30,925 - mmseg - INFO - Iter [14550/20000]	lr: 8.319e-07, eta: 5:01:25, time: 2.564, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2361, decode.loss_mask: 0.4690, decode.loss_dice: 0.7032, decode.d0.loss_cls: 1.5178, decode.d0.loss_mask: 0.4927, decode.d0.loss_dice: 0.7696, decode.d1.loss_cls: 0.3020, decode.d1.loss_mask: 0.4838, decode.d1.loss_dice: 0.7355, decode.d2.loss_cls: 0.2641, decode.d2.loss_mask: 0.4763, decode.d2.loss_dice: 0.7208, decode.d3.loss_cls: 0.2463, decode.d3.loss_mask: 0.4722, decode.d3.loss_dice: 0.7089, decode.d4.loss_cls: 0.2419, decode.d4.loss_mask: 0.4700, decode.d4.loss_dice: 0.7094, decode.d5.loss_cls: 0.2391, decode.d5.loss_mask: 0.4694, decode.d5.loss_dice: 0.7079, decode.d6.loss_cls: 0.2348, decode.d6.loss_mask: 0.4687, decode.d6.loss_dice: 0.7041, decode.d7.loss_cls: 0.2336, decode.d7.loss_mask: 0.4688, decode.d7.loss_dice: 0.7049, loss: 14.2509
2022-10-30 23:54:37,294 - mmseg - INFO - Iter [14600/20000]	lr: 8.242e-07, eta: 4:57:52, time: 2.527, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2287, decode.loss_mask: 0.4669, decode.loss_dice: 0.7014, decode.d0.loss_cls: 1.5071, decode.d0.loss_mask: 0.4931, decode.d0.loss_dice: 0.7693, decode.d1.loss_cls: 0.2942, decode.d1.loss_mask: 0.4796, decode.d1.loss_dice: 0.7379, decode.d2.loss_cls: 0.2598, decode.d2.loss_mask: 0.4725, decode.d2.loss_dice: 0.7156, decode.d3.loss_cls: 0.2415, decode.d3.loss_mask: 0.4689, decode.d3.loss_dice: 0.7082, decode.d4.loss_cls: 0.2382, decode.d4.loss_mask: 0.4688, decode.d4.loss_dice: 0.7061, decode.d5.loss_cls: 0.2337, decode.d5.loss_mask: 0.4686, decode.d5.loss_dice: 0.7039, decode.d6.loss_cls: 0.2330, decode.d6.loss_mask: 0.4665, decode.d6.loss_dice: 0.7005, decode.d7.loss_cls: 0.2303, decode.d7.loss_mask: 0.4661, decode.d7.loss_dice: 0.7016, loss: 14.1621
2022-10-30 23:56:39,361 - mmseg - INFO - Iter [14650/20000]	lr: 8.166e-07, eta: 4:54:17, time: 2.441, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2291, decode.loss_mask: 0.4559, decode.loss_dice: 0.6923, decode.d0.loss_cls: 1.5358, decode.d0.loss_mask: 0.4796, decode.d0.loss_dice: 0.7632, decode.d1.loss_cls: 0.3010, decode.d1.loss_mask: 0.4707, decode.d1.loss_dice: 0.7275, decode.d2.loss_cls: 0.2613, decode.d2.loss_mask: 0.4609, decode.d2.loss_dice: 0.7084, decode.d3.loss_cls: 0.2452, decode.d3.loss_mask: 0.4581, decode.d3.loss_dice: 0.7001, decode.d4.loss_cls: 0.2381, decode.d4.loss_mask: 0.4565, decode.d4.loss_dice: 0.6962, decode.d5.loss_cls: 0.2346, decode.d5.loss_mask: 0.4563, decode.d5.loss_dice: 0.6955, decode.d6.loss_cls: 0.2340, decode.d6.loss_mask: 0.4550, decode.d6.loss_dice: 0.6931, decode.d7.loss_cls: 0.2325, decode.d7.loss_mask: 0.4554, decode.d7.loss_dice: 0.6950, loss: 14.0313
2022-10-30 23:58:50,323 - mmseg - INFO - Iter [14700/20000]	lr: 8.090e-07, eta: 4:50:53, time: 2.619, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2327, decode.loss_mask: 0.4653, decode.loss_dice: 0.7116, decode.d0.loss_cls: 1.5244, decode.d0.loss_mask: 0.4912, decode.d0.loss_dice: 0.7805, decode.d1.loss_cls: 0.3003, decode.d1.loss_mask: 0.4793, decode.d1.loss_dice: 0.7442, decode.d2.loss_cls: 0.2586, decode.d2.loss_mask: 0.4710, decode.d2.loss_dice: 0.7286, decode.d3.loss_cls: 0.2434, decode.d3.loss_mask: 0.4672, decode.d3.loss_dice: 0.7134, decode.d4.loss_cls: 0.2373, decode.d4.loss_mask: 0.4676, decode.d4.loss_dice: 0.7135, decode.d5.loss_cls: 0.2339, decode.d5.loss_mask: 0.4669, decode.d5.loss_dice: 0.7156, decode.d6.loss_cls: 0.2340, decode.d6.loss_mask: 0.4655, decode.d6.loss_dice: 0.7137, decode.d7.loss_cls: 0.2320, decode.d7.loss_mask: 0.4646, decode.d7.loss_dice: 0.7119, loss: 14.2680
2022-10-31 00:00:59,303 - mmseg - INFO - Iter [14750/20000]	lr: 8.013e-07, eta: 4:47:29, time: 2.580, data_time: 0.059, memory: 35615, decode.loss_cls: 0.2322, decode.loss_mask: 0.4654, decode.loss_dice: 0.7011, decode.d0.loss_cls: 1.5329, decode.d0.loss_mask: 0.4935, decode.d0.loss_dice: 0.7695, decode.d1.loss_cls: 0.2955, decode.d1.loss_mask: 0.4802, decode.d1.loss_dice: 0.7387, decode.d2.loss_cls: 0.2557, decode.d2.loss_mask: 0.4727, decode.d2.loss_dice: 0.7198, decode.d3.loss_cls: 0.2423, decode.d3.loss_mask: 0.4673, decode.d3.loss_dice: 0.7055, decode.d4.loss_cls: 0.2366, decode.d4.loss_mask: 0.4673, decode.d4.loss_dice: 0.7067, decode.d5.loss_cls: 0.2340, decode.d5.loss_mask: 0.4657, decode.d5.loss_dice: 0.7024, decode.d6.loss_cls: 0.2307, decode.d6.loss_mask: 0.4653, decode.d6.loss_dice: 0.7033, decode.d7.loss_cls: 0.2300, decode.d7.loss_mask: 0.4656, decode.d7.loss_dice: 0.7009, loss: 14.1807
2022-10-31 00:03:03,714 - mmseg - INFO - Iter [14800/20000]	lr: 7.937e-07, eta: 4:44:02, time: 2.488, data_time: 0.014, memory: 35615, decode.loss_cls: 0.2296, decode.loss_mask: 0.4581, decode.loss_dice: 0.7019, decode.d0.loss_cls: 1.5427, decode.d0.loss_mask: 0.4812, decode.d0.loss_dice: 0.7702, decode.d1.loss_cls: 0.2990, decode.d1.loss_mask: 0.4720, decode.d1.loss_dice: 0.7372, decode.d2.loss_cls: 0.2601, decode.d2.loss_mask: 0.4634, decode.d2.loss_dice: 0.7154, decode.d3.loss_cls: 0.2432, decode.d3.loss_mask: 0.4600, decode.d3.loss_dice: 0.7067, decode.d4.loss_cls: 0.2373, decode.d4.loss_mask: 0.4592, decode.d4.loss_dice: 0.7066, decode.d5.loss_cls: 0.2330, decode.d5.loss_mask: 0.4573, decode.d5.loss_dice: 0.7041, decode.d6.loss_cls: 0.2317, decode.d6.loss_mask: 0.4571, decode.d6.loss_dice: 0.7006, decode.d7.loss_cls: 0.2309, decode.d7.loss_mask: 0.4562, decode.d7.loss_dice: 0.7021, loss: 14.1168
2022-10-31 00:05:05,989 - mmseg - INFO - Iter [14850/20000]	lr: 7.861e-07, eta: 4:40:34, time: 2.445, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2317, decode.loss_mask: 0.4621, decode.loss_dice: 0.7028, decode.d0.loss_cls: 1.5185, decode.d0.loss_mask: 0.4908, decode.d0.loss_dice: 0.7719, decode.d1.loss_cls: 0.3005, decode.d1.loss_mask: 0.4794, decode.d1.loss_dice: 0.7409, decode.d2.loss_cls: 0.2623, decode.d2.loss_mask: 0.4709, decode.d2.loss_dice: 0.7154, decode.d3.loss_cls: 0.2416, decode.d3.loss_mask: 0.4656, decode.d3.loss_dice: 0.7077, decode.d4.loss_cls: 0.2370, decode.d4.loss_mask: 0.4640, decode.d4.loss_dice: 0.7066, decode.d5.loss_cls: 0.2327, decode.d5.loss_mask: 0.4638, decode.d5.loss_dice: 0.7048, decode.d6.loss_cls: 0.2314, decode.d6.loss_mask: 0.4628, decode.d6.loss_dice: 0.7012, decode.d7.loss_cls: 0.2301, decode.d7.loss_mask: 0.4630, decode.d7.loss_dice: 0.7019, loss: 14.1615
2022-10-31 00:07:09,527 - mmseg - INFO - Iter [14900/20000]	lr: 7.785e-07, eta: 4:37:09, time: 2.471, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2279, decode.loss_mask: 0.4673, decode.loss_dice: 0.7032, decode.d0.loss_cls: 1.5170, decode.d0.loss_mask: 0.4928, decode.d0.loss_dice: 0.7695, decode.d1.loss_cls: 0.2985, decode.d1.loss_mask: 0.4826, decode.d1.loss_dice: 0.7387, decode.d2.loss_cls: 0.2594, decode.d2.loss_mask: 0.4741, decode.d2.loss_dice: 0.7177, decode.d3.loss_cls: 0.2441, decode.d3.loss_mask: 0.4702, decode.d3.loss_dice: 0.7071, decode.d4.loss_cls: 0.2369, decode.d4.loss_mask: 0.4687, decode.d4.loss_dice: 0.7050, decode.d5.loss_cls: 0.2339, decode.d5.loss_mask: 0.4670, decode.d5.loss_dice: 0.7019, decode.d6.loss_cls: 0.2309, decode.d6.loss_mask: 0.4669, decode.d6.loss_dice: 0.7027, decode.d7.loss_cls: 0.2306, decode.d7.loss_mask: 0.4666, decode.d7.loss_dice: 0.7027, loss: 14.1841
2022-10-31 00:09:12,924 - mmseg - INFO - Iter [14950/20000]	lr: 7.708e-07, eta: 4:33:45, time: 2.468, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2217, decode.loss_mask: 0.4690, decode.loss_dice: 0.7023, decode.d0.loss_cls: 1.5155, decode.d0.loss_mask: 0.4945, decode.d0.loss_dice: 0.7664, decode.d1.loss_cls: 0.2881, decode.d1.loss_mask: 0.4859, decode.d1.loss_dice: 0.7368, decode.d2.loss_cls: 0.2510, decode.d2.loss_mask: 0.4765, decode.d2.loss_dice: 0.7151, decode.d3.loss_cls: 0.2348, decode.d3.loss_mask: 0.4714, decode.d3.loss_dice: 0.7066, decode.d4.loss_cls: 0.2291, decode.d4.loss_mask: 0.4706, decode.d4.loss_dice: 0.7046, decode.d5.loss_cls: 0.2262, decode.d5.loss_mask: 0.4703, decode.d5.loss_dice: 0.7032, decode.d6.loss_cls: 0.2213, decode.d6.loss_mask: 0.4693, decode.d6.loss_dice: 0.7022, decode.d7.loss_cls: 0.2228, decode.d7.loss_mask: 0.4691, decode.d7.loss_dice: 0.7009, loss: 14.1251
2022-10-31 00:11:14,259 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-31 00:11:14,260 - mmseg - INFO - Iter [15000/20000]	lr: 7.632e-07, eta: 4:30:21, time: 2.427, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2325, decode.loss_mask: 0.4595, decode.loss_dice: 0.7007, decode.d0.loss_cls: 1.5378, decode.d0.loss_mask: 0.4867, decode.d0.loss_dice: 0.7719, decode.d1.loss_cls: 0.2990, decode.d1.loss_mask: 0.4760, decode.d1.loss_dice: 0.7377, decode.d2.loss_cls: 0.2602, decode.d2.loss_mask: 0.4660, decode.d2.loss_dice: 0.7174, decode.d3.loss_cls: 0.2461, decode.d3.loss_mask: 0.4639, decode.d3.loss_dice: 0.7053, decode.d4.loss_cls: 0.2386, decode.d4.loss_mask: 0.4633, decode.d4.loss_dice: 0.7048, decode.d5.loss_cls: 0.2346, decode.d5.loss_mask: 0.4608, decode.d5.loss_dice: 0.7021, decode.d6.loss_cls: 0.2314, decode.d6.loss_mask: 0.4600, decode.d6.loss_dice: 0.7029, decode.d7.loss_cls: 0.2319, decode.d7.loss_mask: 0.4605, decode.d7.loss_dice: 0.7023, loss: 14.1538
2022-10-31 00:13:13,560 - mmseg - INFO - Iter [15050/20000]	lr: 7.556e-07, eta: 4:26:57, time: 2.386, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2310, decode.loss_mask: 0.4689, decode.loss_dice: 0.7109, decode.d0.loss_cls: 1.5167, decode.d0.loss_mask: 0.4946, decode.d0.loss_dice: 0.7803, decode.d1.loss_cls: 0.2965, decode.d1.loss_mask: 0.4852, decode.d1.loss_dice: 0.7495, decode.d2.loss_cls: 0.2583, decode.d2.loss_mask: 0.4772, decode.d2.loss_dice: 0.7268, decode.d3.loss_cls: 0.2423, decode.d3.loss_mask: 0.4719, decode.d3.loss_dice: 0.7188, decode.d4.loss_cls: 0.2350, decode.d4.loss_mask: 0.4714, decode.d4.loss_dice: 0.7155, decode.d5.loss_cls: 0.2300, decode.d5.loss_mask: 0.4707, decode.d5.loss_dice: 0.7144, decode.d6.loss_cls: 0.2286, decode.d6.loss_mask: 0.4692, decode.d6.loss_dice: 0.7084, decode.d7.loss_cls: 0.2275, decode.d7.loss_mask: 0.4701, decode.d7.loss_dice: 0.7100, loss: 14.2798
2022-10-31 00:15:13,010 - mmseg - INFO - Iter [15100/20000]	lr: 7.479e-07, eta: 4:23:35, time: 2.389, data_time: 0.072, memory: 35615, decode.loss_cls: 0.2268, decode.loss_mask: 0.4692, decode.loss_dice: 0.7039, decode.d0.loss_cls: 1.5169, decode.d0.loss_mask: 0.4965, decode.d0.loss_dice: 0.7704, decode.d1.loss_cls: 0.2919, decode.d1.loss_mask: 0.4853, decode.d1.loss_dice: 0.7385, decode.d2.loss_cls: 0.2537, decode.d2.loss_mask: 0.4763, decode.d2.loss_dice: 0.7167, decode.d3.loss_cls: 0.2349, decode.d3.loss_mask: 0.4724, decode.d3.loss_dice: 0.7086, decode.d4.loss_cls: 0.2326, decode.d4.loss_mask: 0.4702, decode.d4.loss_dice: 0.7090, decode.d5.loss_cls: 0.2273, decode.d5.loss_mask: 0.4708, decode.d5.loss_dice: 0.7073, decode.d6.loss_cls: 0.2260, decode.d6.loss_mask: 0.4695, decode.d6.loss_dice: 0.7029, decode.d7.loss_cls: 0.2236, decode.d7.loss_mask: 0.4691, decode.d7.loss_dice: 0.7046, loss: 14.1746
2022-10-31 00:17:06,001 - mmseg - INFO - Iter [15150/20000]	lr: 7.403e-07, eta: 4:20:08, time: 2.260, data_time: 0.011, memory: 35615, decode.loss_cls: 0.2227, decode.loss_mask: 0.4619, decode.loss_dice: 0.6986, decode.d0.loss_cls: 1.5052, decode.d0.loss_mask: 0.4893, decode.d0.loss_dice: 0.7674, decode.d1.loss_cls: 0.2883, decode.d1.loss_mask: 0.4772, decode.d1.loss_dice: 0.7364, decode.d2.loss_cls: 0.2523, decode.d2.loss_mask: 0.4687, decode.d2.loss_dice: 0.7166, decode.d3.loss_cls: 0.2353, decode.d3.loss_mask: 0.4646, decode.d3.loss_dice: 0.7059, decode.d4.loss_cls: 0.2295, decode.d4.loss_mask: 0.4655, decode.d4.loss_dice: 0.7070, decode.d5.loss_cls: 0.2270, decode.d5.loss_mask: 0.4626, decode.d5.loss_dice: 0.7021, decode.d6.loss_cls: 0.2241, decode.d6.loss_mask: 0.4621, decode.d6.loss_dice: 0.7030, decode.d7.loss_cls: 0.2231, decode.d7.loss_mask: 0.4616, decode.d7.loss_dice: 0.7004, loss: 14.0585
2022-10-31 00:19:16,737 - mmseg - INFO - Iter [15200/20000]	lr: 7.327e-07, eta: 4:16:59, time: 2.615, data_time: 0.040, memory: 35615, decode.loss_cls: 0.2277, decode.loss_mask: 0.4590, decode.loss_dice: 0.6982, decode.d0.loss_cls: 1.5210, decode.d0.loss_mask: 0.4853, decode.d0.loss_dice: 0.7660, decode.d1.loss_cls: 0.2973, decode.d1.loss_mask: 0.4724, decode.d1.loss_dice: 0.7360, decode.d2.loss_cls: 0.2555, decode.d2.loss_mask: 0.4648, decode.d2.loss_dice: 0.7143, decode.d3.loss_cls: 0.2390, decode.d3.loss_mask: 0.4629, decode.d3.loss_dice: 0.7054, decode.d4.loss_cls: 0.2328, decode.d4.loss_mask: 0.4605, decode.d4.loss_dice: 0.7018, decode.d5.loss_cls: 0.2302, decode.d5.loss_mask: 0.4583, decode.d5.loss_dice: 0.7008, decode.d6.loss_cls: 0.2298, decode.d6.loss_mask: 0.4588, decode.d6.loss_dice: 0.7014, decode.d7.loss_cls: 0.2276, decode.d7.loss_mask: 0.4595, decode.d7.loss_dice: 0.6975, loss: 14.0638
2022-10-31 00:21:20,754 - mmseg - INFO - Iter [15250/20000]	lr: 7.250e-07, eta: 4:13:45, time: 2.480, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2252, decode.loss_mask: 0.4608, decode.loss_dice: 0.7023, decode.d0.loss_cls: 1.5191, decode.d0.loss_mask: 0.4849, decode.d0.loss_dice: 0.7668, decode.d1.loss_cls: 0.2933, decode.d1.loss_mask: 0.4752, decode.d1.loss_dice: 0.7337, decode.d2.loss_cls: 0.2553, decode.d2.loss_mask: 0.4671, decode.d2.loss_dice: 0.7169, decode.d3.loss_cls: 0.2363, decode.d3.loss_mask: 0.4645, decode.d3.loss_dice: 0.7069, decode.d4.loss_cls: 0.2320, decode.d4.loss_mask: 0.4628, decode.d4.loss_dice: 0.7057, decode.d5.loss_cls: 0.2275, decode.d5.loss_mask: 0.4633, decode.d5.loss_dice: 0.7069, decode.d6.loss_cls: 0.2278, decode.d6.loss_mask: 0.4608, decode.d6.loss_dice: 0.7023, decode.d7.loss_cls: 0.2253, decode.d7.loss_mask: 0.4619, decode.d7.loss_dice: 0.7008, loss: 14.0854
2022-10-31 00:23:24,357 - mmseg - INFO - Iter [15300/20000]	lr: 7.174e-07, eta: 4:10:32, time: 2.472, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2289, decode.loss_mask: 0.4573, decode.loss_dice: 0.6924, decode.d0.loss_cls: 1.5195, decode.d0.loss_mask: 0.4805, decode.d0.loss_dice: 0.7578, decode.d1.loss_cls: 0.2960, decode.d1.loss_mask: 0.4725, decode.d1.loss_dice: 0.7259, decode.d2.loss_cls: 0.2616, decode.d2.loss_mask: 0.4638, decode.d2.loss_dice: 0.7056, decode.d3.loss_cls: 0.2457, decode.d3.loss_mask: 0.4598, decode.d3.loss_dice: 0.6939, decode.d4.loss_cls: 0.2412, decode.d4.loss_mask: 0.4582, decode.d4.loss_dice: 0.6939, decode.d5.loss_cls: 0.2346, decode.d5.loss_mask: 0.4577, decode.d5.loss_dice: 0.6940, decode.d6.loss_cls: 0.2332, decode.d6.loss_mask: 0.4581, decode.d6.loss_dice: 0.6935, decode.d7.loss_cls: 0.2304, decode.d7.loss_mask: 0.4580, decode.d7.loss_dice: 0.6935, loss: 14.0074
2022-10-31 00:25:34,934 - mmseg - INFO - Iter [15350/20000]	lr: 7.098e-07, eta: 4:07:27, time: 2.612, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2336, decode.loss_mask: 0.4681, decode.loss_dice: 0.7062, decode.d0.loss_cls: 1.5181, decode.d0.loss_mask: 0.4964, decode.d0.loss_dice: 0.7727, decode.d1.loss_cls: 0.3021, decode.d1.loss_mask: 0.4841, decode.d1.loss_dice: 0.7399, decode.d2.loss_cls: 0.2615, decode.d2.loss_mask: 0.4756, decode.d2.loss_dice: 0.7212, decode.d3.loss_cls: 0.2439, decode.d3.loss_mask: 0.4716, decode.d3.loss_dice: 0.7094, decode.d4.loss_cls: 0.2399, decode.d4.loss_mask: 0.4697, decode.d4.loss_dice: 0.7074, decode.d5.loss_cls: 0.2363, decode.d5.loss_mask: 0.4690, decode.d5.loss_dice: 0.7072, decode.d6.loss_cls: 0.2337, decode.d6.loss_mask: 0.4683, decode.d6.loss_dice: 0.7039, decode.d7.loss_cls: 0.2352, decode.d7.loss_mask: 0.4683, decode.d7.loss_dice: 0.7022, loss: 14.2456
2022-10-31 00:27:39,240 - mmseg - INFO - Iter [15400/20000]	lr: 7.022e-07, eta: 4:04:17, time: 2.485, data_time: 0.071, memory: 35615, decode.loss_cls: 0.2233, decode.loss_mask: 0.4582, decode.loss_dice: 0.6982, decode.d0.loss_cls: 1.5123, decode.d0.loss_mask: 0.4797, decode.d0.loss_dice: 0.7624, decode.d1.loss_cls: 0.2910, decode.d1.loss_mask: 0.4709, decode.d1.loss_dice: 0.7305, decode.d2.loss_cls: 0.2523, decode.d2.loss_mask: 0.4635, decode.d2.loss_dice: 0.7131, decode.d3.loss_cls: 0.2347, decode.d3.loss_mask: 0.4598, decode.d3.loss_dice: 0.7039, decode.d4.loss_cls: 0.2310, decode.d4.loss_mask: 0.4580, decode.d4.loss_dice: 0.7013, decode.d5.loss_cls: 0.2261, decode.d5.loss_mask: 0.4586, decode.d5.loss_dice: 0.7029, decode.d6.loss_cls: 0.2241, decode.d6.loss_mask: 0.4580, decode.d6.loss_dice: 0.7007, decode.d7.loss_cls: 0.2225, decode.d7.loss_mask: 0.4581, decode.d7.loss_dice: 0.6994, loss: 13.9941
2022-10-31 00:29:41,809 - mmseg - INFO - Iter [15450/20000]	lr: 6.945e-07, eta: 4:01:07, time: 2.452, data_time: 0.019, memory: 35615, decode.loss_cls: 0.2113, decode.loss_mask: 0.4587, decode.loss_dice: 0.6950, decode.d0.loss_cls: 1.5004, decode.d0.loss_mask: 0.4872, decode.d0.loss_dice: 0.7586, decode.d1.loss_cls: 0.2800, decode.d1.loss_mask: 0.4736, decode.d1.loss_dice: 0.7327, decode.d2.loss_cls: 0.2444, decode.d2.loss_mask: 0.4645, decode.d2.loss_dice: 0.7079, decode.d3.loss_cls: 0.2281, decode.d3.loss_mask: 0.4614, decode.d3.loss_dice: 0.6970, decode.d4.loss_cls: 0.2234, decode.d4.loss_mask: 0.4604, decode.d4.loss_dice: 0.6952, decode.d5.loss_cls: 0.2164, decode.d5.loss_mask: 0.4590, decode.d5.loss_dice: 0.6960, decode.d6.loss_cls: 0.2134, decode.d6.loss_mask: 0.4586, decode.d6.loss_dice: 0.6931, decode.d7.loss_cls: 0.2128, decode.d7.loss_mask: 0.4588, decode.d7.loss_dice: 0.6935, loss: 13.8816
2022-10-31 00:31:44,201 - mmseg - INFO - Iter [15500/20000]	lr: 6.869e-07, eta: 3:57:58, time: 2.448, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2123, decode.loss_mask: 0.4458, decode.loss_dice: 0.6824, decode.d0.loss_cls: 1.5059, decode.d0.loss_mask: 0.4725, decode.d0.loss_dice: 0.7467, decode.d1.loss_cls: 0.2756, decode.d1.loss_mask: 0.4596, decode.d1.loss_dice: 0.7145, decode.d2.loss_cls: 0.2419, decode.d2.loss_mask: 0.4506, decode.d2.loss_dice: 0.6976, decode.d3.loss_cls: 0.2245, decode.d3.loss_mask: 0.4489, decode.d3.loss_dice: 0.6870, decode.d4.loss_cls: 0.2176, decode.d4.loss_mask: 0.4472, decode.d4.loss_dice: 0.6840, decode.d5.loss_cls: 0.2142, decode.d5.loss_mask: 0.4461, decode.d5.loss_dice: 0.6854, decode.d6.loss_cls: 0.2122, decode.d6.loss_mask: 0.4457, decode.d6.loss_dice: 0.6826, decode.d7.loss_cls: 0.2111, decode.d7.loss_mask: 0.4455, decode.d7.loss_dice: 0.6818, loss: 13.6391
2022-10-31 00:33:48,260 - mmseg - INFO - Iter [15550/20000]	lr: 6.793e-07, eta: 3:54:52, time: 2.481, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2222, decode.loss_mask: 0.4701, decode.loss_dice: 0.7037, decode.d0.loss_cls: 1.5091, decode.d0.loss_mask: 0.4955, decode.d0.loss_dice: 0.7693, decode.d1.loss_cls: 0.2917, decode.d1.loss_mask: 0.4849, decode.d1.loss_dice: 0.7370, decode.d2.loss_cls: 0.2520, decode.d2.loss_mask: 0.4767, decode.d2.loss_dice: 0.7173, decode.d3.loss_cls: 0.2333, decode.d3.loss_mask: 0.4745, decode.d3.loss_dice: 0.7074, decode.d4.loss_cls: 0.2275, decode.d4.loss_mask: 0.4724, decode.d4.loss_dice: 0.7064, decode.d5.loss_cls: 0.2261, decode.d5.loss_mask: 0.4715, decode.d5.loss_dice: 0.7051, decode.d6.loss_cls: 0.2226, decode.d6.loss_mask: 0.4706, decode.d6.loss_dice: 0.7040, decode.d7.loss_cls: 0.2212, decode.d7.loss_mask: 0.4711, decode.d7.loss_dice: 0.7035, loss: 14.1469
2022-10-31 00:35:49,773 - mmseg - INFO - Iter [15600/20000]	lr: 6.716e-07, eta: 3:51:45, time: 2.430, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2160, decode.loss_mask: 0.4659, decode.loss_dice: 0.6922, decode.d0.loss_cls: 1.5041, decode.d0.loss_mask: 0.4898, decode.d0.loss_dice: 0.7579, decode.d1.loss_cls: 0.2827, decode.d1.loss_mask: 0.4803, decode.d1.loss_dice: 0.7241, decode.d2.loss_cls: 0.2444, decode.d2.loss_mask: 0.4717, decode.d2.loss_dice: 0.7062, decode.d3.loss_cls: 0.2275, decode.d3.loss_mask: 0.4693, decode.d3.loss_dice: 0.6970, decode.d4.loss_cls: 0.2212, decode.d4.loss_mask: 0.4677, decode.d4.loss_dice: 0.6952, decode.d5.loss_cls: 0.2160, decode.d5.loss_mask: 0.4664, decode.d5.loss_dice: 0.6957, decode.d6.loss_cls: 0.2167, decode.d6.loss_mask: 0.4663, decode.d6.loss_dice: 0.6912, decode.d7.loss_cls: 0.2175, decode.d7.loss_mask: 0.4654, decode.d7.loss_dice: 0.6903, loss: 13.9387
2022-10-31 00:37:50,213 - mmseg - INFO - Iter [15650/20000]	lr: 6.640e-07, eta: 3:48:38, time: 2.409, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2255, decode.loss_mask: 0.4505, decode.loss_dice: 0.6892, decode.d0.loss_cls: 1.5053, decode.d0.loss_mask: 0.4746, decode.d0.loss_dice: 0.7610, decode.d1.loss_cls: 0.2942, decode.d1.loss_mask: 0.4621, decode.d1.loss_dice: 0.7252, decode.d2.loss_cls: 0.2560, decode.d2.loss_mask: 0.4580, decode.d2.loss_dice: 0.7065, decode.d3.loss_cls: 0.2379, decode.d3.loss_mask: 0.4531, decode.d3.loss_dice: 0.6954, decode.d4.loss_cls: 0.2329, decode.d4.loss_mask: 0.4521, decode.d4.loss_dice: 0.6946, decode.d5.loss_cls: 0.2297, decode.d5.loss_mask: 0.4507, decode.d5.loss_dice: 0.6920, decode.d6.loss_cls: 0.2273, decode.d6.loss_mask: 0.4497, decode.d6.loss_dice: 0.6902, decode.d7.loss_cls: 0.2266, decode.d7.loss_mask: 0.4501, decode.d7.loss_dice: 0.6895, loss: 13.8799
2022-10-31 00:39:54,948 - mmseg - INFO - Iter [15700/20000]	lr: 6.564e-07, eta: 3:45:35, time: 2.495, data_time: 0.070, memory: 35615, decode.loss_cls: 0.2114, decode.loss_mask: 0.4569, decode.loss_dice: 0.6865, decode.d0.loss_cls: 1.4841, decode.d0.loss_mask: 0.4849, decode.d0.loss_dice: 0.7493, decode.d1.loss_cls: 0.2769, decode.d1.loss_mask: 0.4734, decode.d1.loss_dice: 0.7180, decode.d2.loss_cls: 0.2383, decode.d2.loss_mask: 0.4621, decode.d2.loss_dice: 0.6999, decode.d3.loss_cls: 0.2222, decode.d3.loss_mask: 0.4590, decode.d3.loss_dice: 0.6899, decode.d4.loss_cls: 0.2185, decode.d4.loss_mask: 0.4586, decode.d4.loss_dice: 0.6869, decode.d5.loss_cls: 0.2152, decode.d5.loss_mask: 0.4568, decode.d5.loss_dice: 0.6864, decode.d6.loss_cls: 0.2124, decode.d6.loss_mask: 0.4561, decode.d6.loss_dice: 0.6864, decode.d7.loss_cls: 0.2105, decode.d7.loss_mask: 0.4567, decode.d7.loss_dice: 0.6847, loss: 13.7419
2022-10-31 00:42:04,156 - mmseg - INFO - Iter [15750/20000]	lr: 6.487e-07, eta: 3:42:37, time: 2.584, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2159, decode.loss_mask: 0.4645, decode.loss_dice: 0.6907, decode.d0.loss_cls: 1.5041, decode.d0.loss_mask: 0.4903, decode.d0.loss_dice: 0.7589, decode.d1.loss_cls: 0.2817, decode.d1.loss_mask: 0.4777, decode.d1.loss_dice: 0.7264, decode.d2.loss_cls: 0.2419, decode.d2.loss_mask: 0.4697, decode.d2.loss_dice: 0.7060, decode.d3.loss_cls: 0.2265, decode.d3.loss_mask: 0.4670, decode.d3.loss_dice: 0.6947, decode.d4.loss_cls: 0.2233, decode.d4.loss_mask: 0.4657, decode.d4.loss_dice: 0.6943, decode.d5.loss_cls: 0.2180, decode.d5.loss_mask: 0.4653, decode.d5.loss_dice: 0.6943, decode.d6.loss_cls: 0.2149, decode.d6.loss_mask: 0.4654, decode.d6.loss_dice: 0.6930, decode.d7.loss_cls: 0.2138, decode.d7.loss_mask: 0.4654, decode.d7.loss_dice: 0.6900, loss: 13.9194
2022-10-31 00:44:10,415 - mmseg - INFO - Iter [15800/20000]	lr: 6.411e-07, eta: 3:39:38, time: 2.525, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2243, decode.loss_mask: 0.4536, decode.loss_dice: 0.6871, decode.d0.loss_cls: 1.5126, decode.d0.loss_mask: 0.4803, decode.d0.loss_dice: 0.7568, decode.d1.loss_cls: 0.2915, decode.d1.loss_mask: 0.4661, decode.d1.loss_dice: 0.7261, decode.d2.loss_cls: 0.2530, decode.d2.loss_mask: 0.4599, decode.d2.loss_dice: 0.7037, decode.d3.loss_cls: 0.2360, decode.d3.loss_mask: 0.4561, decode.d3.loss_dice: 0.6930, decode.d4.loss_cls: 0.2309, decode.d4.loss_mask: 0.4554, decode.d4.loss_dice: 0.6898, decode.d5.loss_cls: 0.2272, decode.d5.loss_mask: 0.4542, decode.d5.loss_dice: 0.6906, decode.d6.loss_cls: 0.2250, decode.d6.loss_mask: 0.4533, decode.d6.loss_dice: 0.6874, decode.d7.loss_cls: 0.2253, decode.d7.loss_mask: 0.4531, decode.d7.loss_dice: 0.6878, loss: 13.8801
2022-10-31 00:46:13,732 - mmseg - INFO - Iter [15850/20000]	lr: 6.335e-07, eta: 3:36:37, time: 2.466, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2212, decode.loss_mask: 0.4533, decode.loss_dice: 0.6953, decode.d0.loss_cls: 1.5024, decode.d0.loss_mask: 0.4784, decode.d0.loss_dice: 0.7632, decode.d1.loss_cls: 0.2924, decode.d1.loss_mask: 0.4682, decode.d1.loss_dice: 0.7295, decode.d2.loss_cls: 0.2504, decode.d2.loss_mask: 0.4589, decode.d2.loss_dice: 0.7073, decode.d3.loss_cls: 0.2326, decode.d3.loss_mask: 0.4557, decode.d3.loss_dice: 0.6993, decode.d4.loss_cls: 0.2281, decode.d4.loss_mask: 0.4528, decode.d4.loss_dice: 0.6988, decode.d5.loss_cls: 0.2232, decode.d5.loss_mask: 0.4529, decode.d5.loss_dice: 0.6988, decode.d6.loss_cls: 0.2221, decode.d6.loss_mask: 0.4527, decode.d6.loss_dice: 0.6945, decode.d7.loss_cls: 0.2231, decode.d7.loss_mask: 0.4538, decode.d7.loss_dice: 0.6951, loss: 13.9039
2022-10-31 00:48:18,008 - mmseg - INFO - Iter [15900/20000]	lr: 6.258e-07, eta: 3:33:38, time: 2.485, data_time: 0.018, memory: 35615, decode.loss_cls: 0.2211, decode.loss_mask: 0.4636, decode.loss_dice: 0.7024, decode.d0.loss_cls: 1.5103, decode.d0.loss_mask: 0.4935, decode.d0.loss_dice: 0.7714, decode.d1.loss_cls: 0.2906, decode.d1.loss_mask: 0.4799, decode.d1.loss_dice: 0.7398, decode.d2.loss_cls: 0.2512, decode.d2.loss_mask: 0.4711, decode.d2.loss_dice: 0.7184, decode.d3.loss_cls: 0.2325, decode.d3.loss_mask: 0.4686, decode.d3.loss_dice: 0.7105, decode.d4.loss_cls: 0.2279, decode.d4.loss_mask: 0.4667, decode.d4.loss_dice: 0.7075, decode.d5.loss_cls: 0.2234, decode.d5.loss_mask: 0.4647, decode.d5.loss_dice: 0.7053, decode.d6.loss_cls: 0.2221, decode.d6.loss_mask: 0.4640, decode.d6.loss_dice: 0.7044, decode.d7.loss_cls: 0.2220, decode.d7.loss_mask: 0.4640, decode.d7.loss_dice: 0.7039, loss: 14.1008
2022-10-31 00:50:26,693 - mmseg - INFO - Iter [15950/20000]	lr: 6.182e-07, eta: 3:30:43, time: 2.573, data_time: 0.012, memory: 35615, decode.loss_cls: 0.2161, decode.loss_mask: 0.4631, decode.loss_dice: 0.6897, decode.d0.loss_cls: 1.4814, decode.d0.loss_mask: 0.4861, decode.d0.loss_dice: 0.7548, decode.d1.loss_cls: 0.2860, decode.d1.loss_mask: 0.4771, decode.d1.loss_dice: 0.7214, decode.d2.loss_cls: 0.2461, decode.d2.loss_mask: 0.4695, decode.d2.loss_dice: 0.7051, decode.d3.loss_cls: 0.2303, decode.d3.loss_mask: 0.4657, decode.d3.loss_dice: 0.6944, decode.d4.loss_cls: 0.2224, decode.d4.loss_mask: 0.4646, decode.d4.loss_dice: 0.6929, decode.d5.loss_cls: 0.2189, decode.d5.loss_mask: 0.4633, decode.d5.loss_dice: 0.6911, decode.d6.loss_cls: 0.2167, decode.d6.loss_mask: 0.4629, decode.d6.loss_dice: 0.6907, decode.d7.loss_cls: 0.2153, decode.d7.loss_mask: 0.4629, decode.d7.loss_dice: 0.6899, loss: 13.8786
2022-10-31 00:52:29,138 - mmseg - INFO - Saving checkpoint at 16000 iterations
2022-10-31 00:53:03,372 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-31 00:53:03,373 - mmseg - INFO - Iter [16000/20000]	lr: 6.106e-07, eta: 3:28:07, time: 3.135, data_time: 0.013, memory: 35615, decode.loss_cls: 0.2144, decode.loss_mask: 0.4577, decode.loss_dice: 0.6874, decode.d0.loss_cls: 1.4884, decode.d0.loss_mask: 0.4822, decode.d0.loss_dice: 0.7518, decode.d1.loss_cls: 0.2806, decode.d1.loss_mask: 0.4746, decode.d1.loss_dice: 0.7225, decode.d2.loss_cls: 0.2418, decode.d2.loss_mask: 0.4648, decode.d2.loss_dice: 0.7040, decode.d3.loss_cls: 0.2262, decode.d3.loss_mask: 0.4597, decode.d3.loss_dice: 0.6957, decode.d4.loss_cls: 0.2206, decode.d4.loss_mask: 0.4590, decode.d4.loss_dice: 0.6940, decode.d5.loss_cls: 0.2160, decode.d5.loss_mask: 0.4583, decode.d5.loss_dice: 0.6914, decode.d6.loss_cls: 0.2132, decode.d6.loss_mask: 0.4576, decode.d6.loss_dice: 0.6903, decode.d7.loss_cls: 0.2127, decode.d7.loss_mask: 0.4577, decode.d7.loss_dice: 0.6876, loss: 13.8102

2022-10-31 09:09:50,215 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.8 (default, Feb 24 2021, 21:46:12) [GCC 7.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-40GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.2, V11.2.142
GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
PyTorch: 1.9.0a0+df837d0
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2019.0.4 Product Build 20190411 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash N/A)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.2
  - NVCC architecture flags: -gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_86,code=compute_86
  - CuDNN 8.1.1
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.2, CUDNN_VERSION=8.1.1, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, FORCE_FALLBACK_CUDA_MPI=1, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=ON, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.0a0
OpenCV: 4.5.5
MMCV: 1.6.2
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.2
MMSegmentation: 0.20.2+dd8b542
------------------------------------------------------------

2022-10-31 09:09:50,216 - mmseg - INFO - Distributed training: True
2022-10-31 09:09:51,154 - mmseg - INFO - Config:
num_things_classes = 100
num_stuff_classes = 50
num_classes = 150
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoderMask2FormerAug',
    pretrained=
    '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/checkpoint-149/mp_rank_00_model_states_renamed-s14tos16.pt',
    backbone=dict(
        type='BEiTAdapter',
        patch_size=16,
        embed_dim=1408,
        depth=40,
        num_heads=16,
        mlp_ratio=4.363636363636363,
        qkv_bias=True,
        use_abs_pos_emb=True,
        use_rel_pos_bias=False,
        img_size=896,
        init_values=None,
        drop_path_rate=0.5,
        conv_inplane=64,
        n_points=4,
        deform_num_heads=16,
        cffn_ratio=0.25,
        deform_ratio=0.5,
        with_cp=True,
        interaction_indexes=[[0, 9], [10, 19], [20, 29], [30, 39]]),
    decode_head=dict(
        type='Mask2FormerHead',
        in_channels=[1408, 1408, 1408, 1408],
        feat_channels=1024,
        out_channels=1024,
        in_index=[0, 1, 2, 3],
        num_things_classes=100,
        num_stuff_classes=50,
        num_queries=200,
        num_transformer_feat_level=3,
        pixel_decoder=dict(
            type='MSDeformAttnPixelDecoder',
            num_outs=3,
            norm_cfg=dict(type='GN', num_groups=32),
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=1024,
                        num_heads=32,
                        num_levels=3,
                        num_points=4,
                        im2col_step=64,
                        dropout=0.0,
                        batch_first=False,
                        norm_cfg=None,
                        init_cfg=None),
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=1024,
                        feedforward_channels=4096,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True),
                        with_cp=True),
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')),
                init_cfg=None),
            positional_encoding=dict(
                type='SinePositionalEncoding', num_feats=512, normalize=True),
            init_cfg=None),
        enforce_decoder_input_project=False,
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=512, normalize=True),
        transformer_decoder=dict(
            type='DetrTransformerDecoder',
            return_intermediate=True,
            num_layers=8,
            transformerlayers=dict(
                type='DetrTransformerDecoderLayer',
                attn_cfgs=dict(
                    type='MultiheadAttention',
                    embed_dims=1024,
                    num_heads=32,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=False),
                ffn_cfgs=dict(
                    embed_dims=1024,
                    feedforward_channels=4096,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.0,
                    dropout_layer=None,
                    add_identity=True,
                    with_cp=True),
                feedforward_channels=4096,
                operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                 'ffn', 'norm')),
            init_cfg=None),
        loss_cls=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=2.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1
            ]),
        loss_mask=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            reduction='mean',
            loss_weight=5.0),
        loss_dice=dict(
            type='DiceLoss',
            use_sigmoid=True,
            activate=True,
            reduction='mean',
            naive_dice=True,
            eps=1.0,
            loss_weight=5.0)),
    train_cfg=dict(
        num_points=12544,
        oversample_ratio=3.0,
        importance_sample_ratio=0.75,
        assigner=dict(
            type='MaskHungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=2.0),
            mask_cost=dict(
                type='CrossEntropyLossCost', weight=5.0, use_sigmoid=True),
            dice_cost=dict(
                type='DiceCost', weight=5.0, pred_act=True, eps=1.0)),
        sampler=dict(type='MaskPseudoSampler')),
    test_cfg=dict(
        panoptic_on=True,
        semantic_on=False,
        instance_on=True,
        max_per_image=100,
        iou_thr=0.8,
        filter_low_score=True,
        mode='slide',
        crop_size=(896, 896),
        stride=(512, 512)),
    init_cfg=None)
dataset_type = 'ADE20KDataset'
data_root = '/sharefs/yxf/data/seg/ade/ADEChallengeData2016'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (896, 896)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', reduce_zero_label=True),
    dict(type='Resize', img_scale=(3584, 896), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
    dict(type='ToMask'),
    dict(type='DefaultFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(3584, 896),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='ResizeToMultiple', size_divisor=32),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=4,
    train=dict(
        type='ADE20KDataset',
        data_root='/sharefs/yxf/data/seg/ade/ADEChallengeData2016',
        img_dir='images/training',
        ann_dir='annotations/training',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', reduce_zero_label=True),
            dict(type='Resize', img_scale=(3584, 896), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
            dict(type='ToMask'),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
        ]),
    val=dict(
        type='ADE20KDataset',
        data_root='/sharefs/yxf/data/seg/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(3584, 896),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='ResizeToMultiple', size_divisor=32),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='ADE20KDataset',
        data_root='/sharefs/yxf/data/seg/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(3584, 896),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='ResizeToMultiple', size_divisor=32),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_80k_cocostuff164k_ss/lr1e-5_lrd0.95_enc6_dec8/iter_60000.pth'
resume_from = '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95/latest.pth'
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=3.5e-05,
    betas=(0.9, 0.999),
    weight_decay=0.05,
    constructor='LayerDecayOptimizerConstructor',
    paramwise_cfg=dict(num_layers=40, layer_decay_rate=0.95))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=20000)
checkpoint_config = dict(by_epoch=False, interval=2000, max_keep_ckpts=6)
evaluation = dict(
    interval=2000, metric='mIoU', pre_eval=True, save_best='mIoU')
pretrained = '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/checkpoint-149/mp_rank_00_model_states_renamed-s14tos16.pt'
work_dir = '/sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95'
gpu_ids = range(0, 64)
auto_resume = False

2022-10-31 09:10:17,310 - mmseg - INFO - Set random seed to 613814028, deterministic: False
2022-10-31 09:10:37,313 - mmseg - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: mask_token, norm.weight, norm.bias, lm_head.weight, lm_head.bias

Name of parameter - Initialization information

backbone.cls_token - torch.Size([1, 1, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.pos_embed - torch.Size([1, 3137, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.level_embed - torch.Size([3, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.patch_embed.proj.weight - torch.Size([1408, 3, 16, 16]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.patch_embed.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.0.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.1.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.2.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.3.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.4.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.5.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.6.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.7.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.8.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.9.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.10.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.11.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.12.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.13.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.14.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.15.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.16.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.17.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.18.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.19.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.20.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.21.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.22.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.23.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.24.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.25.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.26.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.27.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.28.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.29.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.30.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.31.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.32.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.33.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.34.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.35.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.36.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.37.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.38.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.q_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.v_bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.qkv.weight - torch.Size([4224, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.proj.weight - torch.Size([1408, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.attn.proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc1.weight - torch.Size([6144, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc1.bias - torch.Size([6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc2.weight - torch.Size([1408, 6144]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.blocks.39.mlp.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.0.weight - torch.Size([64, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.6.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.7.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.stem.7.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv2.0.weight - torch.Size([128, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv2.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv2.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv3.0.weight - torch.Size([256, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv4.0.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.conv4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc1.weight - torch.Size([1408, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc2.weight - torch.Size([1408, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc3.weight - torch.Size([1408, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc3.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc4.weight - torch.Size([1408, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.spm.fc4.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.0.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.1.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.2.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.gamma - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.sampling_offsets.weight - torch.Size([384, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.attention_weights.weight - torch.Size([192, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.injector.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extractor.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.0.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.query_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.query_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.feat_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.feat_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.weight - torch.Size([128, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.attention_weights.weight - torch.Size([64, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.value_proj.weight - torch.Size([704, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.value_proj.bias - torch.Size([704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.output_proj.weight - torch.Size([1408, 704]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.attn.output_proj.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc1.weight - torch.Size([352, 1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc1.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.weight - torch.Size([352, 1, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.bias - torch.Size([352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc2.weight - torch.Size([1408, 352]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn.fc2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn_norm.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.interactions.3.extra_extractors.1.ffn_norm.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.up.weight - torch.Size([1408, 1408, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.up.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm1.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm1.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm2.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm2.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm3.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm3.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm4.weight - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

backbone.norm4.bias - torch.Size([1408]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([768, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([384, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.level_encoding.weight - torch.Size([3, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([1024, 1408, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([1024, 1024, 3, 3]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.pixel_decoder.mask_feature.weight - torch.Size([1024, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.mask_feature.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.ffns.0.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffns.0.layers.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.post_norm.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.transformer_decoder.post_norm.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.query_embed.weight - torch.Size([200, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.query_feat.weight - torch.Size([200, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.level_embed.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.cls_embed.weight - torch.Size([151, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.cls_embed.bias - torch.Size([151]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.0.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.2.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.4.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  

decode_head.mask_embed.4.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderMask2FormerAug  
2022-10-31 09:10:40,875 - mmseg - INFO - EncoderDecoderMask2FormerAug(
  (backbone): BEiTAdapter(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1408, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.012820512987673283)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.025641025975346565)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.03846153989434242)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.05128205195069313)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.06410256773233414)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.07692307978868484)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.08974359184503555)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.10256410390138626)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.11538461595773697)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.12820513546466827)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.14102564752101898)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1538461595773697)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1666666716337204)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1794871836900711)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.19230769574642181)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (16): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.20512820780277252)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (17): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.21794871985912323)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (18): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.23076923191547394)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (19): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.24358974397182465)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (20): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.25641027092933655)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (21): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.26923078298568726)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (22): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.28205129504203796)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (23): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.29487180709838867)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (24): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3076923191547394)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (25): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3205128312110901)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (26): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3333333432674408)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (27): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3461538553237915)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (28): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3589743673801422)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (29): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.3717948794364929)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (30): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.38461539149284363)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (31): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.39743590354919434)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (32): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.41025641560554504)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (33): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.42307692766189575)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (34): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.43589743971824646)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (35): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.44871795177459717)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (36): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.4615384638309479)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (37): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.4743589758872986)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (38): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.4871794879436493)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (39): Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.5)
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (spm): SpatialPriorModule(
      (stem): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
        (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      )
      (conv2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv4): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (fc1): Conv2d(64, 1408, kernel_size=(1, 1), stride=(1, 1))
      (fc2): Conv2d(128, 1408, kernel_size=(1, 1), stride=(1, 1))
      (fc3): Conv2d(256, 1408, kernel_size=(1, 1), stride=(1, 1))
      (fc4): Conv2d(256, 1408, kernel_size=(1, 1), stride=(1, 1))
    )
    (interactions): Sequential(
      (0): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (1): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (2): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (3): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=192, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
            (value_proj): Linear(in_features=1408, out_features=704, bias=True)
            (output_proj): Linear(in_features=704, out_features=1408, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1408, out_features=352, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
            )
            (act): GELU()
            (fc2): Linear(in_features=352, out_features=1408, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (extra_extractors): Sequential(
          (0): Extractor(
            (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
              (value_proj): Linear(in_features=1408, out_features=704, bias=True)
              (output_proj): Linear(in_features=704, out_features=1408, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1408, out_features=352, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
              )
              (act): GELU()
              (fc2): Linear(in_features=352, out_features=1408, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
          (1): Extractor(
            (query_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1408, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1408, out_features=64, bias=True)
              (value_proj): Linear(in_features=1408, out_features=704, bias=True)
              (output_proj): Linear(in_features=704, out_features=1408, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1408, out_features=352, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352)
              )
              (act): GELU()
              (fc2): Linear(in_features=352, out_features=1408, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
        )
      )
    )
    (up): ConvTranspose2d(1408, 1408, kernel_size=(2, 2), stride=(2, 2))
    (norm1): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm2): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm3): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm4): SyncBatchNorm(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (decode_head): Mask2FormerHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (conv_seg): None
    (dropout): Dropout2d(p=0.1, inplace=False)
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (1): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (2): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (postional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
      (level_encoding): Embedding(3, 1024)
      (lateral_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1408, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (output_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
          (activate): ReLU(inplace=True)
        )
      )
      (mask_feature): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    )
    (transformer_decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (6): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (7): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): _LinearWithBias(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (decoder_input_projs): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
    )
    (decoder_positional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (query_embed): Embedding(200, 1024)
    (query_feat): Embedding(200, 1024)
    (level_embed): Embedding(3, 1024)
    (cls_embed): Linear(in_features=1024, out_features=151, bias=True)
    (mask_embed): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (loss_cls): CrossEntropyLoss(avg_non_ignore=False)
    (loss_mask): CrossEntropyLoss(avg_non_ignore=False)
    (loss_dice): DiceLoss()
  )
)
2022-10-31 09:10:41,979 - mmseg - INFO - Loaded 20210 images
2022-10-31 09:10:48,509 - mmseg - INFO - load checkpoint from local path: /sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95/latest.pth
2022-10-31 09:11:18,018 - mmseg - INFO - resumed from epoch: 19, iter 15999
2022-10-31 09:11:18,022 - mmseg - INFO - Start running, host: fangyuxin@yxf-qs5qp-3346373-worker-0, work_dir: /sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95
2022-10-31 09:11:18,023 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2022-10-31 09:11:18,024 - mmseg - INFO - workflow: [('train', 1)], max: 20000 iters
2022-10-31 09:11:18,024 - mmseg - INFO - Checkpoints will be saved to /sharefs/baaivision/yxf/outputs/beitXclip/large-giant/150/merge30M_beit_g_patch14_224_sz224_mask105_lr1e-3_b20.98_eps1e-6_dpr0.1_ls0.0_bsz16x8x32_ep150_wmep2_cj0.0_ftpye2_ltype1_mixup0.0_abspos/seg/mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss/lr2.5e-5_lrd0.95 by HardDiskBackend.
2022-10-31 09:12:42,946 - mmseg - INFO - Saving checkpoint at 16000 iterations
2022-10-31 09:13:17,284 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-31 09:13:17,284 - mmseg - INFO - Iter [16000/20000]	lr: 6.106e-07, eta: 228 days, 19:46:09, time: 98.852, data_time: 0.212, memory: 35120, decode.loss_cls: 0.2684, decode.loss_mask: 0.4538, decode.loss_dice: 0.6730, decode.d0.loss_cls: 1.5055, decode.d0.loss_mask: 0.4993, decode.d0.loss_dice: 0.7407, decode.d1.loss_cls: 0.3294, decode.d1.loss_mask: 0.4692, decode.d1.loss_dice: 0.6976, decode.d2.loss_cls: 0.3037, decode.d2.loss_mask: 0.4661, decode.d2.loss_dice: 0.6778, decode.d3.loss_cls: 0.3006, decode.d3.loss_mask: 0.4546, decode.d3.loss_dice: 0.6569, decode.d4.loss_cls: 0.2814, decode.d4.loss_mask: 0.4628, decode.d4.loss_dice: 0.6768, decode.d5.loss_cls: 0.2925, decode.d5.loss_mask: 0.4523, decode.d5.loss_dice: 0.6630, decode.d6.loss_cls: 0.2728, decode.d6.loss_mask: 0.4545, decode.d6.loss_dice: 0.6676, decode.d7.loss_cls: 0.2736, decode.d7.loss_mask: 0.4526, decode.d7.loss_dice: 0.6733, loss: 14.1197
2022-10-31 09:15:32,967 - mmseg - INFO - Iter [16050/20000]	lr: 6.030e-07, eta: 4 days, 13:15:17, time: 2.714, data_time: 0.012, memory: 35585, decode.loss_cls: 0.2200, decode.loss_mask: 0.4600, decode.loss_dice: 0.6913, decode.d0.loss_cls: 1.4888, decode.d0.loss_mask: 0.4880, decode.d0.loss_dice: 0.7586, decode.d1.loss_cls: 0.2853, decode.d1.loss_mask: 0.4744, decode.d1.loss_dice: 0.7256, decode.d2.loss_cls: 0.2433, decode.d2.loss_mask: 0.4666, decode.d2.loss_dice: 0.7075, decode.d3.loss_cls: 0.2276, decode.d3.loss_mask: 0.4630, decode.d3.loss_dice: 0.6967, decode.d4.loss_cls: 0.2237, decode.d4.loss_mask: 0.4621, decode.d4.loss_dice: 0.6941, decode.d5.loss_cls: 0.2187, decode.d5.loss_mask: 0.4620, decode.d5.loss_dice: 0.6929, decode.d6.loss_cls: 0.2187, decode.d6.loss_mask: 0.4602, decode.d6.loss_dice: 0.6914, decode.d7.loss_cls: 0.2180, decode.d7.loss_mask: 0.4600, decode.d7.loss_dice: 0.6910, loss: 13.8895
2022-10-31 09:17:44,907 - mmseg - INFO - Iter [16100/20000]	lr: 5.953e-07, eta: 2 days, 7:53:06, time: 2.639, data_time: 0.012, memory: 35585, decode.loss_cls: 0.2230, decode.loss_mask: 0.4453, decode.loss_dice: 0.6867, decode.d0.loss_cls: 1.5056, decode.d0.loss_mask: 0.4719, decode.d0.loss_dice: 0.7507, decode.d1.loss_cls: 0.2921, decode.d1.loss_mask: 0.4607, decode.d1.loss_dice: 0.7165, decode.d2.loss_cls: 0.2548, decode.d2.loss_mask: 0.4507, decode.d2.loss_dice: 0.6989, decode.d3.loss_cls: 0.2358, decode.d3.loss_mask: 0.4474, decode.d3.loss_dice: 0.6895, decode.d4.loss_cls: 0.2320, decode.d4.loss_mask: 0.4460, decode.d4.loss_dice: 0.6861, decode.d5.loss_cls: 0.2252, decode.d5.loss_mask: 0.4467, decode.d5.loss_dice: 0.6872, decode.d6.loss_cls: 0.2208, decode.d6.loss_mask: 0.4454, decode.d6.loss_dice: 0.6865, decode.d7.loss_cls: 0.2212, decode.d7.loss_mask: 0.4456, decode.d7.loss_dice: 0.6846, loss: 13.7570
2022-10-31 09:19:47,651 - mmseg - INFO - Iter [16150/20000]	lr: 5.877e-07, eta: 1 day, 13:46:12, time: 2.455, data_time: 0.013, memory: 35585, decode.loss_cls: 0.2072, decode.loss_mask: 0.4548, decode.loss_dice: 0.6908, decode.d0.loss_cls: 1.4775, decode.d0.loss_mask: 0.4801, decode.d0.loss_dice: 0.7548, decode.d1.loss_cls: 0.2741, decode.d1.loss_mask: 0.4682, decode.d1.loss_dice: 0.7224, decode.d2.loss_cls: 0.2353, decode.d2.loss_mask: 0.4612, decode.d2.loss_dice: 0.7054, decode.d3.loss_cls: 0.2190, decode.d3.loss_mask: 0.4577, decode.d3.loss_dice: 0.6985, decode.d4.loss_cls: 0.2138, decode.d4.loss_mask: 0.4570, decode.d4.loss_dice: 0.6959, decode.d5.loss_cls: 0.2108, decode.d5.loss_mask: 0.4550, decode.d5.loss_dice: 0.6940, decode.d6.loss_cls: 0.2064, decode.d6.loss_mask: 0.4552, decode.d6.loss_dice: 0.6923, decode.d7.loss_cls: 0.2080, decode.d7.loss_mask: 0.4553, decode.d7.loss_dice: 0.6911, loss: 13.7416
2022-10-31 09:21:54,583 - mmseg - INFO - Iter [16200/20000]	lr: 5.801e-07, eta: 1 day, 4:40:21, time: 2.539, data_time: 0.011, memory: 35585, decode.loss_cls: 0.2186, decode.loss_mask: 0.4509, decode.loss_dice: 0.6910, decode.d0.loss_cls: 1.4949, decode.d0.loss_mask: 0.4765, decode.d0.loss_dice: 0.7544, decode.d1.loss_cls: 0.2881, decode.d1.loss_mask: 0.4659, decode.d1.loss_dice: 0.7271, decode.d2.loss_cls: 0.2484, decode.d2.loss_mask: 0.4588, decode.d2.loss_dice: 0.7086, decode.d3.loss_cls: 0.2305, decode.d3.loss_mask: 0.4546, decode.d3.loss_dice: 0.6958, decode.d4.loss_cls: 0.2261, decode.d4.loss_mask: 0.4535, decode.d4.loss_dice: 0.6944, decode.d5.loss_cls: 0.2234, decode.d5.loss_mask: 0.4523, decode.d5.loss_dice: 0.6937, decode.d6.loss_cls: 0.2204, decode.d6.loss_mask: 0.4516, decode.d6.loss_dice: 0.6924, decode.d7.loss_cls: 0.2190, decode.d7.loss_mask: 0.4513, decode.d7.loss_dice: 0.6933, loss: 13.8357
2022-10-31 09:23:59,527 - mmseg - INFO - Iter [16250/20000]	lr: 5.724e-07, eta: 23:10:38, time: 2.499, data_time: 0.016, memory: 35585, decode.loss_cls: 0.2128, decode.loss_mask: 0.4545, decode.loss_dice: 0.6886, decode.d0.loss_cls: 1.4889, decode.d0.loss_mask: 0.4815, decode.d0.loss_dice: 0.7516, decode.d1.loss_cls: 0.2779, decode.d1.loss_mask: 0.4688, decode.d1.loss_dice: 0.7222, decode.d2.loss_cls: 0.2418, decode.d2.loss_mask: 0.4595, decode.d2.loss_dice: 0.7054, decode.d3.loss_cls: 0.2244, decode.d3.loss_mask: 0.4562, decode.d3.loss_dice: 0.6938, decode.d4.loss_cls: 0.2201, decode.d4.loss_mask: 0.4544, decode.d4.loss_dice: 0.6944, decode.d5.loss_cls: 0.2145, decode.d5.loss_mask: 0.4541, decode.d5.loss_dice: 0.6924, decode.d6.loss_cls: 0.2124, decode.d6.loss_mask: 0.4535, decode.d6.loss_dice: 0.6903, decode.d7.loss_cls: 0.2142, decode.d7.loss_mask: 0.4530, decode.d7.loss_dice: 0.6894, loss: 13.7703
2022-10-31 09:26:05,282 - mmseg - INFO - Iter [16300/20000]	lr: 5.648e-07, eta: 19:29:56, time: 2.515, data_time: 0.012, memory: 35585, decode.loss_cls: 0.2084, decode.loss_mask: 0.4538, decode.loss_dice: 0.6805, decode.d0.loss_cls: 1.4965, decode.d0.loss_mask: 0.4764, decode.d0.loss_dice: 0.7419, decode.d1.loss_cls: 0.2768, decode.d1.loss_mask: 0.4676, decode.d1.loss_dice: 0.7112, decode.d2.loss_cls: 0.2367, decode.d2.loss_mask: 0.4609, decode.d2.loss_dice: 0.6941, decode.d3.loss_cls: 0.2196, decode.d3.loss_mask: 0.4578, decode.d3.loss_dice: 0.6855, decode.d4.loss_cls: 0.2157, decode.d4.loss_mask: 0.4560, decode.d4.loss_dice: 0.6839, decode.d5.loss_cls: 0.2097, decode.d5.loss_mask: 0.4551, decode.d5.loss_dice: 0.6830, decode.d6.loss_cls: 0.2090, decode.d6.loss_mask: 0.4536, decode.d6.loss_dice: 0.6819, decode.d7.loss_cls: 0.2072, decode.d7.loss_mask: 0.4538, decode.d7.loss_dice: 0.6804, loss: 13.6569
2022-10-31 09:28:14,578 - mmseg - INFO - Iter [16350/20000]	lr: 5.572e-07, eta: 16:52:08, time: 2.586, data_time: 0.066, memory: 35585, decode.loss_cls: 0.1977, decode.loss_mask: 0.4561, decode.loss_dice: 0.6835, decode.d0.loss_cls: 1.4760, decode.d0.loss_mask: 0.4815, decode.d0.loss_dice: 0.7482, decode.d1.loss_cls: 0.2667, decode.d1.loss_mask: 0.4703, decode.d1.loss_dice: 0.7183, decode.d2.loss_cls: 0.2291, decode.d2.loss_mask: 0.4635, decode.d2.loss_dice: 0.7000, decode.d3.loss_cls: 0.2104, decode.d3.loss_mask: 0.4581, decode.d3.loss_dice: 0.6885, decode.d4.loss_cls: 0.2047, decode.d4.loss_mask: 0.4573, decode.d4.loss_dice: 0.6872, decode.d5.loss_cls: 0.1998, decode.d5.loss_mask: 0.4575, decode.d5.loss_dice: 0.6879, decode.d6.loss_cls: 0.1989, decode.d6.loss_mask: 0.4566, decode.d6.loss_dice: 0.6867, decode.d7.loss_cls: 0.1983, decode.d7.loss_mask: 0.4563, decode.d7.loss_dice: 0.6833, loss: 13.6224
2022-10-31 09:30:21,929 - mmseg - INFO - Iter [16400/20000]	lr: 5.495e-07, eta: 14:52:51, time: 2.547, data_time: 0.012, memory: 35591, decode.loss_cls: 0.2152, decode.loss_mask: 0.4647, decode.loss_dice: 0.6953, decode.d0.loss_cls: 1.4873, decode.d0.loss_mask: 0.4899, decode.d0.loss_dice: 0.7581, decode.d1.loss_cls: 0.2769, decode.d1.loss_mask: 0.4788, decode.d1.loss_dice: 0.7310, decode.d2.loss_cls: 0.2430, decode.d2.loss_mask: 0.4697, decode.d2.loss_dice: 0.7092, decode.d3.loss_cls: 0.2276, decode.d3.loss_mask: 0.4665, decode.d3.loss_dice: 0.6981, decode.d4.loss_cls: 0.2194, decode.d4.loss_mask: 0.4664, decode.d4.loss_dice: 0.6973, decode.d5.loss_cls: 0.2169, decode.d5.loss_mask: 0.4657, decode.d5.loss_dice: 0.6953, decode.d6.loss_cls: 0.2164, decode.d6.loss_mask: 0.4649, decode.d6.loss_dice: 0.6952, decode.d7.loss_cls: 0.2156, decode.d7.loss_mask: 0.4635, decode.d7.loss_dice: 0.6928, loss: 13.9209
2022-10-31 09:32:30,510 - mmseg - INFO - Iter [16450/20000]	lr: 5.419e-07, eta: 13:19:42, time: 2.572, data_time: 0.013, memory: 35591, decode.loss_cls: 0.2024, decode.loss_mask: 0.4438, decode.loss_dice: 0.6734, decode.d0.loss_cls: 1.4758, decode.d0.loss_mask: 0.4727, decode.d0.loss_dice: 0.7382, decode.d1.loss_cls: 0.2705, decode.d1.loss_mask: 0.4576, decode.d1.loss_dice: 0.7053, decode.d2.loss_cls: 0.2345, decode.d2.loss_mask: 0.4506, decode.d2.loss_dice: 0.6857, decode.d3.loss_cls: 0.2158, decode.d3.loss_mask: 0.4471, decode.d3.loss_dice: 0.6770, decode.d4.loss_cls: 0.2106, decode.d4.loss_mask: 0.4463, decode.d4.loss_dice: 0.6754, decode.d5.loss_cls: 0.2053, decode.d5.loss_mask: 0.4455, decode.d5.loss_dice: 0.6729, decode.d6.loss_cls: 0.2036, decode.d6.loss_mask: 0.4446, decode.d6.loss_dice: 0.6742, decode.d7.loss_cls: 0.2023, decode.d7.loss_mask: 0.4446, decode.d7.loss_dice: 0.6713, loss: 13.4468
2022-10-31 09:34:32,254 - mmseg - INFO - Iter [16500/20000]	lr: 5.343e-07, eta: 12:03:56, time: 2.435, data_time: 0.012, memory: 35591, decode.loss_cls: 0.2200, decode.loss_mask: 0.4584, decode.loss_dice: 0.6953, decode.d0.loss_cls: 1.5015, decode.d0.loss_mask: 0.4837, decode.d0.loss_dice: 0.7646, decode.d1.loss_cls: 0.2864, decode.d1.loss_mask: 0.4723, decode.d1.loss_dice: 0.7327, decode.d2.loss_cls: 0.2468, decode.d2.loss_mask: 0.4661, decode.d2.loss_dice: 0.7130, decode.d3.loss_cls: 0.2301, decode.d3.loss_mask: 0.4605, decode.d3.loss_dice: 0.7024, decode.d4.loss_cls: 0.2219, decode.d4.loss_mask: 0.4602, decode.d4.loss_dice: 0.7011, decode.d5.loss_cls: 0.2202, decode.d5.loss_mask: 0.4591, decode.d5.loss_dice: 0.6985, decode.d6.loss_cls: 0.2185, decode.d6.loss_mask: 0.4587, decode.d6.loss_dice: 0.6973, decode.d7.loss_cls: 0.2185, decode.d7.loss_mask: 0.4577, decode.d7.loss_dice: 0.6972, loss: 13.9429
2022-10-31 09:36:34,303 - mmseg - INFO - Iter [16550/20000]	lr: 5.267e-07, eta: 11:01:34, time: 2.441, data_time: 0.028, memory: 35591, decode.loss_cls: 0.2112, decode.loss_mask: 0.4510, decode.loss_dice: 0.6896, decode.d0.loss_cls: 1.4801, decode.d0.loss_mask: 0.4754, decode.d0.loss_dice: 0.7515, decode.d1.loss_cls: 0.2799, decode.d1.loss_mask: 0.4662, decode.d1.loss_dice: 0.7206, decode.d2.loss_cls: 0.2406, decode.d2.loss_mask: 0.4574, decode.d2.loss_dice: 0.7019, decode.d3.loss_cls: 0.2244, decode.d3.loss_mask: 0.4526, decode.d3.loss_dice: 0.6941, decode.d4.loss_cls: 0.2193, decode.d4.loss_mask: 0.4522, decode.d4.loss_dice: 0.6945, decode.d5.loss_cls: 0.2155, decode.d5.loss_mask: 0.4517, decode.d5.loss_dice: 0.6920, decode.d6.loss_cls: 0.2120, decode.d6.loss_mask: 0.4504, decode.d6.loss_dice: 0.6899, decode.d7.loss_cls: 0.2115, decode.d7.loss_mask: 0.4508, decode.d7.loss_dice: 0.6900, loss: 13.7265
2022-10-31 09:38:44,719 - mmseg - INFO - Iter [16600/20000]	lr: 5.190e-07, eta: 10:10:02, time: 2.608, data_time: 0.012, memory: 35591, decode.loss_cls: 0.2070, decode.loss_mask: 0.4555, decode.loss_dice: 0.6909, decode.d0.loss_cls: 1.4680, decode.d0.loss_mask: 0.4800, decode.d0.loss_dice: 0.7529, decode.d1.loss_cls: 0.2761, decode.d1.loss_mask: 0.4673, decode.d1.loss_dice: 0.7239, decode.d2.loss_cls: 0.2350, decode.d2.loss_mask: 0.4611, decode.d2.loss_dice: 0.7056, decode.d3.loss_cls: 0.2157, decode.d3.loss_mask: 0.4592, decode.d3.loss_dice: 0.6944, decode.d4.loss_cls: 0.2121, decode.d4.loss_mask: 0.4584, decode.d4.loss_dice: 0.6917, decode.d5.loss_cls: 0.2076, decode.d5.loss_mask: 0.4571, decode.d5.loss_dice: 0.6906, decode.d6.loss_cls: 0.2072, decode.d6.loss_mask: 0.4567, decode.d6.loss_dice: 0.6879, decode.d7.loss_cls: 0.2064, decode.d7.loss_mask: 0.4559, decode.d7.loss_dice: 0.6895, loss: 13.7136
2022-10-31 09:40:56,568 - mmseg - INFO - Iter [16650/20000]	lr: 5.114e-07, eta: 9:26:12, time: 2.637, data_time: 0.060, memory: 35591, decode.loss_cls: 0.2167, decode.loss_mask: 0.4488, decode.loss_dice: 0.6833, decode.d0.loss_cls: 1.4957, decode.d0.loss_mask: 0.4761, decode.d0.loss_dice: 0.7488, decode.d1.loss_cls: 0.2845, decode.d1.loss_mask: 0.4628, decode.d1.loss_dice: 0.7212, decode.d2.loss_cls: 0.2440, decode.d2.loss_mask: 0.4550, decode.d2.loss_dice: 0.6987, decode.d3.loss_cls: 0.2266, decode.d3.loss_mask: 0.4506, decode.d3.loss_dice: 0.6896, decode.d4.loss_cls: 0.2210, decode.d4.loss_mask: 0.4494, decode.d4.loss_dice: 0.6866, decode.d5.loss_cls: 0.2170, decode.d5.loss_mask: 0.4479, decode.d5.loss_dice: 0.6872, decode.d6.loss_cls: 0.2152, decode.d6.loss_mask: 0.4483, decode.d6.loss_dice: 0.6853, decode.d7.loss_cls: 0.2162, decode.d7.loss_mask: 0.4490, decode.d7.loss_dice: 0.6863, loss: 13.7118
2022-10-31 09:43:05,206 - mmseg - INFO - Iter [16700/20000]	lr: 5.038e-07, eta: 8:48:04, time: 2.572, data_time: 0.025, memory: 35591, decode.loss_cls: 0.2125, decode.loss_mask: 0.4482, decode.loss_dice: 0.6831, decode.d0.loss_cls: 1.4915, decode.d0.loss_mask: 0.4728, decode.d0.loss_dice: 0.7511, decode.d1.loss_cls: 0.2812, decode.d1.loss_mask: 0.4635, decode.d1.loss_dice: 0.7181, decode.d2.loss_cls: 0.2441, decode.d2.loss_mask: 0.4538, decode.d2.loss_dice: 0.6990, decode.d3.loss_cls: 0.2265, decode.d3.loss_mask: 0.4510, decode.d3.loss_dice: 0.6881, decode.d4.loss_cls: 0.2214, decode.d4.loss_mask: 0.4500, decode.d4.loss_dice: 0.6879, decode.d5.loss_cls: 0.2145, decode.d5.loss_mask: 0.4495, decode.d5.loss_dice: 0.6871, decode.d6.loss_cls: 0.2150, decode.d6.loss_mask: 0.4482, decode.d6.loss_dice: 0.6833, decode.d7.loss_cls: 0.2129, decode.d7.loss_mask: 0.4482, decode.d7.loss_dice: 0.6838, loss: 13.6865
2022-10-31 09:45:11,168 - mmseg - INFO - Iter [16750/20000]	lr: 4.961e-07, eta: 8:14:31, time: 2.520, data_time: 0.025, memory: 35591, decode.loss_cls: 0.2038, decode.loss_mask: 0.4624, decode.loss_dice: 0.6933, decode.d0.loss_cls: 1.4600, decode.d0.loss_mask: 0.4863, decode.d0.loss_dice: 0.7550, decode.d1.loss_cls: 0.2707, decode.d1.loss_mask: 0.4762, decode.d1.loss_dice: 0.7246, decode.d2.loss_cls: 0.2300, decode.d2.loss_mask: 0.4672, decode.d2.loss_dice: 0.7085, decode.d3.loss_cls: 0.2156, decode.d3.loss_mask: 0.4655, decode.d3.loss_dice: 0.7022, decode.d4.loss_cls: 0.2112, decode.d4.loss_mask: 0.4645, decode.d4.loss_dice: 0.6971, decode.d5.loss_cls: 0.2047, decode.d5.loss_mask: 0.4635, decode.d5.loss_dice: 0.6966, decode.d6.loss_cls: 0.2037, decode.d6.loss_mask: 0.4630, decode.d6.loss_dice: 0.6946, decode.d7.loss_cls: 0.2043, decode.d7.loss_mask: 0.4626, decode.d7.loss_dice: 0.6944, loss: 13.7817
2022-10-31 09:47:14,862 - mmseg - INFO - Iter [16800/20000]	lr: 4.885e-07, eta: 7:44:45, time: 2.474, data_time: 0.012, memory: 35591, decode.loss_cls: 0.2048, decode.loss_mask: 0.4447, decode.loss_dice: 0.6799, decode.d0.loss_cls: 1.4833, decode.d0.loss_mask: 0.4719, decode.d0.loss_dice: 0.7432, decode.d1.loss_cls: 0.2752, decode.d1.loss_mask: 0.4617, decode.d1.loss_dice: 0.7132, decode.d2.loss_cls: 0.2327, decode.d2.loss_mask: 0.4535, decode.d2.loss_dice: 0.6936, decode.d3.loss_cls: 0.2188, decode.d3.loss_mask: 0.4485, decode.d3.loss_dice: 0.6838, decode.d4.loss_cls: 0.2137, decode.d4.loss_mask: 0.4472, decode.d4.loss_dice: 0.6828, decode.d5.loss_cls: 0.2089, decode.d5.loss_mask: 0.4461, decode.d5.loss_dice: 0.6815, decode.d6.loss_cls: 0.2060, decode.d6.loss_mask: 0.4459, decode.d6.loss_dice: 0.6799, decode.d7.loss_cls: 0.2067, decode.d7.loss_mask: 0.4457, decode.d7.loss_dice: 0.6800, loss: 13.5534
2022-10-31 09:49:17,869 - mmseg - INFO - Iter [16850/20000]	lr: 4.809e-07, eta: 7:18:12, time: 2.460, data_time: 0.012, memory: 35591, decode.loss_cls: 0.2015, decode.loss_mask: 0.4506, decode.loss_dice: 0.6787, decode.d0.loss_cls: 1.4695, decode.d0.loss_mask: 0.4757, decode.d0.loss_dice: 0.7371, decode.d1.loss_cls: 0.2666, decode.d1.loss_mask: 0.4628, decode.d1.loss_dice: 0.7085, decode.d2.loss_cls: 0.2276, decode.d2.loss_mask: 0.4564, decode.d2.loss_dice: 0.6914, decode.d3.loss_cls: 0.2102, decode.d3.loss_mask: 0.4519, decode.d3.loss_dice: 0.6803, decode.d4.loss_cls: 0.2043, decode.d4.loss_mask: 0.4516, decode.d4.loss_dice: 0.6789, decode.d5.loss_cls: 0.2037, decode.d5.loss_mask: 0.4503, decode.d5.loss_dice: 0.6791, decode.d6.loss_cls: 0.2012, decode.d6.loss_mask: 0.4501, decode.d6.loss_dice: 0.6757, decode.d7.loss_cls: 0.2020, decode.d7.loss_mask: 0.4506, decode.d7.loss_dice: 0.6752, loss: 13.4915
2022-10-31 09:51:22,982 - mmseg - INFO - Iter [16900/20000]	lr: 4.732e-07, eta: 6:54:29, time: 2.502, data_time: 0.012, memory: 35591, decode.loss_cls: 0.2111, decode.loss_mask: 0.4587, decode.loss_dice: 0.6917, decode.d0.loss_cls: 1.4899, decode.d0.loss_mask: 0.4856, decode.d0.loss_dice: 0.7598, decode.d1.loss_cls: 0.2814, decode.d1.loss_mask: 0.4731, decode.d1.loss_dice: 0.7259, decode.d2.loss_cls: 0.2417, decode.d2.loss_mask: 0.4663, decode.d2.loss_dice: 0.7074, decode.d3.loss_cls: 0.2230, decode.d3.loss_mask: 0.4618, decode.d3.loss_dice: 0.7000, decode.d4.loss_cls: 0.2213, decode.d4.loss_mask: 0.4591, decode.d4.loss_dice: 0.6956, decode.d5.loss_cls: 0.2159, decode.d5.loss_mask: 0.4591, decode.d5.loss_dice: 0.6957, decode.d6.loss_cls: 0.2125, decode.d6.loss_mask: 0.4587, decode.d6.loss_dice: 0.6944, decode.d7.loss_cls: 0.2124, decode.d7.loss_mask: 0.4588, decode.d7.loss_dice: 0.6916, loss: 13.8526
2022-10-31 09:53:31,299 - mmseg - INFO - Iter [16950/20000]	lr: 4.656e-07, eta: 6:33:13, time: 2.566, data_time: 0.059, memory: 35591, decode.loss_cls: 0.2071, decode.loss_mask: 0.4480, decode.loss_dice: 0.6856, decode.d0.loss_cls: 1.4812, decode.d0.loss_mask: 0.4747, decode.d0.loss_dice: 0.7524, decode.d1.loss_cls: 0.2739, decode.d1.loss_mask: 0.4628, decode.d1.loss_dice: 0.7176, decode.d2.loss_cls: 0.2360, decode.d2.loss_mask: 0.4554, decode.d2.loss_dice: 0.7011, decode.d3.loss_cls: 0.2180, decode.d3.loss_mask: 0.4513, decode.d3.loss_dice: 0.6921, decode.d4.loss_cls: 0.2138, decode.d4.loss_mask: 0.4502, decode.d4.loss_dice: 0.6890, decode.d5.loss_cls: 0.2096, decode.d5.loss_mask: 0.4498, decode.d5.loss_dice: 0.6871, decode.d6.loss_cls: 0.2091, decode.d6.loss_mask: 0.4482, decode.d6.loss_dice: 0.6852, decode.d7.loss_cls: 0.2086, decode.d7.loss_mask: 0.4475, decode.d7.loss_dice: 0.6841, loss: 13.6393
2022-10-31 09:55:31,870 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-31 09:55:31,871 - mmseg - INFO - Iter [17000/20000]	lr: 4.580e-07, eta: 6:13:29, time: 2.411, data_time: 0.012, memory: 35591, decode.loss_cls: 0.2131, decode.loss_mask: 0.4550, decode.loss_dice: 0.6963, decode.d0.loss_cls: 1.4854, decode.d0.loss_mask: 0.4831, decode.d0.loss_dice: 0.7647, decode.d1.loss_cls: 0.2827, decode.d1.loss_mask: 0.4676, decode.d1.loss_dice: 0.7346, decode.d2.loss_cls: 0.2395, decode.d2.loss_mask: 0.4606, decode.d2.loss_dice: 0.7127, decode.d3.loss_cls: 0.2238, decode.d3.loss_mask: 0.4572, decode.d3.loss_dice: 0.7038, decode.d4.loss_cls: 0.2167, decode.d4.loss_mask: 0.4561, decode.d4.loss_dice: 0.7022, decode.d5.loss_cls: 0.2151, decode.d5.loss_mask: 0.4561, decode.d5.loss_dice: 0.7004, decode.d6.loss_cls: 0.2110, decode.d6.loss_mask: 0.4557, decode.d6.loss_dice: 0.6985, decode.d7.loss_cls: 0.2117, decode.d7.loss_mask: 0.4550, decode.d7.loss_dice: 0.6975, loss: 13.8559
2022-10-31 09:57:39,784 - mmseg - INFO - Iter [17050/20000]	lr: 4.503e-07, eta: 5:55:46, time: 2.558, data_time: 0.013, memory: 35591, decode.loss_cls: 0.2090, decode.loss_mask: 0.4426, decode.loss_dice: 0.6811, decode.d0.loss_cls: 1.4816, decode.d0.loss_mask: 0.4707, decode.d0.loss_dice: 0.7407, decode.d1.loss_cls: 0.2751, decode.d1.loss_mask: 0.4583, decode.d1.loss_dice: 0.7147, decode.d2.loss_cls: 0.2362, decode.d2.loss_mask: 0.4480, decode.d2.loss_dice: 0.6959, decode.d3.loss_cls: 0.2220, decode.d3.loss_mask: 0.4447, decode.d3.loss_dice: 0.6853, decode.d4.loss_cls: 0.2144, decode.d4.loss_mask: 0.4441, decode.d4.loss_dice: 0.6849, decode.d5.loss_cls: 0.2128, decode.d5.loss_mask: 0.4434, decode.d5.loss_dice: 0.6828, decode.d6.loss_cls: 0.2100, decode.d6.loss_mask: 0.4431, decode.d6.loss_dice: 0.6798, decode.d7.loss_cls: 0.2101, decode.d7.loss_mask: 0.4430, decode.d7.loss_dice: 0.6816, loss: 13.5558
2022-10-31 09:59:36,834 - mmseg - INFO - Iter [17100/20000]	lr: 4.427e-07, eta: 5:38:59, time: 2.341, data_time: 0.013, memory: 35591, decode.loss_cls: 0.2067, decode.loss_mask: 0.4393, decode.loss_dice: 0.6730, decode.d0.loss_cls: 1.4801, decode.d0.loss_mask: 0.4634, decode.d0.loss_dice: 0.7380, decode.d1.loss_cls: 0.2754, decode.d1.loss_mask: 0.4524, decode.d1.loss_dice: 0.7105, decode.d2.loss_cls: 0.2362, decode.d2.loss_mask: 0.4450, decode.d2.loss_dice: 0.6893, decode.d3.loss_cls: 0.2185, decode.d3.loss_mask: 0.4426, decode.d3.loss_dice: 0.6795, decode.d4.loss_cls: 0.2131, decode.d4.loss_mask: 0.4405, decode.d4.loss_dice: 0.6769, decode.d5.loss_cls: 0.2095, decode.d5.loss_mask: 0.4392, decode.d5.loss_dice: 0.6747, decode.d6.loss_cls: 0.2074, decode.d6.loss_mask: 0.4383, decode.d6.loss_dice: 0.6737, decode.d7.loss_cls: 0.2055, decode.d7.loss_mask: 0.4394, decode.d7.loss_dice: 0.6745, loss: 13.4425
2022-10-31 10:01:39,247 - mmseg - INFO - Iter [17150/20000]	lr: 4.351e-07, eta: 5:23:43, time: 2.448, data_time: 0.018, memory: 35591, decode.loss_cls: 0.2136, decode.loss_mask: 0.4556, decode.loss_dice: 0.6869, decode.d0.loss_cls: 1.4586, decode.d0.loss_mask: 0.4834, decode.d0.loss_dice: 0.7534, decode.d1.loss_cls: 0.2776, decode.d1.loss_mask: 0.4726, decode.d1.loss_dice: 0.7194, decode.d2.loss_cls: 0.2405, decode.d2.loss_mask: 0.4614, decode.d2.loss_dice: 0.7023, decode.d3.loss_cls: 0.2252, decode.d3.loss_mask: 0.4593, decode.d3.loss_dice: 0.6930, decode.d4.loss_cls: 0.2207, decode.d4.loss_mask: 0.4572, decode.d4.loss_dice: 0.6916, decode.d5.loss_cls: 0.2150, decode.d5.loss_mask: 0.4570, decode.d5.loss_dice: 0.6896, decode.d6.loss_cls: 0.2119, decode.d6.loss_mask: 0.4562, decode.d6.loss_dice: 0.6871, decode.d7.loss_cls: 0.2140, decode.d7.loss_mask: 0.4557, decode.d7.loss_dice: 0.6867, loss: 13.7455
2022-10-31 10:03:45,819 - mmseg - INFO - Iter [17200/20000]	lr: 4.275e-07, eta: 5:09:43, time: 2.531, data_time: 0.012, memory: 35591, decode.loss_cls: 0.2105, decode.loss_mask: 0.4547, decode.loss_dice: 0.6881, decode.d0.loss_cls: 1.4811, decode.d0.loss_mask: 0.4806, decode.d0.loss_dice: 0.7561, decode.d1.loss_cls: 0.2755, decode.d1.loss_mask: 0.4699, decode.d1.loss_dice: 0.7228, decode.d2.loss_cls: 0.2393, decode.d2.loss_mask: 0.4607, decode.d2.loss_dice: 0.7011, decode.d3.loss_cls: 0.2225, decode.d3.loss_mask: 0.4587, decode.d3.loss_dice: 0.6921, decode.d4.loss_cls: 0.2174, decode.d4.loss_mask: 0.4569, decode.d4.loss_dice: 0.6899, decode.d5.loss_cls: 0.2135, decode.d5.loss_mask: 0.4553, decode.d5.loss_dice: 0.6878, decode.d6.loss_cls: 0.2105, decode.d6.loss_mask: 0.4549, decode.d6.loss_dice: 0.6886, decode.d7.loss_cls: 0.2134, decode.d7.loss_mask: 0.4541, decode.d7.loss_dice: 0.6865, loss: 13.7427
2022-10-31 10:05:53,090 - mmseg - INFO - Iter [17250/20000]	lr: 4.198e-07, eta: 4:56:42, time: 2.546, data_time: 0.014, memory: 35591, decode.loss_cls: 0.2069, decode.loss_mask: 0.4480, decode.loss_dice: 0.6832, decode.d0.loss_cls: 1.4958, decode.d0.loss_mask: 0.4750, decode.d0.loss_dice: 0.7470, decode.d1.loss_cls: 0.2705, decode.d1.loss_mask: 0.4632, decode.d1.loss_dice: 0.7174, decode.d2.loss_cls: 0.2349, decode.d2.loss_mask: 0.4543, decode.d2.loss_dice: 0.6993, decode.d3.loss_cls: 0.2196, decode.d3.loss_mask: 0.4517, decode.d3.loss_dice: 0.6888, decode.d4.loss_cls: 0.2134, decode.d4.loss_mask: 0.4505, decode.d4.loss_dice: 0.6879, decode.d5.loss_cls: 0.2085, decode.d5.loss_mask: 0.4493, decode.d5.loss_dice: 0.6854, decode.d6.loss_cls: 0.2075, decode.d6.loss_mask: 0.4490, decode.d6.loss_dice: 0.6845, decode.d7.loss_cls: 0.2057, decode.d7.loss_mask: 0.4485, decode.d7.loss_dice: 0.6837, loss: 13.6295
2022-10-31 10:07:57,814 - mmseg - INFO - Iter [17300/20000]	lr: 4.122e-07, eta: 4:44:25, time: 2.494, data_time: 0.059, memory: 35591, decode.loss_cls: 0.2056, decode.loss_mask: 0.4478, decode.loss_dice: 0.6829, decode.d0.loss_cls: 1.4908, decode.d0.loss_mask: 0.4739, decode.d0.loss_dice: 0.7478, decode.d1.loss_cls: 0.2724, decode.d1.loss_mask: 0.4622, decode.d1.loss_dice: 0.7161, decode.d2.loss_cls: 0.2357, decode.d2.loss_mask: 0.4543, decode.d2.loss_dice: 0.6980, decode.d3.loss_cls: 0.2163, decode.d3.loss_mask: 0.4513, decode.d3.loss_dice: 0.6844, decode.d4.loss_cls: 0.2134, decode.d4.loss_mask: 0.4493, decode.d4.loss_dice: 0.6866, decode.d5.loss_cls: 0.2071, decode.d5.loss_mask: 0.4490, decode.d5.loss_dice: 0.6833, decode.d6.loss_cls: 0.2053, decode.d6.loss_mask: 0.4476, decode.d6.loss_dice: 0.6846, decode.d7.loss_cls: 0.2048, decode.d7.loss_mask: 0.4478, decode.d7.loss_dice: 0.6835, loss: 13.6016
2022-10-31 10:10:05,816 - mmseg - INFO - Iter [17350/20000]	lr: 4.046e-07, eta: 4:33:00, time: 2.560, data_time: 0.013, memory: 35591, decode.loss_cls: 0.2018, decode.loss_mask: 0.4556, decode.loss_dice: 0.6868, decode.d0.loss_cls: 1.4768, decode.d0.loss_mask: 0.4795, decode.d0.loss_dice: 0.7473, decode.d1.loss_cls: 0.2708, decode.d1.loss_mask: 0.4696, decode.d1.loss_dice: 0.7187, decode.d2.loss_cls: 0.2319, decode.d2.loss_mask: 0.4621, decode.d2.loss_dice: 0.7014, decode.d3.loss_cls: 0.2155, decode.d3.loss_mask: 0.4579, decode.d3.loss_dice: 0.6918, decode.d4.loss_cls: 0.2093, decode.d4.loss_mask: 0.4563, decode.d4.loss_dice: 0.6904, decode.d5.loss_cls: 0.2057, decode.d5.loss_mask: 0.4565, decode.d5.loss_dice: 0.6891, decode.d6.loss_cls: 0.2019, decode.d6.loss_mask: 0.4555, decode.d6.loss_dice: 0.6863, decode.d7.loss_cls: 0.2025, decode.d7.loss_mask: 0.4548, decode.d7.loss_dice: 0.6875, loss: 13.6634
2022-10-31 10:12:13,962 - mmseg - INFO - Iter [17400/20000]	lr: 3.969e-07, eta: 4:22:15, time: 2.563, data_time: 0.014, memory: 35591, decode.loss_cls: 0.1910, decode.loss_mask: 0.4385, decode.loss_dice: 0.6733, decode.d0.loss_cls: 1.4677, decode.d0.loss_mask: 0.4621, decode.d0.loss_dice: 0.7344, decode.d1.loss_cls: 0.2563, decode.d1.loss_mask: 0.4520, decode.d1.loss_dice: 0.7045, decode.d2.loss_cls: 0.2177, decode.d2.loss_mask: 0.4437, decode.d2.loss_dice: 0.6867, decode.d3.loss_cls: 0.2025, decode.d3.loss_mask: 0.4417, decode.d3.loss_dice: 0.6784, decode.d4.loss_cls: 0.1963, decode.d4.loss_mask: 0.4403, decode.d4.loss_dice: 0.6781, decode.d5.loss_cls: 0.1912, decode.d5.loss_mask: 0.4412, decode.d5.loss_dice: 0.6763, decode.d6.loss_cls: 0.1897, decode.d6.loss_mask: 0.4391, decode.d6.loss_dice: 0.6751, decode.d7.loss_cls: 0.1892, decode.d7.loss_mask: 0.4389, decode.d7.loss_dice: 0.6747, loss: 13.2806
2022-10-31 10:14:18,929 - mmseg - INFO - Iter [17450/20000]	lr: 3.893e-07, eta: 4:12:01, time: 2.499, data_time: 0.013, memory: 35591, decode.loss_cls: 0.2091, decode.loss_mask: 0.4503, decode.loss_dice: 0.6868, decode.d0.loss_cls: 1.4905, decode.d0.loss_mask: 0.4775, decode.d0.loss_dice: 0.7547, decode.d1.loss_cls: 0.2770, decode.d1.loss_mask: 0.4678, decode.d1.loss_dice: 0.7219, decode.d2.loss_cls: 0.2381, decode.d2.loss_mask: 0.4578, decode.d2.loss_dice: 0.7011, decode.d3.loss_cls: 0.2225, decode.d3.loss_mask: 0.4533, decode.d3.loss_dice: 0.6925, decode.d4.loss_cls: 0.2153, decode.d4.loss_mask: 0.4526, decode.d4.loss_dice: 0.6904, decode.d5.loss_cls: 0.2093, decode.d5.loss_mask: 0.4506, decode.d5.loss_dice: 0.6905, decode.d6.loss_cls: 0.2088, decode.d6.loss_mask: 0.4499, decode.d6.loss_dice: 0.6875, decode.d7.loss_cls: 0.2071, decode.d7.loss_mask: 0.4504, decode.d7.loss_dice: 0.6854, loss: 13.6988
2022-10-31 10:16:26,014 - mmseg - INFO - Iter [17500/20000]	lr: 3.817e-07, eta: 4:02:22, time: 2.542, data_time: 0.016, memory: 35591, decode.loss_cls: 0.2026, decode.loss_mask: 0.4510, decode.loss_dice: 0.6799, decode.d0.loss_cls: 1.4652, decode.d0.loss_mask: 0.4763, decode.d0.loss_dice: 0.7471, decode.d1.loss_cls: 0.2665, decode.d1.loss_mask: 0.4664, decode.d1.loss_dice: 0.7149, decode.d2.loss_cls: 0.2286, decode.d2.loss_mask: 0.4586, decode.d2.loss_dice: 0.6949, decode.d3.loss_cls: 0.2152, decode.d3.loss_mask: 0.4546, decode.d3.loss_dice: 0.6872, decode.d4.loss_cls: 0.2078, decode.d4.loss_mask: 0.4526, decode.d4.loss_dice: 0.6855, decode.d5.loss_cls: 0.2035, decode.d5.loss_mask: 0.4524, decode.d5.loss_dice: 0.6861, decode.d6.loss_cls: 0.2010, decode.d6.loss_mask: 0.4514, decode.d6.loss_dice: 0.6825, decode.d7.loss_cls: 0.2020, decode.d7.loss_mask: 0.4510, decode.d7.loss_dice: 0.6795, loss: 13.5640
2022-10-31 10:18:31,768 - mmseg - INFO - Iter [17550/20000]	lr: 3.740e-07, eta: 3:53:10, time: 2.515, data_time: 0.013, memory: 35591, decode.loss_cls: 0.2044, decode.loss_mask: 0.4497, decode.loss_dice: 0.6853, decode.d0.loss_cls: 1.4862, decode.d0.loss_mask: 0.4759, decode.d0.loss_dice: 0.7508, decode.d1.loss_cls: 0.2723, decode.d1.loss_mask: 0.4637, decode.d1.loss_dice: 0.7219, decode.d2.loss_cls: 0.2351, decode.d2.loss_mask: 0.4566, decode.d2.loss_dice: 0.7007, decode.d3.loss_cls: 0.2188, decode.d3.loss_mask: 0.4520, decode.d3.loss_dice: 0.6903, decode.d4.loss_cls: 0.2115, decode.d4.loss_mask: 0.4513, decode.d4.loss_dice: 0.6915, decode.d5.loss_cls: 0.2078, decode.d5.loss_mask: 0.4497, decode.d5.loss_dice: 0.6897, decode.d6.loss_cls: 0.2057, decode.d6.loss_mask: 0.4495, decode.d6.loss_dice: 0.6865, decode.d7.loss_cls: 0.2042, decode.d7.loss_mask: 0.4493, decode.d7.loss_dice: 0.6872, loss: 13.6475
2022-10-31 10:20:35,714 - mmseg - INFO - Iter [17600/20000]	lr: 3.664e-07, eta: 3:44:23, time: 2.479, data_time: 0.058, memory: 35591, decode.loss_cls: 0.1932, decode.loss_mask: 0.4408, decode.loss_dice: 0.6689, decode.d0.loss_cls: 1.4560, decode.d0.loss_mask: 0.4669, decode.d0.loss_dice: 0.7274, decode.d1.loss_cls: 0.2598, decode.d1.loss_mask: 0.4552, decode.d1.loss_dice: 0.7015, decode.d2.loss_cls: 0.2238, decode.d2.loss_mask: 0.4470, decode.d2.loss_dice: 0.6812, decode.d3.loss_cls: 0.2082, decode.d3.loss_mask: 0.4427, decode.d3.loss_dice: 0.6692, decode.d4.loss_cls: 0.2017, decode.d4.loss_mask: 0.4439, decode.d4.loss_dice: 0.6719, decode.d5.loss_cls: 0.1990, decode.d5.loss_mask: 0.4418, decode.d5.loss_dice: 0.6694, decode.d6.loss_cls: 0.1968, decode.d6.loss_mask: 0.4409, decode.d6.loss_dice: 0.6668, decode.d7.loss_cls: 0.1947, decode.d7.loss_mask: 0.4405, decode.d7.loss_dice: 0.6669, loss: 13.2762
2022-10-31 10:22:42,978 - mmseg - INFO - Iter [17650/20000]	lr: 3.588e-07, eta: 3:36:04, time: 2.545, data_time: 0.013, memory: 35591, decode.loss_cls: 0.1985, decode.loss_mask: 0.4468, decode.loss_dice: 0.6795, decode.d0.loss_cls: 1.4745, decode.d0.loss_mask: 0.4722, decode.d0.loss_dice: 0.7407, decode.d1.loss_cls: 0.2644, decode.d1.loss_mask: 0.4597, decode.d1.loss_dice: 0.7129, decode.d2.loss_cls: 0.2270, decode.d2.loss_mask: 0.4522, decode.d2.loss_dice: 0.6936, decode.d3.loss_cls: 0.2107, decode.d3.loss_mask: 0.4496, decode.d3.loss_dice: 0.6849, decode.d4.loss_cls: 0.2044, decode.d4.loss_mask: 0.4482, decode.d4.loss_dice: 0.6843, decode.d5.loss_cls: 0.2019, decode.d5.loss_mask: 0.4478, decode.d5.loss_dice: 0.6823, decode.d6.loss_cls: 0.1973, decode.d6.loss_mask: 0.4479, decode.d6.loss_dice: 0.6820, decode.d7.loss_cls: 0.1982, decode.d7.loss_mask: 0.4471, decode.d7.loss_dice: 0.6788, loss: 13.4875
2022-10-31 10:24:48,198 - mmseg - INFO - Iter [17700/20000]	lr: 3.512e-07, eta: 3:28:05, time: 2.504, data_time: 0.013, memory: 35591, decode.loss_cls: 0.2090, decode.loss_mask: 0.4481, decode.loss_dice: 0.6855, decode.d0.loss_cls: 1.4722, decode.d0.loss_mask: 0.4734, decode.d0.loss_dice: 0.7490, decode.d1.loss_cls: 0.2760, decode.d1.loss_mask: 0.4623, decode.d1.loss_dice: 0.7189, decode.d2.loss_cls: 0.2356, decode.d2.loss_mask: 0.4551, decode.d2.loss_dice: 0.7015, decode.d3.loss_cls: 0.2217, decode.d3.loss_mask: 0.4503, decode.d3.loss_dice: 0.6912, decode.d4.loss_cls: 0.2148, decode.d4.loss_mask: 0.4497, decode.d4.loss_dice: 0.6895, decode.d5.loss_cls: 0.2095, decode.d5.loss_mask: 0.4503, decode.d5.loss_dice: 0.6900, decode.d6.loss_cls: 0.2085, decode.d6.loss_mask: 0.4487, decode.d6.loss_dice: 0.6868, decode.d7.loss_cls: 0.2085, decode.d7.loss_mask: 0.4489, decode.d7.loss_dice: 0.6858, loss: 13.6409
2022-10-31 10:26:50,850 - mmseg - INFO - Iter [17750/20000]	lr: 3.435e-07, eta: 3:20:22, time: 2.453, data_time: 0.013, memory: 35591, decode.loss_cls: 0.2025, decode.loss_mask: 0.4483, decode.loss_dice: 0.6808, decode.d0.loss_cls: 1.4808, decode.d0.loss_mask: 0.4743, decode.d0.loss_dice: 0.7471, decode.d1.loss_cls: 0.2763, decode.d1.loss_mask: 0.4611, decode.d1.loss_dice: 0.7118, decode.d2.loss_cls: 0.2374, decode.d2.loss_mask: 0.4527, decode.d2.loss_dice: 0.6922, decode.d3.loss_cls: 0.2172, decode.d3.loss_mask: 0.4502, decode.d3.loss_dice: 0.6860, decode.d4.loss_cls: 0.2119, decode.d4.loss_mask: 0.4490, decode.d4.loss_dice: 0.6846, decode.d5.loss_cls: 0.2083, decode.d5.loss_mask: 0.4485, decode.d5.loss_dice: 0.6845, decode.d6.loss_cls: 0.2057, decode.d6.loss_mask: 0.4473, decode.d6.loss_dice: 0.6821, decode.d7.loss_cls: 0.2052, decode.d7.loss_mask: 0.4476, decode.d7.loss_dice: 0.6813, loss: 13.5746
2022-10-31 10:28:45,932 - mmseg - INFO - Iter [17800/20000]	lr: 3.359e-07, eta: 3:12:49, time: 2.302, data_time: 0.013, memory: 35591, decode.loss_cls: 0.1996, decode.loss_mask: 0.4431, decode.loss_dice: 0.6830, decode.d0.loss_cls: 1.4529, decode.d0.loss_mask: 0.4705, decode.d0.loss_dice: 0.7463, decode.d1.loss_cls: 0.2644, decode.d1.loss_mask: 0.4560, decode.d1.loss_dice: 0.7161, decode.d2.loss_cls: 0.2265, decode.d2.loss_mask: 0.4489, decode.d2.loss_dice: 0.6980, decode.d3.loss_cls: 0.2097, decode.d3.loss_mask: 0.4472, decode.d3.loss_dice: 0.6862, decode.d4.loss_cls: 0.2059, decode.d4.loss_mask: 0.4449, decode.d4.loss_dice: 0.6856, decode.d5.loss_cls: 0.2033, decode.d5.loss_mask: 0.4448, decode.d5.loss_dice: 0.6840, decode.d6.loss_cls: 0.2007, decode.d6.loss_mask: 0.4440, decode.d6.loss_dice: 0.6828, decode.d7.loss_cls: 0.1992, decode.d7.loss_mask: 0.4427, decode.d7.loss_dice: 0.6824, loss: 13.4690
2022-10-31 10:30:48,136 - mmseg - INFO - Iter [17850/20000]	lr: 3.283e-07, eta: 3:05:43, time: 2.444, data_time: 0.013, memory: 35591, decode.loss_cls: 0.1946, decode.loss_mask: 0.4391, decode.loss_dice: 0.6713, decode.d0.loss_cls: 1.4566, decode.d0.loss_mask: 0.4646, decode.d0.loss_dice: 0.7318, decode.d1.loss_cls: 0.2614, decode.d1.loss_mask: 0.4546, decode.d1.loss_dice: 0.7015, decode.d2.loss_cls: 0.2273, decode.d2.loss_mask: 0.4443, decode.d2.loss_dice: 0.6826, decode.d3.loss_cls: 0.2103, decode.d3.loss_mask: 0.4408, decode.d3.loss_dice: 0.6738, decode.d4.loss_cls: 0.2052, decode.d4.loss_mask: 0.4405, decode.d4.loss_dice: 0.6736, decode.d5.loss_cls: 0.2007, decode.d5.loss_mask: 0.4399, decode.d5.loss_dice: 0.6704, decode.d6.loss_cls: 0.1982, decode.d6.loss_mask: 0.4392, decode.d6.loss_dice: 0.6703, decode.d7.loss_cls: 0.1966, decode.d7.loss_mask: 0.4395, decode.d7.loss_dice: 0.6690, loss: 13.2978
2022-10-31 10:32:48,588 - mmseg - INFO - Iter [17900/20000]	lr: 3.206e-07, eta: 2:58:50, time: 2.408, data_time: 0.059, memory: 35597, decode.loss_cls: 0.1978, decode.loss_mask: 0.4497, decode.loss_dice: 0.6821, decode.d0.loss_cls: 1.4650, decode.d0.loss_mask: 0.4796, decode.d0.loss_dice: 0.7438, decode.d1.loss_cls: 0.2608, decode.d1.loss_mask: 0.4661, decode.d1.loss_dice: 0.7092, decode.d2.loss_cls: 0.2239, decode.d2.loss_mask: 0.4582, decode.d2.loss_dice: 0.6976, decode.d3.loss_cls: 0.2090, decode.d3.loss_mask: 0.4545, decode.d3.loss_dice: 0.6882, decode.d4.loss_cls: 0.2025, decode.d4.loss_mask: 0.4535, decode.d4.loss_dice: 0.6859, decode.d5.loss_cls: 0.2017, decode.d5.loss_mask: 0.4521, decode.d5.loss_dice: 0.6822, decode.d6.loss_cls: 0.1980, decode.d6.loss_mask: 0.4508, decode.d6.loss_dice: 0.6828, decode.d7.loss_cls: 0.1955, decode.d7.loss_mask: 0.4507, decode.d7.loss_dice: 0.6834, loss: 13.5247
2022-10-31 10:34:53,081 - mmseg - INFO - Iter [17950/20000]	lr: 3.130e-07, eta: 2:52:17, time: 2.491, data_time: 0.015, memory: 35597, decode.loss_cls: 0.1933, decode.loss_mask: 0.4488, decode.loss_dice: 0.6830, decode.d0.loss_cls: 1.4647, decode.d0.loss_mask: 0.4770, decode.d0.loss_dice: 0.7479, decode.d1.loss_cls: 0.2629, decode.d1.loss_mask: 0.4634, decode.d1.loss_dice: 0.7160, decode.d2.loss_cls: 0.2243, decode.d2.loss_mask: 0.4539, decode.d2.loss_dice: 0.6978, decode.d3.loss_cls: 0.2055, decode.d3.loss_mask: 0.4510, decode.d3.loss_dice: 0.6896, decode.d4.loss_cls: 0.2013, decode.d4.loss_mask: 0.4500, decode.d4.loss_dice: 0.6872, decode.d5.loss_cls: 0.1966, decode.d5.loss_mask: 0.4495, decode.d5.loss_dice: 0.6852, decode.d6.loss_cls: 0.1947, decode.d6.loss_mask: 0.4484, decode.d6.loss_dice: 0.6852, decode.d7.loss_cls: 0.1934, decode.d7.loss_mask: 0.4490, decode.d7.loss_dice: 0.6840, loss: 13.5035
2022-10-31 10:37:00,761 - mmseg - INFO - Saving checkpoint at 18000 iterations
2022-10-31 10:37:33,400 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-31 10:37:33,401 - mmseg - INFO - Iter [18000/20000]	lr: 3.054e-07, eta: 2:46:33, time: 3.206, data_time: 0.014, memory: 35597, decode.loss_cls: 0.2037, decode.loss_mask: 0.4515, decode.loss_dice: 0.6801, decode.d0.loss_cls: 1.4508, decode.d0.loss_mask: 0.4778, decode.d0.loss_dice: 0.7408, decode.d1.loss_cls: 0.2656, decode.d1.loss_mask: 0.4661, decode.d1.loss_dice: 0.7096, decode.d2.loss_cls: 0.2313, decode.d2.loss_mask: 0.4569, decode.d2.loss_dice: 0.6930, decode.d3.loss_cls: 0.2138, decode.d3.loss_mask: 0.4543, decode.d3.loss_dice: 0.6848, decode.d4.loss_cls: 0.2096, decode.d4.loss_mask: 0.4533, decode.d4.loss_dice: 0.6854, decode.d5.loss_cls: 0.2042, decode.d5.loss_mask: 0.4526, decode.d5.loss_dice: 0.6821, decode.d6.loss_cls: 0.2031, decode.d6.loss_mask: 0.4515, decode.d6.loss_dice: 0.6771, decode.d7.loss_cls: 0.2009, decode.d7.loss_mask: 0.4516, decode.d7.loss_dice: 0.6801, loss: 13.5315
2022-10-31 10:39:34,941 - mmseg - INFO - Iter [18050/20000]	lr: 2.977e-07, eta: 2:40:21, time: 2.431, data_time: 0.014, memory: 35597, decode.loss_cls: 0.2013, decode.loss_mask: 0.4464, decode.loss_dice: 0.6750, decode.d0.loss_cls: 1.4641, decode.d0.loss_mask: 0.4743, decode.d0.loss_dice: 0.7380, decode.d1.loss_cls: 0.2664, decode.d1.loss_mask: 0.4591, decode.d1.loss_dice: 0.7057, decode.d2.loss_cls: 0.2273, decode.d2.loss_mask: 0.4528, decode.d2.loss_dice: 0.6865, decode.d3.loss_cls: 0.2078, decode.d3.loss_mask: 0.4510, decode.d3.loss_dice: 0.6797, decode.d4.loss_cls: 0.2055, decode.d4.loss_mask: 0.4496, decode.d4.loss_dice: 0.6808, decode.d5.loss_cls: 0.2033, decode.d5.loss_mask: 0.4488, decode.d5.loss_dice: 0.6751, decode.d6.loss_cls: 0.1998, decode.d6.loss_mask: 0.4485, decode.d6.loss_dice: 0.6734, decode.d7.loss_cls: 0.2009, decode.d7.loss_mask: 0.4473, decode.d7.loss_dice: 0.6741, loss: 13.4424
2022-10-31 10:41:35,252 - mmseg - INFO - Iter [18100/20000]	lr: 2.901e-07, eta: 2:34:20, time: 2.406, data_time: 0.014, memory: 35597, decode.loss_cls: 0.2130, decode.loss_mask: 0.4492, decode.loss_dice: 0.6878, decode.d0.loss_cls: 1.4876, decode.d0.loss_mask: 0.4757, decode.d0.loss_dice: 0.7553, decode.d1.loss_cls: 0.2813, decode.d1.loss_mask: 0.4633, decode.d1.loss_dice: 0.7218, decode.d2.loss_cls: 0.2413, decode.d2.loss_mask: 0.4549, decode.d2.loss_dice: 0.7029, decode.d3.loss_cls: 0.2267, decode.d3.loss_mask: 0.4506, decode.d3.loss_dice: 0.6909, decode.d4.loss_cls: 0.2212, decode.d4.loss_mask: 0.4492, decode.d4.loss_dice: 0.6913, decode.d5.loss_cls: 0.2149, decode.d5.loss_mask: 0.4489, decode.d5.loss_dice: 0.6923, decode.d6.loss_cls: 0.2126, decode.d6.loss_mask: 0.4494, decode.d6.loss_dice: 0.6895, decode.d7.loss_cls: 0.2111, decode.d7.loss_mask: 0.4480, decode.d7.loss_dice: 0.6871, loss: 13.7179
2022-10-31 10:43:39,900 - mmseg - INFO - Iter [18150/20000]	lr: 2.825e-07, eta: 2:28:34, time: 2.493, data_time: 0.013, memory: 35597, decode.loss_cls: 0.2058, decode.loss_mask: 0.4491, decode.loss_dice: 0.6847, decode.d0.loss_cls: 1.4571, decode.d0.loss_mask: 0.4788, decode.d0.loss_dice: 0.7504, decode.d1.loss_cls: 0.2704, decode.d1.loss_mask: 0.4681, decode.d1.loss_dice: 0.7202, decode.d2.loss_cls: 0.2358, decode.d2.loss_mask: 0.4581, decode.d2.loss_dice: 0.6998, decode.d3.loss_cls: 0.2194, decode.d3.loss_mask: 0.4539, decode.d3.loss_dice: 0.6891, decode.d4.loss_cls: 0.2136, decode.d4.loss_mask: 0.4525, decode.d4.loss_dice: 0.6874, decode.d5.loss_cls: 0.2088, decode.d5.loss_mask: 0.4518, decode.d5.loss_dice: 0.6860, decode.d6.loss_cls: 0.2073, decode.d6.loss_mask: 0.4505, decode.d6.loss_dice: 0.6845, decode.d7.loss_cls: 0.2057, decode.d7.loss_mask: 0.4505, decode.d7.loss_dice: 0.6845, loss: 13.6238
2022-10-31 10:45:41,190 - mmseg - INFO - Iter [18200/20000]	lr: 2.748e-07, eta: 2:22:55, time: 2.426, data_time: 0.014, memory: 35597, decode.loss_cls: 0.1957, decode.loss_mask: 0.4454, decode.loss_dice: 0.6770, decode.d0.loss_cls: 1.4807, decode.d0.loss_mask: 0.4708, decode.d0.loss_dice: 0.7417, decode.d1.loss_cls: 0.2633, decode.d1.loss_mask: 0.4606, decode.d1.loss_dice: 0.7120, decode.d2.loss_cls: 0.2246, decode.d2.loss_mask: 0.4524, decode.d2.loss_dice: 0.6908, decode.d3.loss_cls: 0.2089, decode.d3.loss_mask: 0.4488, decode.d3.loss_dice: 0.6823, decode.d4.loss_cls: 0.2043, decode.d4.loss_mask: 0.4476, decode.d4.loss_dice: 0.6795, decode.d5.loss_cls: 0.1998, decode.d5.loss_mask: 0.4471, decode.d5.loss_dice: 0.6775, decode.d6.loss_cls: 0.1962, decode.d6.loss_mask: 0.4465, decode.d6.loss_dice: 0.6754, decode.d7.loss_cls: 0.1944, decode.d7.loss_mask: 0.4464, decode.d7.loss_dice: 0.6767, loss: 13.4463
2022-10-31 10:47:44,386 - mmseg - INFO - Iter [18250/20000]	lr: 2.672e-07, eta: 2:17:28, time: 2.464, data_time: 0.058, memory: 35597, decode.loss_cls: 0.2035, decode.loss_mask: 0.4448, decode.loss_dice: 0.6796, decode.d0.loss_cls: 1.4653, decode.d0.loss_mask: 0.4697, decode.d0.loss_dice: 0.7421, decode.d1.loss_cls: 0.2703, decode.d1.loss_mask: 0.4602, decode.d1.loss_dice: 0.7162, decode.d2.loss_cls: 0.2328, decode.d2.loss_mask: 0.4513, decode.d2.loss_dice: 0.6971, decode.d3.loss_cls: 0.2167, decode.d3.loss_mask: 0.4474, decode.d3.loss_dice: 0.6868, decode.d4.loss_cls: 0.2122, decode.d4.loss_mask: 0.4455, decode.d4.loss_dice: 0.6873, decode.d5.loss_cls: 0.2087, decode.d5.loss_mask: 0.4457, decode.d5.loss_dice: 0.6845, decode.d6.loss_cls: 0.2053, decode.d6.loss_mask: 0.4451, decode.d6.loss_dice: 0.6825, decode.d7.loss_cls: 0.2034, decode.d7.loss_mask: 0.4453, decode.d7.loss_dice: 0.6816, loss: 13.5308
2022-10-31 10:49:43,988 - mmseg - INFO - Iter [18300/20000]	lr: 2.596e-07, eta: 2:12:06, time: 2.392, data_time: 0.014, memory: 35597, decode.loss_cls: 0.1982, decode.loss_mask: 0.4436, decode.loss_dice: 0.6755, decode.d0.loss_cls: 1.4816, decode.d0.loss_mask: 0.4686, decode.d0.loss_dice: 0.7356, decode.d1.loss_cls: 0.2680, decode.d1.loss_mask: 0.4583, decode.d1.loss_dice: 0.7075, decode.d2.loss_cls: 0.2309, decode.d2.loss_mask: 0.4495, decode.d2.loss_dice: 0.6922, decode.d3.loss_cls: 0.2113, decode.d3.loss_mask: 0.4466, decode.d3.loss_dice: 0.6832, decode.d4.loss_cls: 0.2062, decode.d4.loss_mask: 0.4453, decode.d4.loss_dice: 0.6784, decode.d5.loss_cls: 0.2018, decode.d5.loss_mask: 0.4452, decode.d5.loss_dice: 0.6797, decode.d6.loss_cls: 0.2001, decode.d6.loss_mask: 0.4441, decode.d6.loss_dice: 0.6767, decode.d7.loss_cls: 0.2001, decode.d7.loss_mask: 0.4434, decode.d7.loss_dice: 0.6748, loss: 13.4463
2022-10-31 10:51:42,797 - mmseg - INFO - Iter [18350/20000]	lr: 2.520e-07, eta: 2:06:53, time: 2.376, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1931, decode.loss_mask: 0.4467, decode.loss_dice: 0.6745, decode.d0.loss_cls: 1.4606, decode.d0.loss_mask: 0.4745, decode.d0.loss_dice: 0.7397, decode.d1.loss_cls: 0.2620, decode.d1.loss_mask: 0.4614, decode.d1.loss_dice: 0.7097, decode.d2.loss_cls: 0.2232, decode.d2.loss_mask: 0.4526, decode.d2.loss_dice: 0.6913, decode.d3.loss_cls: 0.2050, decode.d3.loss_mask: 0.4498, decode.d3.loss_dice: 0.6840, decode.d4.loss_cls: 0.1994, decode.d4.loss_mask: 0.4478, decode.d4.loss_dice: 0.6826, decode.d5.loss_cls: 0.1959, decode.d5.loss_mask: 0.4480, decode.d5.loss_dice: 0.6815, decode.d6.loss_cls: 0.1918, decode.d6.loss_mask: 0.4471, decode.d6.loss_dice: 0.6789, decode.d7.loss_cls: 0.1912, decode.d7.loss_mask: 0.4465, decode.d7.loss_dice: 0.6757, loss: 13.4145
2022-10-31 10:53:45,120 - mmseg - INFO - Iter [18400/20000]	lr: 2.443e-07, eta: 2:01:50, time: 2.446, data_time: 0.014, memory: 35597, decode.loss_cls: 0.1953, decode.loss_mask: 0.4361, decode.loss_dice: 0.6652, decode.d0.loss_cls: 1.4550, decode.d0.loss_mask: 0.4645, decode.d0.loss_dice: 0.7323, decode.d1.loss_cls: 0.2659, decode.d1.loss_mask: 0.4492, decode.d1.loss_dice: 0.7004, decode.d2.loss_cls: 0.2270, decode.d2.loss_mask: 0.4413, decode.d2.loss_dice: 0.6804, decode.d3.loss_cls: 0.2103, decode.d3.loss_mask: 0.4392, decode.d3.loss_dice: 0.6705, decode.d4.loss_cls: 0.2036, decode.d4.loss_mask: 0.4370, decode.d4.loss_dice: 0.6711, decode.d5.loss_cls: 0.1974, decode.d5.loss_mask: 0.4368, decode.d5.loss_dice: 0.6683, decode.d6.loss_cls: 0.1967, decode.d6.loss_mask: 0.4357, decode.d6.loss_dice: 0.6656, decode.d7.loss_cls: 0.1948, decode.d7.loss_mask: 0.4354, decode.d7.loss_dice: 0.6653, loss: 13.2405
2022-10-31 10:55:50,124 - mmseg - INFO - Iter [18450/20000]	lr: 2.367e-07, eta: 1:56:56, time: 2.500, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1934, decode.loss_mask: 0.4414, decode.loss_dice: 0.6751, decode.d0.loss_cls: 1.4641, decode.d0.loss_mask: 0.4678, decode.d0.loss_dice: 0.7392, decode.d1.loss_cls: 0.2604, decode.d1.loss_mask: 0.4573, decode.d1.loss_dice: 0.7105, decode.d2.loss_cls: 0.2228, decode.d2.loss_mask: 0.4479, decode.d2.loss_dice: 0.6899, decode.d3.loss_cls: 0.2053, decode.d3.loss_mask: 0.4451, decode.d3.loss_dice: 0.6797, decode.d4.loss_cls: 0.2017, decode.d4.loss_mask: 0.4441, decode.d4.loss_dice: 0.6803, decode.d5.loss_cls: 0.1970, decode.d5.loss_mask: 0.4427, decode.d5.loss_dice: 0.6777, decode.d6.loss_cls: 0.1959, decode.d6.loss_mask: 0.4428, decode.d6.loss_dice: 0.6740, decode.d7.loss_cls: 0.1923, decode.d7.loss_mask: 0.4420, decode.d7.loss_dice: 0.6749, loss: 13.3653
2022-10-31 10:57:50,352 - mmseg - INFO - Iter [18500/20000]	lr: 2.291e-07, eta: 1:52:06, time: 2.405, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1853, decode.loss_mask: 0.4413, decode.loss_dice: 0.6632, decode.d0.loss_cls: 1.4425, decode.d0.loss_mask: 0.4674, decode.d0.loss_dice: 0.7273, decode.d1.loss_cls: 0.2505, decode.d1.loss_mask: 0.4549, decode.d1.loss_dice: 0.6990, decode.d2.loss_cls: 0.2114, decode.d2.loss_mask: 0.4484, decode.d2.loss_dice: 0.6798, decode.d3.loss_cls: 0.1995, decode.d3.loss_mask: 0.4447, decode.d3.loss_dice: 0.6688, decode.d4.loss_cls: 0.1914, decode.d4.loss_mask: 0.4435, decode.d4.loss_dice: 0.6683, decode.d5.loss_cls: 0.1895, decode.d5.loss_mask: 0.4423, decode.d5.loss_dice: 0.6648, decode.d6.loss_cls: 0.1868, decode.d6.loss_mask: 0.4423, decode.d6.loss_dice: 0.6670, decode.d7.loss_cls: 0.1866, decode.d7.loss_mask: 0.4420, decode.d7.loss_dice: 0.6650, loss: 13.1733
2022-10-31 10:59:56,397 - mmseg - INFO - Iter [18550/20000]	lr: 2.214e-07, eta: 1:47:26, time: 2.521, data_time: 0.057, memory: 35597, decode.loss_cls: 0.2009, decode.loss_mask: 0.4452, decode.loss_dice: 0.6874, decode.d0.loss_cls: 1.4527, decode.d0.loss_mask: 0.4731, decode.d0.loss_dice: 0.7538, decode.d1.loss_cls: 0.2719, decode.d1.loss_mask: 0.4595, decode.d1.loss_dice: 0.7225, decode.d2.loss_cls: 0.2295, decode.d2.loss_mask: 0.4516, decode.d2.loss_dice: 0.7029, decode.d3.loss_cls: 0.2143, decode.d3.loss_mask: 0.4474, decode.d3.loss_dice: 0.6927, decode.d4.loss_cls: 0.2055, decode.d4.loss_mask: 0.4470, decode.d4.loss_dice: 0.6917, decode.d5.loss_cls: 0.2027, decode.d5.loss_mask: 0.4456, decode.d5.loss_dice: 0.6898, decode.d6.loss_cls: 0.2008, decode.d6.loss_mask: 0.4454, decode.d6.loss_dice: 0.6886, decode.d7.loss_cls: 0.2003, decode.d7.loss_mask: 0.4451, decode.d7.loss_dice: 0.6876, loss: 13.5554
2022-10-31 11:02:04,020 - mmseg - INFO - Iter [18600/20000]	lr: 2.138e-07, eta: 1:42:53, time: 2.552, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1926, decode.loss_mask: 0.4408, decode.loss_dice: 0.6739, decode.d0.loss_cls: 1.4478, decode.d0.loss_mask: 0.4676, decode.d0.loss_dice: 0.7394, decode.d1.loss_cls: 0.2613, decode.d1.loss_mask: 0.4543, decode.d1.loss_dice: 0.7074, decode.d2.loss_cls: 0.2219, decode.d2.loss_mask: 0.4467, decode.d2.loss_dice: 0.6882, decode.d3.loss_cls: 0.2071, decode.d3.loss_mask: 0.4432, decode.d3.loss_dice: 0.6788, decode.d4.loss_cls: 0.2006, decode.d4.loss_mask: 0.4418, decode.d4.loss_dice: 0.6770, decode.d5.loss_cls: 0.1958, decode.d5.loss_mask: 0.4414, decode.d5.loss_dice: 0.6763, decode.d6.loss_cls: 0.1944, decode.d6.loss_mask: 0.4414, decode.d6.loss_dice: 0.6726, decode.d7.loss_cls: 0.1913, decode.d7.loss_mask: 0.4407, decode.d7.loss_dice: 0.6727, loss: 13.3169
2022-10-31 11:04:03,199 - mmseg - INFO - Iter [18650/20000]	lr: 2.062e-07, eta: 1:38:21, time: 2.384, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1969, decode.loss_mask: 0.4432, decode.loss_dice: 0.6752, decode.d0.loss_cls: 1.4519, decode.d0.loss_mask: 0.4700, decode.d0.loss_dice: 0.7395, decode.d1.loss_cls: 0.2593, decode.d1.loss_mask: 0.4569, decode.d1.loss_dice: 0.7086, decode.d2.loss_cls: 0.2211, decode.d2.loss_mask: 0.4509, decode.d2.loss_dice: 0.6897, decode.d3.loss_cls: 0.2059, decode.d3.loss_mask: 0.4469, decode.d3.loss_dice: 0.6813, decode.d4.loss_cls: 0.2009, decode.d4.loss_mask: 0.4452, decode.d4.loss_dice: 0.6800, decode.d5.loss_cls: 0.1975, decode.d5.loss_mask: 0.4441, decode.d5.loss_dice: 0.6783, decode.d6.loss_cls: 0.1949, decode.d6.loss_mask: 0.4449, decode.d6.loss_dice: 0.6754, decode.d7.loss_cls: 0.1947, decode.d7.loss_mask: 0.4444, decode.d7.loss_dice: 0.6754, loss: 13.3729
2022-10-31 11:06:02,591 - mmseg - INFO - Iter [18700/20000]	lr: 1.985e-07, eta: 1:33:55, time: 2.388, data_time: 0.014, memory: 35597, decode.loss_cls: 0.2087, decode.loss_mask: 0.4433, decode.loss_dice: 0.6792, decode.d0.loss_cls: 1.4870, decode.d0.loss_mask: 0.4696, decode.d0.loss_dice: 0.7464, decode.d1.loss_cls: 0.2752, decode.d1.loss_mask: 0.4590, decode.d1.loss_dice: 0.7137, decode.d2.loss_cls: 0.2339, decode.d2.loss_mask: 0.4517, decode.d2.loss_dice: 0.6964, decode.d3.loss_cls: 0.2190, decode.d3.loss_mask: 0.4482, decode.d3.loss_dice: 0.6866, decode.d4.loss_cls: 0.2125, decode.d4.loss_mask: 0.4465, decode.d4.loss_dice: 0.6873, decode.d5.loss_cls: 0.2089, decode.d5.loss_mask: 0.4458, decode.d5.loss_dice: 0.6853, decode.d6.loss_cls: 0.2068, decode.d6.loss_mask: 0.4443, decode.d6.loss_dice: 0.6845, decode.d7.loss_cls: 0.2090, decode.d7.loss_mask: 0.4437, decode.d7.loss_dice: 0.6806, loss: 13.5733
2022-10-31 11:08:03,694 - mmseg - INFO - Iter [18750/20000]	lr: 1.909e-07, eta: 1:29:34, time: 2.422, data_time: 0.013, memory: 35597, decode.loss_cls: 0.2055, decode.loss_mask: 0.4430, decode.loss_dice: 0.6804, decode.d0.loss_cls: 1.4698, decode.d0.loss_mask: 0.4700, decode.d0.loss_dice: 0.7461, decode.d1.loss_cls: 0.2724, decode.d1.loss_mask: 0.4576, decode.d1.loss_dice: 0.7106, decode.d2.loss_cls: 0.2359, decode.d2.loss_mask: 0.4485, decode.d2.loss_dice: 0.6959, decode.d3.loss_cls: 0.2195, decode.d3.loss_mask: 0.4453, decode.d3.loss_dice: 0.6874, decode.d4.loss_cls: 0.2132, decode.d4.loss_mask: 0.4434, decode.d4.loss_dice: 0.6834, decode.d5.loss_cls: 0.2052, decode.d5.loss_mask: 0.4428, decode.d5.loss_dice: 0.6834, decode.d6.loss_cls: 0.2074, decode.d6.loss_mask: 0.4428, decode.d6.loss_dice: 0.6816, decode.d7.loss_cls: 0.2057, decode.d7.loss_mask: 0.4429, decode.d7.loss_dice: 0.6814, loss: 13.5211
2022-10-31 11:10:05,489 - mmseg - INFO - Iter [18800/20000]	lr: 1.833e-07, eta: 1:25:19, time: 2.436, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1939, decode.loss_mask: 0.4458, decode.loss_dice: 0.6726, decode.d0.loss_cls: 1.4579, decode.d0.loss_mask: 0.4700, decode.d0.loss_dice: 0.7393, decode.d1.loss_cls: 0.2579, decode.d1.loss_mask: 0.4603, decode.d1.loss_dice: 0.7073, decode.d2.loss_cls: 0.2208, decode.d2.loss_mask: 0.4516, decode.d2.loss_dice: 0.6900, decode.d3.loss_cls: 0.2070, decode.d3.loss_mask: 0.4479, decode.d3.loss_dice: 0.6785, decode.d4.loss_cls: 0.2031, decode.d4.loss_mask: 0.4475, decode.d4.loss_dice: 0.6772, decode.d5.loss_cls: 0.1978, decode.d5.loss_mask: 0.4465, decode.d5.loss_dice: 0.6782, decode.d6.loss_cls: 0.1953, decode.d6.loss_mask: 0.4462, decode.d6.loss_dice: 0.6742, decode.d7.loss_cls: 0.1925, decode.d7.loss_mask: 0.4458, decode.d7.loss_dice: 0.6733, loss: 13.3783
2022-10-31 11:12:14,001 - mmseg - INFO - Iter [18850/20000]	lr: 1.757e-07, eta: 1:21:12, time: 2.570, data_time: 0.059, memory: 35597, decode.loss_cls: 0.1897, decode.loss_mask: 0.4389, decode.loss_dice: 0.6667, decode.d0.loss_cls: 1.4503, decode.d0.loss_mask: 0.4618, decode.d0.loss_dice: 0.7286, decode.d1.loss_cls: 0.2518, decode.d1.loss_mask: 0.4539, decode.d1.loss_dice: 0.6995, decode.d2.loss_cls: 0.2212, decode.d2.loss_mask: 0.4450, decode.d2.loss_dice: 0.6838, decode.d3.loss_cls: 0.2051, decode.d3.loss_mask: 0.4416, decode.d3.loss_dice: 0.6710, decode.d4.loss_cls: 0.1991, decode.d4.loss_mask: 0.4413, decode.d4.loss_dice: 0.6718, decode.d5.loss_cls: 0.1957, decode.d5.loss_mask: 0.4398, decode.d5.loss_dice: 0.6667, decode.d6.loss_cls: 0.1941, decode.d6.loss_mask: 0.4390, decode.d6.loss_dice: 0.6671, decode.d7.loss_cls: 0.1912, decode.d7.loss_mask: 0.4393, decode.d7.loss_dice: 0.6667, loss: 13.2206
2022-10-31 11:14:15,069 - mmseg - INFO - Iter [18900/20000]	lr: 1.680e-07, eta: 1:17:06, time: 2.421, data_time: 0.014, memory: 35597, decode.loss_cls: 0.1910, decode.loss_mask: 0.4413, decode.loss_dice: 0.6625, decode.d0.loss_cls: 1.4542, decode.d0.loss_mask: 0.4674, decode.d0.loss_dice: 0.7243, decode.d1.loss_cls: 0.2544, decode.d1.loss_mask: 0.4561, decode.d1.loss_dice: 0.6939, decode.d2.loss_cls: 0.2184, decode.d2.loss_mask: 0.4476, decode.d2.loss_dice: 0.6766, decode.d3.loss_cls: 0.2025, decode.d3.loss_mask: 0.4449, decode.d3.loss_dice: 0.6685, decode.d4.loss_cls: 0.1982, decode.d4.loss_mask: 0.4424, decode.d4.loss_dice: 0.6665, decode.d5.loss_cls: 0.1952, decode.d5.loss_mask: 0.4427, decode.d5.loss_dice: 0.6622, decode.d6.loss_cls: 0.1922, decode.d6.loss_mask: 0.4420, decode.d6.loss_dice: 0.6607, decode.d7.loss_cls: 0.1924, decode.d7.loss_mask: 0.4416, decode.d7.loss_dice: 0.6631, loss: 13.2029
2022-10-31 11:16:19,082 - mmseg - INFO - Iter [18950/20000]	lr: 1.604e-07, eta: 1:13:05, time: 2.480, data_time: 0.014, memory: 35597, decode.loss_cls: 0.1935, decode.loss_mask: 0.4490, decode.loss_dice: 0.6792, decode.d0.loss_cls: 1.4484, decode.d0.loss_mask: 0.4760, decode.d0.loss_dice: 0.7398, decode.d1.loss_cls: 0.2551, decode.d1.loss_mask: 0.4639, decode.d1.loss_dice: 0.7107, decode.d2.loss_cls: 0.2206, decode.d2.loss_mask: 0.4552, decode.d2.loss_dice: 0.6922, decode.d3.loss_cls: 0.2062, decode.d3.loss_mask: 0.4515, decode.d3.loss_dice: 0.6828, decode.d4.loss_cls: 0.2000, decode.d4.loss_mask: 0.4509, decode.d4.loss_dice: 0.6792, decode.d5.loss_cls: 0.1961, decode.d5.loss_mask: 0.4503, decode.d5.loss_dice: 0.6793, decode.d6.loss_cls: 0.1938, decode.d6.loss_mask: 0.4493, decode.d6.loss_dice: 0.6769, decode.d7.loss_cls: 0.1936, decode.d7.loss_mask: 0.4486, decode.d7.loss_dice: 0.6760, loss: 13.4182
2022-10-31 11:18:28,029 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-31 11:18:28,030 - mmseg - INFO - Iter [19000/20000]	lr: 1.528e-07, eta: 1:09:09, time: 2.579, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1875, decode.loss_mask: 0.4455, decode.loss_dice: 0.6798, decode.d0.loss_cls: 1.4590, decode.d0.loss_mask: 0.4692, decode.d0.loss_dice: 0.7382, decode.d1.loss_cls: 0.2532, decode.d1.loss_mask: 0.4580, decode.d1.loss_dice: 0.7093, decode.d2.loss_cls: 0.2176, decode.d2.loss_mask: 0.4509, decode.d2.loss_dice: 0.6932, decode.d3.loss_cls: 0.2010, decode.d3.loss_mask: 0.4478, decode.d3.loss_dice: 0.6845, decode.d4.loss_cls: 0.1943, decode.d4.loss_mask: 0.4472, decode.d4.loss_dice: 0.6821, decode.d5.loss_cls: 0.1906, decode.d5.loss_mask: 0.4457, decode.d5.loss_dice: 0.6809, decode.d6.loss_cls: 0.1879, decode.d6.loss_mask: 0.4449, decode.d6.loss_dice: 0.6796, decode.d7.loss_cls: 0.1893, decode.d7.loss_mask: 0.4449, decode.d7.loss_dice: 0.6818, loss: 13.3640
2022-10-31 11:20:26,852 - mmseg - INFO - Iter [19050/20000]	lr: 1.451e-07, eta: 1:05:14, time: 2.376, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1944, decode.loss_mask: 0.4402, decode.loss_dice: 0.6714, decode.d0.loss_cls: 1.4450, decode.d0.loss_mask: 0.4663, decode.d0.loss_dice: 0.7363, decode.d1.loss_cls: 0.2626, decode.d1.loss_mask: 0.4540, decode.d1.loss_dice: 0.7028, decode.d2.loss_cls: 0.2208, decode.d2.loss_mask: 0.4462, decode.d2.loss_dice: 0.6847, decode.d3.loss_cls: 0.2043, decode.d3.loss_mask: 0.4432, decode.d3.loss_dice: 0.6764, decode.d4.loss_cls: 0.2002, decode.d4.loss_mask: 0.4422, decode.d4.loss_dice: 0.6763, decode.d5.loss_cls: 0.1973, decode.d5.loss_mask: 0.4412, decode.d5.loss_dice: 0.6734, decode.d6.loss_cls: 0.1933, decode.d6.loss_mask: 0.4412, decode.d6.loss_dice: 0.6727, decode.d7.loss_cls: 0.1927, decode.d7.loss_mask: 0.4403, decode.d7.loss_dice: 0.6719, loss: 13.2915
2022-10-31 11:22:27,947 - mmseg - INFO - Iter [19100/20000]	lr: 1.375e-07, eta: 1:01:23, time: 2.422, data_time: 0.022, memory: 35597, decode.loss_cls: 0.2044, decode.loss_mask: 0.4500, decode.loss_dice: 0.6837, decode.d0.loss_cls: 1.4770, decode.d0.loss_mask: 0.4771, decode.d0.loss_dice: 0.7542, decode.d1.loss_cls: 0.2741, decode.d1.loss_mask: 0.4654, decode.d1.loss_dice: 0.7180, decode.d2.loss_cls: 0.2348, decode.d2.loss_mask: 0.4570, decode.d2.loss_dice: 0.6967, decode.d3.loss_cls: 0.2198, decode.d3.loss_mask: 0.4529, decode.d3.loss_dice: 0.6877, decode.d4.loss_cls: 0.2121, decode.d4.loss_mask: 0.4514, decode.d4.loss_dice: 0.6873, decode.d5.loss_cls: 0.2088, decode.d5.loss_mask: 0.4510, decode.d5.loss_dice: 0.6835, decode.d6.loss_cls: 0.2046, decode.d6.loss_mask: 0.4505, decode.d6.loss_dice: 0.6811, decode.d7.loss_cls: 0.2038, decode.d7.loss_mask: 0.4506, decode.d7.loss_dice: 0.6815, loss: 13.6190
2022-10-31 11:24:32,489 - mmseg - INFO - Iter [19150/20000]	lr: 1.299e-07, eta: 0:57:37, time: 2.489, data_time: 0.013, memory: 35597, decode.loss_cls: 0.2040, decode.loss_mask: 0.4492, decode.loss_dice: 0.6822, decode.d0.loss_cls: 1.4649, decode.d0.loss_mask: 0.4758, decode.d0.loss_dice: 0.7516, decode.d1.loss_cls: 0.2754, decode.d1.loss_mask: 0.4645, decode.d1.loss_dice: 0.7182, decode.d2.loss_cls: 0.2354, decode.d2.loss_mask: 0.4564, decode.d2.loss_dice: 0.6981, decode.d3.loss_cls: 0.2182, decode.d3.loss_mask: 0.4527, decode.d3.loss_dice: 0.6894, decode.d4.loss_cls: 0.2123, decode.d4.loss_mask: 0.4513, decode.d4.loss_dice: 0.6870, decode.d5.loss_cls: 0.2095, decode.d5.loss_mask: 0.4506, decode.d5.loss_dice: 0.6847, decode.d6.loss_cls: 0.2071, decode.d6.loss_mask: 0.4486, decode.d6.loss_dice: 0.6830, decode.d7.loss_cls: 0.2063, decode.d7.loss_mask: 0.4490, decode.d7.loss_dice: 0.6821, loss: 13.6075
2022-10-31 11:26:40,129 - mmseg - INFO - Iter [19200/20000]	lr: 1.222e-07, eta: 0:53:55, time: 2.555, data_time: 0.058, memory: 35597, decode.loss_cls: 0.1958, decode.loss_mask: 0.4414, decode.loss_dice: 0.6725, decode.d0.loss_cls: 1.4519, decode.d0.loss_mask: 0.4686, decode.d0.loss_dice: 0.7380, decode.d1.loss_cls: 0.2628, decode.d1.loss_mask: 0.4551, decode.d1.loss_dice: 0.7046, decode.d2.loss_cls: 0.2235, decode.d2.loss_mask: 0.4474, decode.d2.loss_dice: 0.6846, decode.d3.loss_cls: 0.2088, decode.d3.loss_mask: 0.4439, decode.d3.loss_dice: 0.6776, decode.d4.loss_cls: 0.2022, decode.d4.loss_mask: 0.4430, decode.d4.loss_dice: 0.6760, decode.d5.loss_cls: 0.1978, decode.d5.loss_mask: 0.4424, decode.d5.loss_dice: 0.6762, decode.d6.loss_cls: 0.1972, decode.d6.loss_mask: 0.4419, decode.d6.loss_dice: 0.6738, decode.d7.loss_cls: 0.1955, decode.d7.loss_mask: 0.4411, decode.d7.loss_dice: 0.6712, loss: 13.3347
2022-10-31 11:28:46,115 - mmseg - INFO - Iter [19250/20000]	lr: 1.146e-07, eta: 0:50:15, time: 2.520, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1948, decode.loss_mask: 0.4337, decode.loss_dice: 0.6694, decode.d0.loss_cls: 1.4729, decode.d0.loss_mask: 0.4603, decode.d0.loss_dice: 0.7317, decode.d1.loss_cls: 0.2683, decode.d1.loss_mask: 0.4463, decode.d1.loss_dice: 0.6973, decode.d2.loss_cls: 0.2262, decode.d2.loss_mask: 0.4402, decode.d2.loss_dice: 0.6825, decode.d3.loss_cls: 0.2109, decode.d3.loss_mask: 0.4366, decode.d3.loss_dice: 0.6732, decode.d4.loss_cls: 0.2039, decode.d4.loss_mask: 0.4362, decode.d4.loss_dice: 0.6736, decode.d5.loss_cls: 0.2014, decode.d5.loss_mask: 0.4340, decode.d5.loss_dice: 0.6698, decode.d6.loss_cls: 0.1981, decode.d6.loss_mask: 0.4342, decode.d6.loss_dice: 0.6675, decode.d7.loss_cls: 0.1970, decode.d7.loss_mask: 0.4335, decode.d7.loss_dice: 0.6667, loss: 13.2603
2022-10-31 11:30:52,965 - mmseg - INFO - Iter [19300/20000]	lr: 1.070e-07, eta: 0:46:38, time: 2.537, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1952, decode.loss_mask: 0.4426, decode.loss_dice: 0.6791, decode.d0.loss_cls: 1.4676, decode.d0.loss_mask: 0.4697, decode.d0.loss_dice: 0.7469, decode.d1.loss_cls: 0.2669, decode.d1.loss_mask: 0.4586, decode.d1.loss_dice: 0.7124, decode.d2.loss_cls: 0.2233, decode.d2.loss_mask: 0.4492, decode.d2.loss_dice: 0.6940, decode.d3.loss_cls: 0.2079, decode.d3.loss_mask: 0.4457, decode.d3.loss_dice: 0.6841, decode.d4.loss_cls: 0.2023, decode.d4.loss_mask: 0.4445, decode.d4.loss_dice: 0.6847, decode.d5.loss_cls: 0.1988, decode.d5.loss_mask: 0.4426, decode.d5.loss_dice: 0.6807, decode.d6.loss_cls: 0.1947, decode.d6.loss_mask: 0.4428, decode.d6.loss_dice: 0.6811, decode.d7.loss_cls: 0.1964, decode.d7.loss_mask: 0.4424, decode.d7.loss_dice: 0.6786, loss: 13.4328
2022-10-31 11:32:55,968 - mmseg - INFO - Iter [19350/20000]	lr: 9.935e-08, eta: 0:43:03, time: 2.460, data_time: 0.014, memory: 35597, decode.loss_cls: 0.1933, decode.loss_mask: 0.4499, decode.loss_dice: 0.6756, decode.d0.loss_cls: 1.4411, decode.d0.loss_mask: 0.4780, decode.d0.loss_dice: 0.7389, decode.d1.loss_cls: 0.2563, decode.d1.loss_mask: 0.4662, decode.d1.loss_dice: 0.7075, decode.d2.loss_cls: 0.2225, decode.d2.loss_mask: 0.4564, decode.d2.loss_dice: 0.6871, decode.d3.loss_cls: 0.2086, decode.d3.loss_mask: 0.4533, decode.d3.loss_dice: 0.6809, decode.d4.loss_cls: 0.2003, decode.d4.loss_mask: 0.4519, decode.d4.loss_dice: 0.6775, decode.d5.loss_cls: 0.1979, decode.d5.loss_mask: 0.4511, decode.d5.loss_dice: 0.6759, decode.d6.loss_cls: 0.1945, decode.d6.loss_mask: 0.4500, decode.d6.loss_dice: 0.6758, decode.d7.loss_cls: 0.1953, decode.d7.loss_mask: 0.4495, decode.d7.loss_dice: 0.6737, loss: 13.4088
2022-10-31 11:34:56,860 - mmseg - INFO - Iter [19400/20000]	lr: 9.172e-08, eta: 0:39:31, time: 2.418, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1922, decode.loss_mask: 0.4407, decode.loss_dice: 0.6721, decode.d0.loss_cls: 1.4474, decode.d0.loss_mask: 0.4672, decode.d0.loss_dice: 0.7356, decode.d1.loss_cls: 0.2600, decode.d1.loss_mask: 0.4561, decode.d1.loss_dice: 0.7078, decode.d2.loss_cls: 0.2198, decode.d2.loss_mask: 0.4483, decode.d2.loss_dice: 0.6884, decode.d3.loss_cls: 0.2050, decode.d3.loss_mask: 0.4446, decode.d3.loss_dice: 0.6795, decode.d4.loss_cls: 0.1972, decode.d4.loss_mask: 0.4430, decode.d4.loss_dice: 0.6790, decode.d5.loss_cls: 0.1936, decode.d5.loss_mask: 0.4425, decode.d5.loss_dice: 0.6776, decode.d6.loss_cls: 0.1913, decode.d6.loss_mask: 0.4416, decode.d6.loss_dice: 0.6761, decode.d7.loss_cls: 0.1925, decode.d7.loss_mask: 0.4411, decode.d7.loss_dice: 0.6742, loss: 13.3147
2022-10-31 11:36:58,447 - mmseg - INFO - Iter [19450/20000]	lr: 8.409e-08, eta: 0:36:01, time: 2.432, data_time: 0.027, memory: 35597, decode.loss_cls: 0.1909, decode.loss_mask: 0.4418, decode.loss_dice: 0.6664, decode.d0.loss_cls: 1.4497, decode.d0.loss_mask: 0.4677, decode.d0.loss_dice: 0.7303, decode.d1.loss_cls: 0.2585, decode.d1.loss_mask: 0.4566, decode.d1.loss_dice: 0.6986, decode.d2.loss_cls: 0.2214, decode.d2.loss_mask: 0.4488, decode.d2.loss_dice: 0.6821, decode.d3.loss_cls: 0.2033, decode.d3.loss_mask: 0.4451, decode.d3.loss_dice: 0.6735, decode.d4.loss_cls: 0.1957, decode.d4.loss_mask: 0.4447, decode.d4.loss_dice: 0.6732, decode.d5.loss_cls: 0.1924, decode.d5.loss_mask: 0.4439, decode.d5.loss_dice: 0.6700, decode.d6.loss_cls: 0.1927, decode.d6.loss_mask: 0.4425, decode.d6.loss_dice: 0.6678, decode.d7.loss_cls: 0.1924, decode.d7.loss_mask: 0.4420, decode.d7.loss_dice: 0.6679, loss: 13.2599
2022-10-31 11:39:03,669 - mmseg - INFO - Iter [19500/20000]	lr: 7.646e-08, eta: 0:32:35, time: 2.504, data_time: 0.065, memory: 35597, decode.loss_cls: 0.1998, decode.loss_mask: 0.4457, decode.loss_dice: 0.6779, decode.d0.loss_cls: 1.4728, decode.d0.loss_mask: 0.4737, decode.d0.loss_dice: 0.7443, decode.d1.loss_cls: 0.2692, decode.d1.loss_mask: 0.4608, decode.d1.loss_dice: 0.7093, decode.d2.loss_cls: 0.2278, decode.d2.loss_mask: 0.4548, decode.d2.loss_dice: 0.6940, decode.d3.loss_cls: 0.2138, decode.d3.loss_mask: 0.4498, decode.d3.loss_dice: 0.6850, decode.d4.loss_cls: 0.2075, decode.d4.loss_mask: 0.4489, decode.d4.loss_dice: 0.6832, decode.d5.loss_cls: 0.2064, decode.d5.loss_mask: 0.4463, decode.d5.loss_dice: 0.6801, decode.d6.loss_cls: 0.2046, decode.d6.loss_mask: 0.4456, decode.d6.loss_dice: 0.6785, decode.d7.loss_cls: 0.2028, decode.d7.loss_mask: 0.4461, decode.d7.loss_dice: 0.6794, loss: 13.5080
2022-10-31 11:41:03,207 - mmseg - INFO - Iter [19550/20000]	lr: 6.883e-08, eta: 0:29:09, time: 2.391, data_time: 0.019, memory: 35597, decode.loss_cls: 0.1986, decode.loss_mask: 0.4427, decode.loss_dice: 0.6754, decode.d0.loss_cls: 1.4780, decode.d0.loss_mask: 0.4697, decode.d0.loss_dice: 0.7456, decode.d1.loss_cls: 0.2690, decode.d1.loss_mask: 0.4574, decode.d1.loss_dice: 0.7111, decode.d2.loss_cls: 0.2282, decode.d2.loss_mask: 0.4494, decode.d2.loss_dice: 0.6916, decode.d3.loss_cls: 0.2136, decode.d3.loss_mask: 0.4456, decode.d3.loss_dice: 0.6800, decode.d4.loss_cls: 0.2080, decode.d4.loss_mask: 0.4442, decode.d4.loss_dice: 0.6786, decode.d5.loss_cls: 0.2017, decode.d5.loss_mask: 0.4434, decode.d5.loss_dice: 0.6804, decode.d6.loss_cls: 0.1996, decode.d6.loss_mask: 0.4429, decode.d6.loss_dice: 0.6781, decode.d7.loss_cls: 0.1991, decode.d7.loss_mask: 0.4426, decode.d7.loss_dice: 0.6777, loss: 13.4523
2022-10-31 11:43:04,759 - mmseg - INFO - Iter [19600/20000]	lr: 6.120e-08, eta: 0:25:47, time: 2.431, data_time: 0.012, memory: 35597, decode.loss_cls: 0.1947, decode.loss_mask: 0.4399, decode.loss_dice: 0.6715, decode.d0.loss_cls: 1.4610, decode.d0.loss_mask: 0.4681, decode.d0.loss_dice: 0.7365, decode.d1.loss_cls: 0.2625, decode.d1.loss_mask: 0.4560, decode.d1.loss_dice: 0.7058, decode.d2.loss_cls: 0.2247, decode.d2.loss_mask: 0.4493, decode.d2.loss_dice: 0.6880, decode.d3.loss_cls: 0.2090, decode.d3.loss_mask: 0.4434, decode.d3.loss_dice: 0.6772, decode.d4.loss_cls: 0.2019, decode.d4.loss_mask: 0.4427, decode.d4.loss_dice: 0.6775, decode.d5.loss_cls: 0.2005, decode.d5.loss_mask: 0.4414, decode.d5.loss_dice: 0.6750, decode.d6.loss_cls: 0.1989, decode.d6.loss_mask: 0.4404, decode.d6.loss_dice: 0.6725, decode.d7.loss_cls: 0.1933, decode.d7.loss_mask: 0.4411, decode.d7.loss_dice: 0.6731, loss: 13.3458
2022-10-31 11:45:07,900 - mmseg - INFO - Iter [19650/20000]	lr: 5.357e-08, eta: 0:22:27, time: 2.463, data_time: 0.014, memory: 35597, decode.loss_cls: 0.1878, decode.loss_mask: 0.4427, decode.loss_dice: 0.6729, decode.d0.loss_cls: 1.4432, decode.d0.loss_mask: 0.4690, decode.d0.loss_dice: 0.7378, decode.d1.loss_cls: 0.2560, decode.d1.loss_mask: 0.4555, decode.d1.loss_dice: 0.7074, decode.d2.loss_cls: 0.2158, decode.d2.loss_mask: 0.4488, decode.d2.loss_dice: 0.6885, decode.d3.loss_cls: 0.1990, decode.d3.loss_mask: 0.4462, decode.d3.loss_dice: 0.6807, decode.d4.loss_cls: 0.1915, decode.d4.loss_mask: 0.4457, decode.d4.loss_dice: 0.6806, decode.d5.loss_cls: 0.1918, decode.d5.loss_mask: 0.4430, decode.d5.loss_dice: 0.6782, decode.d6.loss_cls: 0.1850, decode.d6.loss_mask: 0.4438, decode.d6.loss_dice: 0.6759, decode.d7.loss_cls: 0.1852, decode.d7.loss_mask: 0.4434, decode.d7.loss_dice: 0.6760, loss: 13.2913
2022-10-31 11:47:12,644 - mmseg - INFO - Iter [19700/20000]	lr: 4.594e-08, eta: 0:19:09, time: 2.495, data_time: 0.013, memory: 35597, decode.loss_cls: 0.2040, decode.loss_mask: 0.4378, decode.loss_dice: 0.6721, decode.d0.loss_cls: 1.4549, decode.d0.loss_mask: 0.4670, decode.d0.loss_dice: 0.7401, decode.d1.loss_cls: 0.2735, decode.d1.loss_mask: 0.4535, decode.d1.loss_dice: 0.7071, decode.d2.loss_cls: 0.2339, decode.d2.loss_mask: 0.4461, decode.d2.loss_dice: 0.6877, decode.d3.loss_cls: 0.2167, decode.d3.loss_mask: 0.4411, decode.d3.loss_dice: 0.6782, decode.d4.loss_cls: 0.2123, decode.d4.loss_mask: 0.4396, decode.d4.loss_dice: 0.6764, decode.d5.loss_cls: 0.2096, decode.d5.loss_mask: 0.4394, decode.d5.loss_dice: 0.6748, decode.d6.loss_cls: 0.2055, decode.d6.loss_mask: 0.4378, decode.d6.loss_dice: 0.6715, decode.d7.loss_cls: 0.2067, decode.d7.loss_mask: 0.4376, decode.d7.loss_dice: 0.6720, loss: 13.3971
2022-10-31 11:49:22,138 - mmseg - INFO - Iter [19750/20000]	lr: 3.830e-08, eta: 0:15:53, time: 2.590, data_time: 0.030, memory: 35597, decode.loss_cls: 0.1915, decode.loss_mask: 0.4398, decode.loss_dice: 0.6734, decode.d0.loss_cls: 1.4577, decode.d0.loss_mask: 0.4640, decode.d0.loss_dice: 0.7358, decode.d1.loss_cls: 0.2587, decode.d1.loss_mask: 0.4526, decode.d1.loss_dice: 0.7076, decode.d2.loss_cls: 0.2201, decode.d2.loss_mask: 0.4474, decode.d2.loss_dice: 0.6882, decode.d3.loss_cls: 0.2043, decode.d3.loss_mask: 0.4432, decode.d3.loss_dice: 0.6789, decode.d4.loss_cls: 0.1972, decode.d4.loss_mask: 0.4426, decode.d4.loss_dice: 0.6772, decode.d5.loss_cls: 0.1952, decode.d5.loss_mask: 0.4421, decode.d5.loss_dice: 0.6782, decode.d6.loss_cls: 0.1937, decode.d6.loss_mask: 0.4398, decode.d6.loss_dice: 0.6756, decode.d7.loss_cls: 0.1919, decode.d7.loss_mask: 0.4392, decode.d7.loss_dice: 0.6756, loss: 13.3113
2022-10-31 11:51:27,907 - mmseg - INFO - Iter [19800/20000]	lr: 3.067e-08, eta: 0:12:39, time: 2.515, data_time: 0.058, memory: 35597, decode.loss_cls: 0.1892, decode.loss_mask: 0.4498, decode.loss_dice: 0.6829, decode.d0.loss_cls: 1.4238, decode.d0.loss_mask: 0.4761, decode.d0.loss_dice: 0.7443, decode.d1.loss_cls: 0.2517, decode.d1.loss_mask: 0.4637, decode.d1.loss_dice: 0.7144, decode.d2.loss_cls: 0.2197, decode.d2.loss_mask: 0.4562, decode.d2.loss_dice: 0.6948, decode.d3.loss_cls: 0.2010, decode.d3.loss_mask: 0.4528, decode.d3.loss_dice: 0.6867, decode.d4.loss_cls: 0.1957, decode.d4.loss_mask: 0.4523, decode.d4.loss_dice: 0.6865, decode.d5.loss_cls: 0.1929, decode.d5.loss_mask: 0.4505, decode.d5.loss_dice: 0.6866, decode.d6.loss_cls: 0.1912, decode.d6.loss_mask: 0.4502, decode.d6.loss_dice: 0.6836, decode.d7.loss_cls: 0.1898, decode.d7.loss_mask: 0.4487, decode.d7.loss_dice: 0.6820, loss: 13.4172
2022-10-31 11:53:31,282 - mmseg - INFO - Iter [19850/20000]	lr: 2.304e-08, eta: 0:09:26, time: 2.468, data_time: 0.014, memory: 35597, decode.loss_cls: 0.2008, decode.loss_mask: 0.4396, decode.loss_dice: 0.6696, decode.d0.loss_cls: 1.4618, decode.d0.loss_mask: 0.4667, decode.d0.loss_dice: 0.7354, decode.d1.loss_cls: 0.2637, decode.d1.loss_mask: 0.4558, decode.d1.loss_dice: 0.7039, decode.d2.loss_cls: 0.2298, decode.d2.loss_mask: 0.4460, decode.d2.loss_dice: 0.6826, decode.d3.loss_cls: 0.2121, decode.d3.loss_mask: 0.4422, decode.d3.loss_dice: 0.6747, decode.d4.loss_cls: 0.2079, decode.d4.loss_mask: 0.4411, decode.d4.loss_dice: 0.6738, decode.d5.loss_cls: 0.2028, decode.d5.loss_mask: 0.4400, decode.d5.loss_dice: 0.6750, decode.d6.loss_cls: 0.2003, decode.d6.loss_mask: 0.4391, decode.d6.loss_dice: 0.6711, decode.d7.loss_cls: 0.2019, decode.d7.loss_mask: 0.4399, decode.d7.loss_dice: 0.6708, loss: 13.3484
2022-10-31 11:55:32,619 - mmseg - INFO - Iter [19900/20000]	lr: 1.541e-08, eta: 0:06:16, time: 2.425, data_time: 0.013, memory: 35597, decode.loss_cls: 0.1935, decode.loss_mask: 0.4427, decode.loss_dice: 0.6688, decode.d0.loss_cls: 1.4684, decode.d0.loss_mask: 0.4707, decode.d0.loss_dice: 0.7346, decode.d1.loss_cls: 0.2648, decode.d1.loss_mask: 0.4562, decode.d1.loss_dice: 0.7019, decode.d2.loss_cls: 0.2242, decode.d2.loss_mask: 0.4502, decode.d2.loss_dice: 0.6847, decode.d3.loss_cls: 0.2063, decode.d3.loss_mask: 0.4458, decode.d3.loss_dice: 0.6760, decode.d4.loss_cls: 0.2032, decode.d4.loss_mask: 0.4440, decode.d4.loss_dice: 0.6744, decode.d5.loss_cls: 0.1985, decode.d5.loss_mask: 0.4429, decode.d5.loss_dice: 0.6721, decode.d6.loss_cls: 0.1944, decode.d6.loss_mask: 0.4431, decode.d6.loss_dice: 0.6718, decode.d7.loss_cls: 0.1932, decode.d7.loss_mask: 0.4426, decode.d7.loss_dice: 0.6698, loss: 13.3388
2022-10-31 11:57:39,528 - mmseg - INFO - Iter [19950/20000]	lr: 7.783e-09, eta: 0:03:07, time: 2.540, data_time: 0.016, memory: 35597, decode.loss_cls: 0.1938, decode.loss_mask: 0.4370, decode.loss_dice: 0.6764, decode.d0.loss_cls: 1.4633, decode.d0.loss_mask: 0.4617, decode.d0.loss_dice: 0.7366, decode.d1.loss_cls: 0.2607, decode.d1.loss_mask: 0.4505, decode.d1.loss_dice: 0.7069, decode.d2.loss_cls: 0.2229, decode.d2.loss_mask: 0.4425, decode.d2.loss_dice: 0.6907, decode.d3.loss_cls: 0.2056, decode.d3.loss_mask: 0.4398, decode.d3.loss_dice: 0.6820, decode.d4.loss_cls: 0.1992, decode.d4.loss_mask: 0.4385, decode.d4.loss_dice: 0.6800, decode.d5.loss_cls: 0.1976, decode.d5.loss_mask: 0.4382, decode.d5.loss_dice: 0.6778, decode.d6.loss_cls: 0.1934, decode.d6.loss_mask: 0.4365, decode.d6.loss_dice: 0.6764, decode.d7.loss_cls: 0.1949, decode.d7.loss_mask: 0.4370, decode.d7.loss_dice: 0.6764, loss: 13.3163
2022-10-31 11:59:40,129 - mmseg - INFO - Saving checkpoint at 20000 iterations
2022-10-31 12:00:13,447 - mmseg - INFO - Exp name: mask2former_beitXclip_adapter_giant_896_20k_coco164k2ade20k_ss.py
2022-10-31 12:00:13,447 - mmseg - INFO - Iter [20000/20000]	lr: 1.526e-10, eta: 0:00:00, time: 3.078, data_time: 0.014, memory: 35597, decode.loss_cls: 0.1882, decode.loss_mask: 0.4399, decode.loss_dice: 0.6807, decode.d0.loss_cls: 1.4548, decode.d0.loss_mask: 0.4643, decode.d0.loss_dice: 0.7419, decode.d1.loss_cls: 0.2548, decode.d1.loss_mask: 0.4541, decode.d1.loss_dice: 0.7135, decode.d2.loss_cls: 0.2163, decode.d2.loss_mask: 0.4468, decode.d2.loss_dice: 0.6930, decode.d3.loss_cls: 0.2025, decode.d3.loss_mask: 0.4436, decode.d3.loss_dice: 0.6849, decode.d4.loss_cls: 0.1956, decode.d4.loss_mask: 0.4420, decode.d4.loss_dice: 0.6824, decode.d5.loss_cls: 0.1907, decode.d5.loss_mask: 0.4410, decode.d5.loss_dice: 0.6828, decode.d6.loss_cls: 0.1915, decode.d6.loss_mask: 0.4400, decode.d6.loss_dice: 0.6797, decode.d7.loss_cls: 0.1921, decode.d7.loss_mask: 0.4397, decode.d7.loss_dice: 0.6793, loss: 13.3363
